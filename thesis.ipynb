{"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Xi9z2tGsLgV_OMGB36wWYdspsBzsG8Sb","authorship_tag":"ABX9TyPua6D/VG55KQLJKEi/Yp89"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67880,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":56596}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook contains the empirical study regarding the attention mechanism.","metadata":{}},{"cell_type":"markdown","source":"______________\n# FINAL STEPS:\n* Train networks on MNIST for X epochs\n* Train networks for augmented MNIST for X epochs\n* Ablation Study (VAN without LKA)\n* Compare Computational Costs (Deepspeed Profiler?)\n - Training time\n - Number of Modules\n - Number of parameters\n - FLOPs\n ","metadata":{}},{"cell_type":"markdown","source":"# Relevant information about the training run\n\n- OS: Linux-5.15.133+-x86_64-with-glibc2.31\n- GPU: Tesla P100-PCIE-16GB\n- PyTorch: 2.1.2\n- CUDA: 12.1\n- cudnn: 8900\n- Python Version: 3.10.13","metadata":{}},{"cell_type":"markdown","source":"# Relevant information about the external package versions\n- matplotlib==3.7.5\n- numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1707225380409/work/dist/numpy-1.26.4-cp310-cp310-linux_x86_64.whl#sha256=51131fd8fc130cd168aecaf1bc0ea85f92e8ffebf211772ceb16ac2e7f10d7ca\n- torch @ file:///tmp/torch/torch-2.1.2-cp310-cp310-linux_x86_64.whl#sha256=ae3259980b8d6551608b32fde2695baca64c72ed15ab2332023a248c113815a8\n- torchvision @ file:///tmp/torch/torchvision-0.16.2-cp310-cp310-linux_x86_64.whl#sha256=105901a20924f652ee62df0bb57580c67725eb21f11a349658952c4be2050d94\n","metadata":{}},{"cell_type":"code","source":"# Installation and import of relevant packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport platform\nimport random\nimport time\nimport torch\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import v2\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.profiler import profile, record_function, ProfilerActivity","metadata":{"executionInfo":{"elapsed":214,"status":"ok","timestamp":1711009733593,"user":{"displayName":"Tobias Jedlicka","userId":"10721733797122281021"},"user_tz":-60},"id":"hj481hNsaSLk","execution":{"iopub.status.busy":"2024-06-26T07:27:19.346548Z","iopub.execute_input":"2024-06-26T07:27:19.347136Z","iopub.status.idle":"2024-06-26T07:27:24.002450Z","shell.execute_reply.started":"2024-06-26T07:27:19.347105Z","shell.execute_reply":"2024-06-26T07:27:24.001553Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Get information about current runtime and package versions.\n\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\n\n# Get CUDA device count\ncuda_device_count = torch.cuda.device_count() if cuda_available else 0\n\n# Get current CUDA device index\ncuda_device_index = torch.cuda.current_device() if cuda_available else None\n\n# Get name of current CUDA device\ncuda_device_name = torch.cuda.get_device_name(cuda_device_index) if cuda_available else None\n\n# Get CUDA capability of the device\ncuda_capability = torch.cuda.get_device_capability(cuda_device_index) if cuda_available else None\n\n# Get CUDA version\ncuda_version = torch.version.cuda if cuda_available else None\n\n# Get cuDNN version\ncudnn_version = torch.backends.cudnn.version() if cuda_available else None\n\n# Get PyTorch version\npytorch_version = torch.__version__\n\n# Get OS information\nos_info = platform.platform()\n\npython_version = platform.python_version()\n\n# Print the information\nenvironment_dict = {\"OS:\", os_info,\n                    \"GPU:\", cuda_device_name,\n                    \"PyTorch:\", pytorch_version,\n                    \"CUDA:\", cuda_version,\n                    \"cudnn:\", cudnn_version,\n                    \"Python Version:\", python_version\n                   }\nprint(\"OS:\", os_info)\nprint(\"GPU:\", cuda_device_name)\nprint(\"PyTorch:\", pytorch_version)\nprint(\"CUDA:\", cuda_version)\nprint(\"cudnn:\", cudnn_version)\nprint(\"Python Version:\", python_version)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T19:22:05.771929Z","iopub.execute_input":"2024-06-25T19:22:05.773118Z","iopub.status.idle":"2024-06-25T19:22:05.846857Z","shell.execute_reply.started":"2024-06-25T19:22:05.773063Z","shell.execute_reply":"2024-06-25T19:22:05.845736Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"OS: Linux-5.15.133+-x86_64-with-glibc2.31\nGPU: Tesla P100-PCIE-16GB\nPyTorch: 2.1.2\nCUDA: 12.1\ncudnn: 8900\nPython Version: 3.10.13\n","output_type":"stream"}]},{"cell_type":"code","source":"# List package versions\n!pip freeze","metadata":{"execution":{"iopub.status.busy":"2024-06-25T19:22:08.283457Z","iopub.execute_input":"2024-06-25T19:22:08.283855Z","iopub.status.idle":"2024-06-25T19:22:11.774313Z","shell.execute_reply.started":"2024-06-25T19:22:08.283823Z","shell.execute_reply":"2024-06-25T19:22:11.772939Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"absl-py==1.4.0\naccelerate==0.27.2\naccess==1.1.9\naffine==2.4.0\naiobotocore==2.11.2\naiofiles==22.1.0\naiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1701099469104/work\naiohttp-cors==0.7.0\naioitertools==0.11.0\naiorwlock==1.3.0\naiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work\naiosqlite==0.19.0\nalbumentations==1.4.0\nalembic==1.13.1\naltair==5.2.0\nannotated-types @ file:///home/conda/feedstock_root/build_artifacts/annotated-types_1696634205638/work\nannoy==1.17.3\nanyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1702909220329/work\napache-beam==2.46.0\naplus==0.11.0\nappdirs==1.4.4\narchspec @ file:///home/conda/feedstock_root/build_artifacts/archspec_1699370045702/work\nargon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1692818318753/work\nargon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1695386546427/work\narray-record==0.5.0\narrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1696128962909/work\narviz==0.17.0\nastroid==3.0.3\nastropy==6.0.0\nastropy-iers-data==0.2024.2.19.0.28.47\nasttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work\nastunparse==1.6.3\nasync-lru==2.0.4\nasync-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work\nattrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1704011227531/work\naudioread==3.0.1\nautopep8==2.0.4\nBabel==2.14.0\nbackoff==2.2.1\nbayesian-optimization==1.4.3\nbayespy==0.5.28\nbeatrix_jupyterlab @ file:///home/kbuilder/miniconda3/conda-bld/dlenv-tf-2-15-gpu_1704941576253/work/packages/beatrix_jupyterlab-2023.128.151533.tar.gz#sha256=8c6941d08ce18f5b9ea7719574d611c18163074ff8254e0734342014eb064a48\nbeautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work\nbidict==0.23.1\nbiopython==1.83\nblake3==0.2.1\nbleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1696630167146/work\nblessed==1.20.0\nblinker==1.7.0\nblis @ file:///home/conda/feedstock_root/build_artifacts/cython-blis_1696148805003/work\nblosc2==2.5.1\nbokeh @ file:///home/conda/feedstock_root/build_artifacts/bokeh_1706215790147/work\nboltons @ file:///home/conda/feedstock_root/build_artifacts/boltons_1703154663129/work\nBoruta==0.3\nboto3==1.26.100\nbotocore==1.34.34\nbq_helper==0.4.1\nbqplot==0.12.43\nbranca==0.7.1\nbrewer2mpl==1.4.1\nBrotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1687884021435/work\nbrotlipy==0.7.0\ncached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work\ncachetools==4.2.4\nCartopy @ file:///home/conda/feedstock_root/build_artifacts/cartopy_1698172724393/work\ncatalogue @ file:///home/conda/feedstock_root/build_artifacts/catalogue_1695626339626/work\ncatalyst @ git+https://github.com/Philmod/catalyst.git@9420384a98c4b9d3b17b959e66f845b98457b545\ncatboost==1.2.2\ncategory-encoders==2.6.3\ncertifi @ file:///home/conda/feedstock_root/build_artifacts/certifi_1707022139797/work/certifi\ncesium==0.12.1\ncffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1696001684923/work\ncharset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1698833585322/work\nchex==0.1.85\ncleverhans==4.0.0\nclick @ file:///home/conda/feedstock_root/build_artifacts/click_1692311806742/work\nclick-plugins==1.1.1\ncligj==0.7.2\ncloud-tpu-client==0.10\ncloud-tpu-profiler==2.4.0\ncloudpathlib @ file:///home/conda/feedstock_root/build_artifacts/cloudpathlib-meta_1697837790453/work\ncloudpickle==2.2.1\ncmdstanpy==1.2.1\ncmudict==1.0.18\ncolorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work\ncolorcet==3.0.1\ncolorful==0.5.6\ncolorlog==6.8.2\ncolorlover==0.3.0\ncomm @ file:///home/conda/feedstock_root/build_artifacts/comm_1704278392174/work\nconda @ file:///home/conda/feedstock_root/build_artifacts/conda_1694556045812/work\nconda-libmamba-solver @ file:///home/conda/feedstock_root/build_artifacts/conda-libmamba-solver_1690880668143/work/src\nconda-package-handling @ file:///home/conda/feedstock_root/build_artifacts/conda-package-handling_1691048088238/work\nconda_package_streaming @ file:///home/conda/feedstock_root/build_artifacts/conda-package-streaming_1691009212940/work\nconfection @ file:///home/conda/feedstock_root/build_artifacts/confection_1701179074719/work\ncontextily==1.5.0\ncontourpy @ file:///home/conda/feedstock_root/build_artifacts/contourpy_1699041363598/work\nconvertdate==2.4.0\ncrcmod==1.7\ncryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1701563205069/work\ncuda-python @ file:///opt/conda/conda-bld/cuda-python_1696638333144/work\ncudf @ file:///opt/conda/conda-bld/work/python/cudf\ncufflinks==0.17.3\ncuml @ file:///opt/conda/conda-bld/work/python\ncupy @ file:///home/conda/feedstock_root/build_artifacts/cupy-split_1707093121318/work\nCVXcanon==0.1.2\ncycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1696677705766/work\ncymem @ file:///home/conda/feedstock_root/build_artifacts/cymem_1695443485440/work\ncysignals==1.11.4\nCython==3.0.8\ncytoolz @ file:///home/conda/feedstock_root/build_artifacts/cytoolz_1706897049115/work\ndaal==2024.1.0\ndaal4py==2024.1.0\ndacite==1.8.1\ndask==2024.2.0\ndask-cuda @ file:///opt/conda/conda-bld/work\ndask-cudf @ file:///opt/conda/conda-bld/work/python/dask_cudf\ndataclasses-json==0.6.4\ndataproc_jupyter_plugin==0.1.66\ndatasets==2.1.0\ndatashader==0.16.0\ndatatile==1.0.3\ndb-dtypes==1.2.0\ndeap==1.4.1\ndebugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1695534290310/work\ndecorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\ndeepdiff==6.7.1\ndefusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work\nDelorean==1.0.0\nDeprecated==1.2.14\ndeprecation==2.1.0\ndescartes==1.1.0\ndill==0.3.8\ndipy==1.8.0\ndistlib==0.3.8\ndistributed @ file:///home/conda/feedstock_root/build_artifacts/distributed_1689891044039/work\ndistro @ file:///home/conda/feedstock_root/build_artifacts/distro_1704321475663/work\ndm-tree==0.1.8\ndocker==7.0.0\ndocker-pycreds==0.4.0\ndocopt==0.6.2\ndocstring-parser==0.15\ndocstring-to-markdown==0.15\ndocutils==0.20.1\nearthengine-api==0.1.391\neasydict==1.12\neasyocr==1.7.1\necos==2.0.13\neli5==0.13.0\nemoji==2.10.1\nen-core-web-lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl#sha256=ab70aeb6172cde82508f7739f35ebc9918a3d07debeed637403c8f794ba3d3dc\nen-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\nentrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work\nephem==4.1.5\nesda==2.5.1\nessentia==2.1b6.dev1110\net-xmlfile==1.1.0\netils==1.6.0\nexceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1704921103267/work\nexecuting @ file:///home/conda/feedstock_root/build_artifacts/executing_1698579936712/work\nexplainable-ai-sdk==1.3.3\nFarama-Notifications==0.0.4\nfastai==2.7.14\nfastapi==0.108.0\nfastavro==1.9.3\nfastcore==1.5.29\nfastdownload==0.0.7\nfasteners==0.19\nfastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1703780968325/work/dist\nfastprogress==1.0.3\nfastrlock @ file:///home/conda/feedstock_root/build_artifacts/fastrlock_1702696298817/work\nfasttext==0.9.2\nfbpca==1.0\nfeather-format==0.4.1\nfeaturetools==1.29.0\nfilelock==3.13.1\nfiona==1.9.5\nfitter==1.7.0\nflake8==7.0.0\nflashtext==2.7\nFlask==3.0.2\nflatbuffers==23.5.26\nflax==0.8.1\nfolium==0.15.1\nfonttools==4.47.0\nfqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist\nfrozendict==2.4.0\nfrozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1702645481127/work\nfsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1707102468451/work\nfuncy==2.0\nfury==0.9.0\nfuture==1.0.0\nfuzzywuzzy==0.18.0\ngast==0.5.4\ngatspy==0.3\ngcsfs==2023.12.2.post1\ngensim==4.3.2\ngeographiclib==2.0\nGeohash==1.0\ngeojson==3.1.0\ngeopandas==0.14.3\ngeoplot==0.5.1\ngeopy==2.4.1\ngeoviews==1.11.1\nggplot @ https://github.com/hbasria/ggpy/archive/0.11.5.zip#sha256=7df947ba3fd86d3757686afec264785ad8df38dc50ffb2d2d31064fb355f69b1\ngiddy==2.3.5\ngitdb==4.0.11\nGitPython==3.1.41\ngoogle-ai-generativelanguage==0.4.0\ngoogle-api-core==2.11.1\ngoogle-api-python-client==2.118.0\ngoogle-apitools==0.5.31\ngoogle-auth==2.26.1\ngoogle-auth-httplib2==0.1.1\ngoogle-auth-oauthlib==1.2.0\ngoogle-cloud-aiplatform==0.6.0a1\ngoogle-cloud-artifact-registry==1.10.0\ngoogle-cloud-automl==1.0.1\ngoogle-cloud-bigquery==2.34.4\ngoogle-cloud-bigtable==1.7.3\ngoogle-cloud-core==2.4.1\ngoogle-cloud-datastore==2.19.0\ngoogle-cloud-dlp==3.14.0\ngoogle-cloud-jupyter-config==0.0.5\ngoogle-cloud-language==2.13.1\ngoogle-cloud-monitoring==2.18.0\ngoogle-cloud-pubsub==2.19.0\ngoogle-cloud-pubsublite==1.9.0\ngoogle-cloud-recommendations-ai==0.7.1\ngoogle-cloud-resource-manager==1.11.0\ngoogle-cloud-spanner==3.40.1\ngoogle-cloud-storage==1.44.0\ngoogle-cloud-translate==3.12.1\ngoogle-cloud-videointelligence==2.13.1\ngoogle-cloud-vision==2.8.0\ngoogle-crc32c==1.5.0\ngoogle-generativeai==0.3.2\ngoogle-pasta==0.2.0\ngoogle-resumable-media==2.7.0\ngoogleapis-common-protos==1.62.0\ngplearn==0.4.2\ngpustat==1.0.0\ngpxpy==1.6.2\ngraphviz==0.20.1\ngreenlet==3.0.3\ngrpc-google-iam-v1==0.12.7\ngrpcio @ file:///home/conda/feedstock_root/build_artifacts/grpc-split_1677499296072/work\ngrpcio-status @ file:///home/conda/feedstock_root/build_artifacts/grpcio-status_1662108958711/work\ngviz-api==1.10.0\ngym==0.26.2\ngym-notices==0.0.8\ngymnasium==0.29.0\nh11==0.14.0\nh2o==3.44.0.3\nh5netcdf==1.3.0\nh5py==3.10.0\nhaversine==2.8.1\nhdfs==2.7.3\nhep-ml==0.7.2\nhijri-converter==2.3.1\nhmmlearn==0.3.0\nholidays==0.24\nholoviews==1.18.3\nhpsklearn==0.1.0\nhtml5lib==1.1\nhtmlmin==0.1.12\nhttpcore==1.0.4\nhttplib2==0.21.0\nhttptools==0.6.1\nhttpx==0.27.0\nhuggingface-hub==0.20.3\nhumanize==4.9.0\nhunspell==0.5.5\nhusl==4.0.3\nhydra-slayer==0.5.0\nhyperopt==0.2.7\nhypertools==0.8.0\nidna @ file:///home/conda/feedstock_root/build_artifacts/idna_1701026962277/work\nigraph==0.11.4\nimagecodecs==2024.1.1\nImageHash==4.3.1\nimageio==2.33.1\nimbalanced-learn==0.12.0\nimgaug==0.4.0\nimportlib-metadata==6.11.0\nimportlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1699364556997/work\ninequality==1.0.1\niniconfig==2.0.0\nipydatawidgets==4.3.5\nipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1703631723894/work\nipyleaflet==0.18.2\nipympl==0.7.0\nipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1704718870316/work\nipython-genutils==0.2.0\nipython-sql==0.5.0\nipyvolume==0.6.3\nipyvue==1.10.1\nipyvuetify==1.8.10\nipywebrtc==0.6.0\nipywidgets==7.7.1\nisoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist\nisort==5.13.2\nisoweek==1.3.3\nitsdangerous==2.1.2\nJanome==0.5.0\njaraco.classes==3.3.0\njax==0.4.23\njax-jumpy==1.0.0\njaxlib @ file:///tmp/jax/jaxlib-0.4.23.dev20240116-cp310-cp310-manylinux2014_x86_64.whl#sha256=2adde6b0fff8a64af0b461e617ac514b80d8ee4aa52f1b1cf9a9139f427be8ba\njedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work\njeepney==0.8.0\njieba==0.42.1\nJinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work\njmespath==1.0.1\njoblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1691577114857/work\njson5==0.9.14\njsonpatch @ file:///home/conda/feedstock_root/build_artifacts/jsonpatch_1695536281965/work\njsonpointer @ file:///home/conda/feedstock_root/build_artifacts/jsonpointer_1695397238043/work\njsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1700159890288/work\njsonschema-specifications @ file:///tmp/tmpkv1z7p57/src\njupyter-console==6.6.3\njupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1699285872613/work\njupyter-http-over-ws==0.0.8\njupyter-lsp==1.5.1\njupyter-server-mathjax==0.2.6\njupyter-ydoc==0.2.5\njupyter_client==7.4.9\njupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1704727030956/work\njupyter_server==2.12.5\njupyter_server_fileid==0.9.1\njupyter_server_proxy==4.1.0\njupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1703611053195/work\njupyter_server_ydoc==0.8.0\njupyterlab==4.1.2\njupyterlab-lsp==5.0.3\njupyterlab-widgets==3.0.9\njupyterlab_git==0.44.0\njupyterlab_pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1700744013163/work\njupyterlab_server==2.25.2\njupytext==1.16.0\nkaggle==1.6.6\nkaggle-environments==1.14.3\nkagglehub==0.1.9\nkeras==3.0.5\nkeras-cv==0.8.2\nkeras-nlp==0.8.1\nkeras-tuner==1.4.6\nkernels-mixer==0.0.7\nkeyring==24.3.0\nkeyrings.google-artifactregistry-auth==1.1.2\nkfp==2.5.0\nkfp-pipeline-spec==0.2.2\nkfp-server-api==2.0.5\nkiwisolver @ file:///home/conda/feedstock_root/build_artifacts/kiwisolver_1695379902431/work\nkmapper==2.0.1\nkmodes==0.12.2\nkorean-lunar-calendar==0.3.1\nkornia==0.7.1\nkt-legacy==1.0.5\nkubernetes==26.1.0\nlangcodes @ file:///home/conda/feedstock_root/build_artifacts/langcodes_1636741340529/work\nlangid==1.1.6\nlazy_loader==0.3\nlearntools @ git+https://github.com/Kaggle/learntools@183cdad0530e7c898cd4658a63b579c54e91f056\nleven==1.0.4\nLevenshtein==0.25.0\nlibclang==16.0.6\nlibmambapy @ file:///home/conda/feedstock_root/build_artifacts/mamba-split_1692866066721/work/libmambapy\nlibpysal==4.9.2\nlibrosa==0.10.1\nlightgbm @ file:///tmp/lightgbm/lightgbm-4.2.0-py3-none-manylinux_2_31_x86_64.whl#sha256=26ed21477c12bb26edc4d6d51336cd43d5a8f7daf55ebbe27b0faf50ce96db23\nlightning-utilities==0.10.1\nlime==0.2.0.1\nline-profiler==4.1.2\nlinkify-it-py==2.0.3\nllvmlite==0.41.1\nlml==0.1.0\nlocket @ file:///home/conda/feedstock_root/build_artifacts/locket_1650660393415/work\nloguru==0.7.2\nLunarCalendar==0.0.9\nlxml==5.1.0\nlz4 @ file:///home/conda/feedstock_root/build_artifacts/lz4_1704831084136/work\nMako==1.3.2\nmamba @ file:///home/conda/feedstock_root/build_artifacts/mamba-split_1692866066721/work/mamba\nmapclassify==2.6.1\nmarisa-trie==1.1.0\nMarkdown==3.5.2\nmarkdown-it-py @ file:///home/conda/feedstock_root/build_artifacts/markdown-it-py_1686175045316/work\nmarkovify==0.9.4\nMarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1695367434228/work\nmarshmallow==3.20.2\nmatplotlib==3.7.5\nmatplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\nmatplotlib-venn==0.11.10\nmccabe==0.7.0\nmdit-py-plugins==0.4.0\nmdurl @ file:///home/conda/feedstock_root/build_artifacts/mdurl_1704317613764/work\nmemory-profiler==0.61.0\nmenuinst @ file:///home/conda/feedstock_root/build_artifacts/menuinst_1702317041727/work\nmercantile==1.2.1\nmgwr==2.2.1\nmissingno==0.5.2\nmistune==0.8.4\nmizani==0.11.0\nml-dtypes==0.2.0\nmlcrate==0.2.0\nmlens==0.2.3\nmlxtend==0.23.1\nmmh3==4.1.0\nmne==1.6.1\nmnist==0.2.2\nmock==5.1.0\nmomepy==0.7.0\nmore-itertools==10.2.0\nmpld3==0.5.10\nmpmath==1.3.0\nmsgpack @ file:///home/conda/feedstock_root/build_artifacts/msgpack-python_1700926504817/work\nmsgpack-numpy==0.4.8\nmultidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1696716075096/work\nmultimethod==1.10\nmultipledispatch==1.0.0\nmultiprocess==0.70.16\nmunkres==1.1.4\nmurmurhash @ file:///home/conda/feedstock_root/build_artifacts/murmurhash_1695449783955/work\nmypy-extensions==1.0.0\nnamex==0.0.7\nnb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1699980974206/work\nnb_conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1704789357480/work\nnbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1683202081046/work\nnbclient==0.5.13\nnbconvert==6.4.5\nnbdime==3.2.0\nnbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work\nndindex==1.8\nnest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1697083700168/work\nnetworkx==3.2.1\nnibabel==5.2.0\nnilearn==0.10.3\nninja==1.11.1.1\nnltk==3.2.4\nnose==1.3.7\nnotebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1680870634737/work\nnotebook_executor @ file:///home/kbuilder/miniconda3/conda-bld/dlenv-tf-2-15-gpu_1704941576253/work/packages/notebook_executor\nnotebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work\nnumba==0.58.1\nnumexpr==2.9.0\nnumpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1707225380409/work/dist/numpy-1.26.4-cp310-cp310-linux_x86_64.whl#sha256=51131fd8fc130cd168aecaf1bc0ea85f92e8ffebf211772ceb16ac2e7f10d7ca\nnvidia-ml-py==11.495.46\nnvtx @ file:///home/conda/feedstock_root/build_artifacts/nvtx_1708093799817/work\noauth2client==4.1.3\noauthlib==3.2.2\nobjsize==0.6.1\nodfpy==1.4.1\nolefile==0.47\nonnx==1.15.0\nopencensus==0.11.4\nopencensus-context==0.1.3\nopencv-contrib-python==4.9.0.80\nopencv-python==4.9.0.80\nopencv-python-headless==4.9.0.80\nopenpyxl==3.1.2\nopenslide-python==1.3.1\nopentelemetry-api==1.22.0\nopentelemetry-exporter-otlp==1.22.0\nopentelemetry-exporter-otlp-proto-common==1.22.0\nopentelemetry-exporter-otlp-proto-grpc==1.22.0\nopentelemetry-exporter-otlp-proto-http==1.22.0\nopentelemetry-proto==1.22.0\nopentelemetry-sdk==1.22.0\nopentelemetry-semantic-conventions==0.43b0\nopt-einsum==3.3.0\noptax==0.1.9\noptuna==3.5.0\norbax-checkpoint==0.5.3\nordered-set==4.1.0\norderedmultidict==1.0.1\norjson==3.9.10\nortools==9.4.1874\nosmnx==1.9.1\noverrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work\npackaging==21.3\npandas==2.1.4\npandas-datareader==0.10.0\npandas-profiling==3.6.6\npandas-summary==0.2.0\npandasql==0.7.3\npandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work\npanel==1.3.8\npapermill==2.5.0\nparam==2.0.2\nparso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\npartd @ file:///home/conda/feedstock_root/build_artifacts/partd_1695667515973/work\npath==16.10.0\npath.py==12.5.0\npathos==0.3.2\npathy @ file:///croot/pathy_1703688110387/work\npatsy==0.5.6\npdf2image==1.17.0\npettingzoo==1.24.0\npexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work\nphik==0.12.4\npickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\nPillow==9.5.0\npkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1694617248815/work\nplatformdirs==4.2.0\nplotly==5.18.0\nplotly-express==0.4.1\nplotnine==0.13.0\npluggy @ file:///home/conda/feedstock_root/build_artifacts/pluggy_1693086607691/work\npointpats==2.4.0\npolars==0.20.10\npolyglot==16.7.4\npooch==1.8.1\npox==0.3.4\nppca==0.0.4\nppft==1.7.6.8\npreprocessing==0.1.13\npreshed @ file:///home/conda/feedstock_root/build_artifacts/preshed_1695644760607/work\nprettytable==3.9.0\nprogressbar2==4.3.2\nprometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1700579315247/work\npromise==2.3\nprompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1702399386289/work\npronouncing==0.2.0\nprophet==1.1.1\nproto-plus @ file:///home/conda/feedstock_root/build_artifacts/proto-plus_1702003338643/work\nprotobuf==3.20.3\npsutil==5.9.3\nptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\npudb==2024.1\nPuLP==2.8.0\npure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\npy-cpuinfo==9.0.0\npy-spy==0.3.14\npy4j==0.10.9.7\npyaml==23.12.0\nPyArabic==0.6.15\npyarrow==11.0.0\npyasn1 @ file:///home/conda/feedstock_root/build_artifacts/pyasn1_1701287008248/work\npyasn1-modules @ file:///home/conda/feedstock_root/build_artifacts/pyasn1-modules_1695107857548/work\nPyAstronomy==0.20.0\npybind11==2.11.1\npyclipper==1.3.0.post5\npycodestyle==2.11.1\npycosat @ file:///home/conda/feedstock_root/build_artifacts/pycosat_1696355758174/work\npycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\npycryptodome==3.20.0\npyct==0.5.0\npycuda==2024.1\npydantic==2.5.3\npydantic_core==2.14.6\npydegensac==0.1.2\npydicom==2.4.4\npydocstyle==6.3.0\npydot==1.4.2\npydub==0.25.1\npyemd==1.0.0\npyerfa==2.0.1.1\npyexcel-io==0.6.6\npyexcel-ods==0.6.0\npyfasttext==0.4.6\npyflakes==3.2.0\npygltflib==1.16.1\nPygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1700607939962/work\nPyJWT==2.8.0\npykalman==0.9.5\npyLDAvis==3.4.1\npylibraft @ file:///opt/conda/conda-bld/work/python/pylibraft\npylint==3.0.3\npymc3==3.11.4\nPyMeeus==0.5.12\npymongo==3.13.0\nPympler==1.0.1\npynndescent==0.5.11\npynvml @ file:///home/conda/feedstock_root/build_artifacts/pynvml_1639061605391/work\npynvrtc==9.2\npyocr==0.8.5\npyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1698795453264/work\npyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1690737849915/work\npypdf==4.0.2\npyproj @ file:///home/conda/feedstock_root/build_artifacts/pyproj_1702028071709/work\npysal==24.1\npyshp @ file:///home/conda/feedstock_root/build_artifacts/pyshp_1659002966020/work\nPySocks @ file:///home/builder/ci_310/pysocks_1640793678128/work\npytesseract==0.3.10\npytest==8.0.1\npython-bidi==0.4.2\npython-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work\npython-dotenv==1.0.0\npython-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work\npython-Levenshtein==0.25.0\npython-louvain==0.16\npython-lsp-jsonrpc==1.1.2\npython-lsp-server==1.10.0\npython-slugify==8.0.4\npython-utils==3.8.2\npythreejs==2.4.2\npytoolconfig==1.3.1\npytools==2023.1.1\npytorch-ignite==0.4.13\npytorch-lightning==2.2.0.post0\npytz==2023.3.post1\npyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work\nPyUpSet==0.1.1.post7\npyviz_comms==3.0.1\nPyWavelets==1.5.0\nPyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1695373428874/work\npyzmq==24.0.1\nqgrid==1.3.1\nqtconsole==5.5.1\nQtPy==2.4.1\nquantecon==0.7.1\nquantities==0.15.0\nqudida==0.0.4\nraft-dask @ file:///opt/conda/conda-bld/work/python/raft-dask\nrapidfuzz==3.6.1\nrasterio==1.3.9\nrasterstats==0.19.0\nray==2.9.0\nray-cpp==2.9.0\nreferencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1704489226496/work\nregex==2023.12.25\nrequests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work\nrequests-oauthlib==1.3.1\nrequests-toolbelt==0.10.1\nresponses==0.18.0\nretrying==1.3.3\nrfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work\nrfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work\nrgf-python==3.12.0\nrich @ file:///home/conda/feedstock_root/build_artifacts/rich-split_1700160075651/work/dist\nrich-click==1.7.3\nrmm @ file:///opt/conda/conda-bld/work/python\nrope==1.12.0\nrpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1703822618592/work\nrsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work\nRtree==1.2.0\nruamel-yaml-conda @ file:///home/builder/ci_310/ruamel_yaml_1640794439226/work\nruamel.yaml @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml_1698138615000/work\nruamel.yaml.clib @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml.clib_1695996839082/work\ns2sphere==0.2.5\ns3fs==2024.2.0\ns3transfer==0.6.2\nsafetensors==0.4.2\nscattertext==0.1.19\nscikit-image==0.22.0\nscikit-learn==1.2.2\nscikit-learn-intelex==2024.1.0\nscikit-multilearn==0.2.0\nscikit-optimize==0.9.0\nscikit-plot==0.3.7\nscikit-surprise==1.1.3\nscipy==1.11.4\nseaborn==0.12.2\nSecretStorage==3.3.3\nsegment_anything @ git+https://github.com/facebookresearch/segment-anything.git@6fdee8f2727f4506cfbbe553e23b895e27956588\nsegregation==2.5\nsemver==3.0.2\nSend2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work\nsentencepiece==0.2.0\nsentry-sdk==1.40.5\nsetproctitle==1.3.3\nsetuptools-git==1.2\nsetuptools-scm==8.0.4\nshap==0.44.1\nShapely==1.8.5.post1\nshellingham @ file:///home/conda/feedstock_root/build_artifacts/shellingham_1698144360966/work\nShimmy==1.3.0\nsimpervisor==1.0.0\nSimpleITK==2.3.1\nsimplejson==3.19.2\nsix @ file:///tmp/build/80754af9/six_1644875935023/work\nsklearn-pandas==2.2.0\nslicer==0.0.7\nsmart-open @ file:///home/conda/feedstock_root/build_artifacts/smart_open_split_1694066705667/work/dist\nsmhasher==0.150.1\nsmmap==5.0.1\nsniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work\nsnowballstemmer==2.2.0\nsnuggs==1.4.7\nsortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work\nsoundfile==0.12.1\nsoupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1693929250441/work\nsoxr==0.3.7\nspacy @ file:///home/conda/feedstock_root/build_artifacts/spacy_1699194962107/work\nspacy-legacy @ file:///home/conda/feedstock_root/build_artifacts/spacy-legacy_1674550301837/work\nspacy-loggers @ file:///home/conda/feedstock_root/build_artifacts/spacy-loggers_1694527114282/work\nspaghetti==1.7.5.post1\nspectral==0.23.1\nspglm==1.1.0\nsphinx-rtd-theme==0.2.4\nspint==1.0.7\nsplot==1.1.5.post1\nspopt==0.6.0\nspreg==1.4.2\nspvcm==0.3.0\nSQLAlchemy==2.0.25\nsqlparse==0.4.4\nsquarify==0.4.3\nsrsly @ file:///home/conda/feedstock_root/build_artifacts/srsly_1695653949688/work\nstable-baselines3==2.1.0\nstack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work\nstanio==0.3.0\nstarlette==0.32.0.post1\nstatsmodels==0.14.1\nstemming==1.0.1\nstop-words==2018.7.23\nstopit==1.1.2\nstumpy==1.12.0\nsympy==1.12\ntables==3.9.2\ntabulate==0.9.0\ntangled-up-in-unicode==0.2.0\ntbb==2021.11.0\ntblib @ file:///home/conda/feedstock_root/build_artifacts/tblib_1702066284995/work\ntenacity==8.2.3\ntensorboard==2.15.1\ntensorboard-data-server==0.7.2\ntensorboard-plugin-profile==2.15.0\ntensorboardX==2.6.2.2\ntensorflow==2.15.0\ntensorflow-cloud==0.1.16\ntensorflow-datasets==4.9.4\ntensorflow-decision-forests==1.8.1\ntensorflow-estimator==2.15.0\ntensorflow-hub==0.16.1\ntensorflow-io==0.35.0\ntensorflow-io-gcs-filesystem==0.35.0\ntensorflow-metadata==0.14.0\ntensorflow-probability==0.23.0\ntensorflow-serving-api==2.14.1\ntensorflow-text==2.15.0\ntensorflow-transform==0.14.0\ntensorpack==0.11\ntensorstore==0.1.53\ntermcolor==2.4.0\nterminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1699810101464/work\ntestpath==0.6.0\ntext-unidecode==1.3\ntextblob==0.18.0.post0\ntexttable==1.7.0\ntf-keras==2.15.0\ntfp-nightly @ git+https://github.com/tensorflow/probability.git@fbc5ebe9b1d343113fb917010096cfd88b32eecf\nTheano==1.0.5\nTheano-PyMC==1.1.2\nthinc @ file:///home/conda/feedstock_root/build_artifacts/thinc_1703842165913/work\nthreadpoolctl==3.2.0\ntifffile==2023.12.9\ntimm==0.9.16\ntinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work\ntobler==0.11.2\ntokenizers==0.15.2\ntoml==0.10.2\ntomli==2.0.1\ntomlkit==0.12.3\ntoolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1706112571092/work\ntorch @ file:///tmp/torch/torch-2.1.2-cp310-cp310-linux_x86_64.whl#sha256=ae3259980b8d6551608b32fde2695baca64c72ed15ab2332023a248c113815a8\ntorchaudio @ file:///tmp/torch/torchaudio-2.1.2-cp310-cp310-linux_x86_64.whl#sha256=10966b20361b49bc41b6c6ba842d3ea842320fb8c589823b4120f24a98013b4a\ntorchdata==0.7.1\ntorchinfo==1.8.0\ntorchmetrics==1.3.1\ntorchtext @ file:///tmp/torch/torchtext-0.16.2-cp310-cp310-linux_x86_64.whl#sha256=a2a382655a08e1f6eeab6a307d0c8d78139cfa04cc329a7dc15a3f7c1e6e7a19\ntorchvision @ file:///tmp/torch/torchvision-0.16.2-cp310-cp310-linux_x86_64.whl#sha256=105901a20924f652ee62df0bb57580c67725eb21f11a349658952c4be2050d94\ntornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1695373560918/work\nTPOT==0.12.1\ntqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1691671248568/work\ntraceml==1.0.8\ntraitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work\ntraittypes==0.2.1\ntransformers==4.38.1\ntreelite==3.2.0\ntreelite-runtime==3.2.0\ntrueskill==0.4.5\ntruststore @ file:///home/conda/feedstock_root/build_artifacts/truststore_1694154605758/work\ntrx-python==0.2.9\ntsfresh==0.20.2\ntypeguard==4.1.5\ntyper @ file:///home/conda/feedstock_root/build_artifacts/typer_1683029246636/work\ntypes-python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/types-python-dateutil_1704512562698/work\ntyping-inspect==0.9.0\ntyping-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work\ntyping_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1702176139754/work\ntzdata==2023.4\ntzlocal==5.2\nuc-micro-py==1.0.3\nucx-py @ file:///opt/conda/conda-bld/work\nujson==5.9.0\numap-learn==0.5.5\nunicodedata2 @ file:///home/conda/feedstock_root/build_artifacts/unicodedata2_1695847980273/work\nUnidecode==1.3.8\nupdate-checker==0.18.0\nuri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist\nuritemplate==3.0.1\nurllib3==1.26.18\nurwid==2.6.4\nurwid_readline==0.13\nuvicorn==0.25.0\nuvloop==0.19.0\nvaex==4.17.0\nvaex-astro==0.9.3\nvaex-core==4.17.1\nvaex-hdf5==0.14.1\nvaex-jupyter==0.8.2\nvaex-ml==0.18.3\nvaex-server==0.9.0\nvaex-viz==0.5.4\nvec_noise==1.1.4\nvecstack==0.4.0\nvirtualenv==20.21.0\nvisions==0.7.5\nvowpalwabbit==9.9.0\nvtk==9.3.0\nWand==0.6.13\nwandb==0.16.3\nwasabi @ file:///home/conda/feedstock_root/build_artifacts/wasabi_1686131297168/work\nwatchfiles==0.21.0\nwavio==0.0.8\nwcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1704731205417/work\nweasel @ file:///home/conda/feedstock_root/build_artifacts/weasel_1699295455892/work\nwebcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work\nwebencodings @ file:///home/conda/feedstock_root/build_artifacts/webencodings_1694681268211/work\nwebsocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1701630677416/work\nwebsockets==12.0\nWerkzeug==3.0.1\nwfdb==4.1.2\nwhatthepatch==1.0.5\nwidgetsnbextension==3.6.6\nwitwidget==1.8.1\nwoodwork==0.28.0\nwordcloud==1.9.3\nwordsegment==1.3.1\nwrapt==1.14.1\nxarray==2024.2.0\nxarray-einstats==0.7.0\nxgboost==2.0.3\nxvfbwrapper==0.2.9\nxxhash==3.4.1\nxyzservices @ file:///home/conda/feedstock_root/build_artifacts/xyzservices_1698325309404/work\ny-py==0.6.2\nyapf==0.40.2\nyarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1701168553642/work\nydata-profiling==4.6.4\nyellowbrick==1.5\nypy-websocket==0.8.4\nzict @ file:///home/conda/feedstock_root/build_artifacts/zict_1681770155528/work\nzipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work\nzstandard==0.22.0\n","output_type":"stream"}]},{"cell_type":"code","source":"######################################\n### CREATE CLUTTERED MNIST DATASET ###\n######################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Transformation classes\n\nclass RandomPlacement(object):\n  def __call__(self, img):\n    canvas = torch.zeros(1,100,100)\n    x = torch.randint(0, 73, (1,))\n    y = torch.randint(0, 73, (1,))\n    canvas[:, x:x+28, y:y+28] = img\n\n    return canvas\n\nclass RandomCropAndCombine(object):\n  def __init__(self, dataset):\n    self.dataset = dataset\n\n  def __call__(self, canvas):\n    for _ in range(8):\n      img, _ = random.choice(self.dataset)\n      img = transforms.ToTensor()(img)\n      x = torch.randint(0, 91, (1,))\n      y = torch.randint(0, 91, (1,))\n      patch = transforms.RandomCrop((9,9))(img)\n      canvas[:, x:x+9, y:y+9] += patch\n      canvas = canvas.clamp(0, 1)\n\n    return canvas","metadata":{"executionInfo":{"elapsed":227,"status":"ok","timestamp":1711009738485,"user":{"displayName":"Tobias Jedlicka","userId":"10721733797122281021"},"user_tz":-60},"id":"YM6ay7S8Vu6-","execution":{"iopub.status.busy":"2024-06-26T07:27:36.739562Z","iopub.execute_input":"2024-06-26T07:27:36.740017Z","iopub.status.idle":"2024-06-26T07:27:36.748889Z","shell.execute_reply.started":"2024-06-26T07:27:36.739987Z","shell.execute_reply":"2024-06-26T07:27:36.747989Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create cluttered MNIST dataset, split datasets and create DataLoaders.\n\n# Set seed for reproducability\ntorch.manual_seed(1)\nnp.random.seed(1)\ngenerator = torch.Generator().manual_seed(1)\n\n# Set batch size parameter\nBATCH_SIZE = 128\n\n# Define data transformations\noriginal_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\nmnist_original = datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/', train=True, download=True, transform=None)\naugmented_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,)),\n    RandomPlacement(),\n    RandomCropAndCombine(mnist_original),\n])\n\n# Train datasets\nmnist_dataset_train = datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/', train=True, download=True, transform=original_transform)\naugmented_dataset_train = datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/', train=True, download=True, transform=augmented_transform)\n\n# Test datasets\naugmented_dataset_test = datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/', train=False, download=True, transform=augmented_transform)\nmnist_dataset_test = datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/', train=False, download=True, transform=original_transform)\n\n# Split train dataset into 90% train and 10% validation data\nmnist_train, mnist_val = torch.utils.data.random_split(dataset=mnist_dataset_train, lengths=[0.9, 0.1], generator=generator)\naugmented_train, augmented_val = torch.utils.data.random_split(dataset=augmented_dataset_train, lengths=[0.9, 0.1], generator=generator)\n\n# Create data loaders for original and augmented datasets\n\n# Load train data\nmnist_train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=BATCH_SIZE, shuffle=True)\ntrain_loader = torch.utils.data.DataLoader(dataset=augmented_train, batch_size=BATCH_SIZE, shuffle=True)\n# Load validation data\nmnist_val_loader = torch.utils.data.DataLoader(dataset=mnist_val, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = torch.utils.data.DataLoader(dataset=augmented_val, batch_size=BATCH_SIZE, shuffle=True)\n# Load test data\ntest_loader = torch.utils.data.DataLoader(dataset=augmented_dataset_test, batch_size=BATCH_SIZE, shuffle=False)\nmnist_test_loader = torch.utils.data.DataLoader(dataset=mnist_dataset_test, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:27:38.767654Z","iopub.execute_input":"2024-06-26T07:27:38.768008Z","iopub.status.idle":"2024-06-26T07:27:42.878694Z","shell.execute_reply.started":"2024-06-26T07:27:38.767981Z","shell.execute_reply":"2024-06-26T07:27:42.877706Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 15807816.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/train-images-idx3-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 460478.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/train-labels-idx1-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 3780214.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 2048664.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/drive/MyDrive/Colab Notebooks/Masterarbeit - Coding/data/MNIST/raw\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Plot the original and the augmented dataset\nfig, axes = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(4):\n  original_img, _ = mnist_original[i]\n  original_img = transforms.ToTensor()(original_img).squeeze(0)\n  axes[0, i].imshow(original_img, cmap='gray')\n  axes[0, i].axis('off')\n  axes[0, i].set_title('Original')\n\n  augmented_img, _ = augmented_dataset_train[i]\n  augmented_img = augmented_img.squeeze(0)\n  axes[1, i].imshow(augmented_img, cmap='gray')\n  axes[1, i].axis('off')\n  axes[1, i].set_title('Transformed')\n    \nplt.tight_layout()\nplt.show()","metadata":{"executionInfo":{"elapsed":822,"status":"ok","timestamp":1711014574594,"user":{"displayName":"Tobias Jedlicka","userId":"10721733797122281021"},"user_tz":-60},"id":"kGbbSYiscR-9","outputId":"94d74291-72ba-4227-c66c-289378c48520","execution":{"iopub.status.busy":"2024-06-26T07:27:42.880304Z","iopub.execute_input":"2024-06-26T07:27:42.880626Z","iopub.status.idle":"2024-06-26T07:27:43.708009Z","shell.execute_reply.started":"2024-06-26T07:27:42.880601Z","shell.execute_reply":"2024-06-26T07:27:43.707097Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 8 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"###############################################################\n### Helper Trainer Class to faciliate training the networks ###\n###############################################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer():\n    '''\n    Trainer class containing multiple helper functions for training.\n    '''\n    def __init__(self, model, optimizer, train_loader, val_loader, test_loader,\n                 num_epochs, estimate_step=100, save_checkpoints=True, path='model'):\n        \n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n        self.criterion = nn.CrossEntropyLoss()\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.test_loader = test_loader\n        self.num_epochs = num_epochs\n        self.estimate_step = estimate_step\n        self.model_name = type(self.model).__name__\n        self.path = path\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.save_checkpoints = save_checkpoints\n        self.running_loss_train = []\n        self.running_loss_val = []\n        self.train_time_per_epoch = []\n        self.total_epochs_trained = 0\n        self.test_accuracy = None\n        self.inference_time = None\n\n    \n    def train(self):\n        '''\n        Function to train the model. Trains the model on a training dataset and evaluates the current performance\n        on a validation dataset at the end of each epoch. Reduces the learning rate, if there is no\n        improvement in the validation loss for 10 epochs. \n        '''\n        self.model.to(self.device)\n        self.model.train()\n        for epoch in range(self.num_epochs):\n            # Start timer\n            #torch.cuda.synchronize() if self.device == 'cuda' else None\n            start_epoch = time.time()\n            \n            # Training process for one epoch\n            self.model.train(True)\n            running_loss_train = 0.00\n            for batch_idx, (data, target) in enumerate(self.train_loader):\n                data, target = data.to(self.device), target.to(self.device)\n                self.optimizer.zero_grad(set_to_none=True)\n                output = self.model(data)\n                loss = self.criterion(output, target)\n                running_loss_train += loss.item()\n                loss.backward()\n                self.optimizer.step()\n                if batch_idx % self.estimate_step == 0:\n                    print(f'Epoch {epoch+1}/{self.num_epochs}, Batch {batch_idx}/{len(self.train_loader)}, Train Loss: {loss.item():.4f}')     \n            avg_loss_train = running_loss_train / len(self.train_loader)\n            self.running_loss_train.append(avg_loss_train)\n            \n            # Validate model at the end of each epoch\n            self.model.eval()\n            running_loss_val = 0.00\n            with torch.no_grad():\n                for i, (data_val, target_val) in enumerate(self.val_loader):\n                    data_val, target_val = data_val.to(self.device), target_val.to(self.device)\n                    output_val = self.model(data_val)\n                    loss_val = self.criterion(output_val, target_val)\n                    running_loss_val += loss_val.item()\n                avg_loss_val = running_loss_val / len(self.val_loader)\n                self.running_loss_val.append(avg_loss_val)\n            \n            # Recuce LR if model does not improve for 10 epochs\n            self.scheduler.step(avg_loss_val)\n            \n            # Track training time for each epoch\n            #torch.cuda.synchronize() if self.device == 'cuda' else None\n            end_epoch = time.time()\n            self.train_time_per_epoch.append(end_epoch-start_epoch)\n            print(f'Model {self.model_name} at epoch {epoch+1}/{self.num_epochs}: Avg Train Loss: {avg_loss_train:.4f}, Avg Val Loss: {avg_loss_val:.4f}')\n            \n            # Increase epoch counter\n            self.total_epochs_trained += 1\n            \n            # Save checkpoint at the end of each epoch\n            if self.save_checkpoints:\n                self.save_checkpoint(self.path)\n            \n        # Print total training time at the end\n        train_time = \"{:.2f} minutes\".format(sum(self.train_time_per_epoch) / 60)\n        print(f'Model {self.model_name} took {train_time} to run on {self.total_epochs_trained} epochs.')\n        \n                \n    def eval(self):\n        '''\n        Function to evaluate the performance of the model.\n        Therefore, the models accuracy on a test dataset is measured.\n        '''\n        self.model.to(self.device)\n        self.model.eval()\n        correct = 0\n        total = 0\n        \n        #torch.cuda.synchronize() if self.device == 'cuda' else None\n        start_eval = time.time()\n        with torch.no_grad():\n            for data, target in self.test_loader:\n                data, target = data.to(self.device), target.to(self.device)\n                output = self.model(data)\n                _, predicted = torch.max(output.data, 1)\n                total += target.size(0)\n                correct += (predicted == target).sum().item()\n        self.test_accuracy = correct / total\n        \n        #torch.cuda.synchronize() if self.device == 'cuda' else None\n        end_eval = time.time()\n        self.inference_time = end_eval - start_eval\n        print(f'Test Accuracy for {self.model_name}: {self.test_accuracy:.4f} - in {self.inference_time} seconds.')\n        # Save checkpoint after inference.\n        if self.save_checkpoints:\n                self.save_checkpoint(self.path)\n    def save_checkpoint(self, path):\n        '''\n        Function to save the current state of the model, optimizer and scheduler.\n        Also saves the model metrics training loss, validation loss, accuracy,\n        training and inference time.\n        '''\n        torch.save({'epoch': self.total_epochs_trained,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'scheduler_state_dict': self.scheduler.state_dict(),\n                    'loss': self.running_loss_train[-1],\n                    'running_loss_train': self.running_loss_train,\n                    'running_loss_val': self.running_loss_val,\n                    'test_accuracy': self.test_accuracy,\n                    'train_time': self.train_time_per_epoch,\n                    'inference_time': self.inference_time}, \n                    f'{path}.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:33:00.779723Z","iopub.execute_input":"2024-06-26T07:33:00.780538Z","iopub.status.idle":"2024-06-26T07:33:00.815057Z","shell.execute_reply.started":"2024-06-26T07:33:00.780489Z","shell.execute_reply":"2024-06-26T07:33:00.814144Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"###############################################\n### CNN model for MNIST and cluttered MNIST ###\n###############################################\n# NOTE: Only difference if the models is the linear layer fc1. Input features needs adjustment as the size of the input images is different.","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:33:02.153591Z","iopub.execute_input":"2024-06-26T07:33:02.154388Z","iopub.status.idle":"2024-06-26T07:33:02.158641Z","shell.execute_reply.started":"2024-06-26T07:33:02.154355Z","shell.execute_reply":"2024-06-26T07:33:02.157578Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"###############################\n####### CNN Model MNIST #######\n###############################\nclass CNN_MNIST(nn.Module):\n    def __init__(self, num_classes=10, image_channels=1):\n        super(CNN_MNIST, self).__init__()\n        self.conv1 = nn.Conv2d(image_channels, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(9216, 128)\n        self.dropout = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:35:53.480905Z","iopub.execute_input":"2024-06-26T09:35:53.481713Z","iopub.status.idle":"2024-06-26T09:35:53.489596Z","shell.execute_reply.started":"2024-06-26T09:35:53.481682Z","shell.execute_reply":"2024-06-26T09:35:53.488705Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"#################################\n####### CNN Model Clutter #######\n#################################\nclass CNN(nn.Module):\n    def __init__(self, num_classes=10, image_channels=1):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(image_channels, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(147456, 128)\n        self.dropout = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x","metadata":{"id":"Wdk5Gp7tlc4s","executionInfo":{"status":"ok","timestamp":1711019808528,"user_tz":-60,"elapsed":1920959,"user":{"displayName":"Tobias Jedlicka","userId":"10721733797122281021"}},"outputId":"357f0e45-2497-42e1-bb5c-1f23c3ae1434","execution":{"iopub.status.busy":"2024-06-26T09:35:55.173733Z","iopub.execute_input":"2024-06-26T09:35:55.174573Z","iopub.status.idle":"2024-06-26T09:35:55.182599Z","shell.execute_reply.started":"2024-06-26T09:35:55.174533Z","shell.execute_reply":"2024-06-26T09:35:55.181478Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"###############################################\n### VAN model for MNIST and cluttered MNIST ###\n###############################################","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:33:05.324034Z","iopub.execute_input":"2024-06-26T07:33:05.324781Z","iopub.status.idle":"2024-06-26T07:33:05.328748Z","shell.execute_reply.started":"2024-06-26T07:33:05.324749Z","shell.execute_reply":"2024-06-26T07:33:05.327672Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"###############################\n####### VAN Model MNIST #######\n###############################\nclass Downsampling_MNIST(nn.Module):\n  def __init__(self, in_channels, out_channels, kernel_size, stride):\n    super().__init__()\n\n    self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n\n  def forward(self, x):\n\n    x = self.conv(x)\n\n    return x\n\n\nclass Block_MNIST(nn.Module):\n  def __init__(self, channels, expansion_ratio, dropout):\n    super().__init__()\n\n    self.batch_norm1 = nn.BatchNorm2d(channels) \n    self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels ,kernel_size=1)\n    self.act1 = nn.GELU()\n    self.LKA = LKA_MNIST(in_channels=channels, out_channels=channels)\n    self.batch_norm2 = nn.BatchNorm2d(channels)\n    self.FFN = FFN_MNIST(channels=channels, expansion_ratio=expansion_ratio, dropout=dropout)\n\n  def forward(self, x):\n    x = self.batch_norm1(x)\n    x = self.conv1(x)\n    x = self.act1(x)\n    x = self.LKA(x)\n    x = self.batch_norm2(x)\n    x = self.FFN(x)\n\n    return x\n\n\nclass FFN_MNIST(nn.Module):\n  def __init__(self, channels, expansion_ratio, dropout):\n    super().__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels*expansion_ratio, kernel_size=1)\n    self.conv2 = nn.Conv2d(in_channels=channels*expansion_ratio, out_channels=channels*expansion_ratio,\n                           kernel_size=3, stride=1, padding=1, groups=channels*expansion_ratio)  #DW3x3Conv\n    self.act1 = nn.GELU()\n    self.conv3 = nn.Conv2d(in_channels=channels*expansion_ratio, out_channels=channels, kernel_size=1)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.act1(x)\n    x = self.conv3(x)\n    x = self.dropout(x)\n\n    return x\n\n\nclass LKA_MNIST(nn.Module):\n  def __init__(self, in_channels, out_channels, k=1):\n    super().__init__()\n    '''\n    When groups == in_channels and out_channels == K * in_channels,\n    where K is a positive integer, this operation is also known as a “depthwise convolution”.\n    '''\n    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels*k, kernel_size=5, groups=in_channels)#DW-Conv\n    self.conv2 = nn.Conv2d(in_channels=in_channels*k, out_channels=in_channels*k, kernel_size=5,\n                           dilation=3, groups=in_channels, padding=8)#DW-D-Conv\n    self.conv3 = nn.Conv2d(in_channels=in_channels*k, out_channels=out_channels, kernel_size=1) #1x1 Conv\n\n  def forward(self, x):\n    input = x.clone()\n    attn = self.conv1(x)\n    attn = self.conv2(attn)\n    attn = self.conv3(attn)\n\n    return input * attn\n\n\nclass VAN_MNIST(nn.Module):\n  def __init__(self, num_classes=10, stages=4, l=[2, 2, 1, 1], channels=[32, 64, 128, 256],\n               expansion_ratio=[2, 2, 2, 2], image_channels=1, dropout=0.5):\n    super().__init__()\n    '''\n    The block and downsampler need to be initialized within the __init__ method in order to\n    determine the number of parameters correctly. This also makes the moving to device easier,\n    as moving the whole model will move the blocks and downsamplers aswell.\n    '''\n    self.stages = stages\n    self.channels = channels\n    self.expansion_ratio = expansion_ratio\n    self.l = l\n    self.num_classes = num_classes\n    self.classifier = nn.Linear(in_features=channels[-1], out_features=num_classes)\n\n    for j in range(self.stages):\n      downsampler = Downsampling_MNIST(in_channels=image_channels if j == 0 else self.channels[j-1],\n                                 out_channels=self.channels[j],\n                                 kernel_size=3, stride=2)\n      block = nn.ModuleList([Block_MNIST(channels=self.channels[j],\n                                   expansion_ratio=self.expansion_ratio[j],\n                                   dropout=dropout)\n                                   for _ in range(self.l[j])])\n\n      setattr(self, f'downsampler_{j+1}', downsampler)\n      setattr(self, f'block_{j+1}', block)\n\n  def forward(self, x):\n\n    for j in range(self.stages):\n\n      downsampler = getattr(self, f'downsampler_{j+1}')\n      block = getattr(self, f'block_{j+1}')\n      x = downsampler(x)\n      for blk in block:\n        x = blk(x)\n\n    x = x.flatten(2).transpose(1, 2)\n    x = x.mean(dim=1)\n    x = self.classifier(x)\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:43:59.829072Z","iopub.execute_input":"2024-06-26T09:43:59.829397Z","iopub.status.idle":"2024-06-26T09:43:59.853612Z","shell.execute_reply.started":"2024-06-26T09:43:59.829371Z","shell.execute_reply":"2024-06-26T09:43:59.852547Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"#################################\n####### VAN Model Clutter #######\n#################################\nclass Downsampling(nn.Module):\n  def __init__(self, in_channels, out_channels, kernel_size, stride):\n    super().__init__()\n\n    self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n\n  def forward(self, x):\n\n    x = self.conv(x)\n\n    return x\n\n\nclass Block(nn.Module):\n  def __init__(self, channels, expansion_ratio, dropout):\n    super().__init__()\n\n    self.batch_norm1 = nn.BatchNorm2d(channels) \n    self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels ,kernel_size=1)\n    self.act1 = nn.GELU()\n    self.LKA = LKA(in_channels=channels, out_channels=channels)\n    self.batch_norm2 = nn.BatchNorm2d(channels)\n    self.FFN = FFN(channels=channels, expansion_ratio=expansion_ratio, dropout=dropout)\n\n  def forward(self, x):\n    x = self.batch_norm1(x)\n    x = self.conv1(x)\n    x = self.act1(x)\n    x = self.LKA(x)\n    x = self.batch_norm2(x)\n    x = self.FFN(x)\n\n    return x\n\n\nclass FFN(nn.Module):\n  def __init__(self, channels, expansion_ratio, dropout):\n    super().__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels*expansion_ratio, kernel_size=1)\n    self.conv2 = nn.Conv2d(in_channels=channels*expansion_ratio, out_channels=channels*expansion_ratio,\n                           kernel_size=3, stride=1, padding=1, groups=channels*expansion_ratio)  #DW-3x3 Conv\n    self.act1 = nn.GELU()\n    self.conv3 = nn.Conv2d(in_channels=channels*expansion_ratio, out_channels=channels, kernel_size=1)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.act1(x)\n    x = self.conv3(x)\n    x = self.dropout(x)\n\n    return x\n\n\nclass LKA(nn.Module):\n  def __init__(self, in_channels, out_channels, k=1):\n    super().__init__()\n    '''\n    When groups == in_channels and out_channels == K * in_channels,\n    where K is a positive integer, this operation is also known as a “depthwise convolution”.\n    '''\n    \n    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels*k, kernel_size=5, groups=in_channels)#DW-Conv\n    self.conv2 = nn.Conv2d(in_channels=in_channels*k, out_channels=in_channels*k, kernel_size=5,\n                           dilation=3, groups=in_channels, padding=8)#DW-D-Conv\n    self.conv3 = nn.Conv2d(in_channels=in_channels*k, out_channels=out_channels, kernel_size=1) #1x1 Conv\n\n  def forward(self, x):\n    input = x.clone()\n    attn = self.conv1(x)\n    attn = self.conv2(attn)\n    attn = self.conv3(attn)\n\n    return input * attn\n\n\nclass VAN(nn.Module):\n  def __init__(self, num_classes=10, stages=4, l=[2, 2, 1, 1], channels=[32, 64, 128, 256],\n               expansion_ratio=[2, 2, 2, 2], image_channels=1, dropout=0.5):\n    super().__init__()\n    '''\n    The block and downsampler need to be initialized within the __init__ method in order to\n    determine the number of parameters correctly. This also makes the moving to device easier,\n    as moving the whole model will move the blocks and downsamplers aswell.\n    '''\n    self.stages = stages\n    self.channels = channels\n    self.expansion_ratio = expansion_ratio\n    self.l = l\n    self.num_classes = num_classes\n    self.classifier = nn.Linear(in_features=channels[-1], out_features=num_classes)\n\n    for j in range(self.stages):\n      downsampler = Downsampling(in_channels=image_channels if j == 0 else self.channels[j-1],\n                                 out_channels=self.channels[j],\n                                 kernel_size=3, stride=2)\n      block = nn.ModuleList([Block(channels=self.channels[j],\n                                   expansion_ratio=self.expansion_ratio[j],\n                                   dropout=dropout)\n                                   for _ in range(self.l[j])])\n\n      setattr(self, f'downsampler_{j+1}', downsampler)\n      setattr(self, f'block_{j+1}', block)\n\n  def forward(self, x):\n\n    for j in range(self.stages):\n\n      downsampler = getattr(self, f'downsampler_{j+1}')\n      block = getattr(self, f'block_{j+1}')\n      x = downsampler(x)\n      for blk in block:\n        x = blk(x)\n\n    x = x.flatten(2).transpose(1, 2)\n    x = x.mean(dim=1)\n    x = self.classifier(x)\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:43:58.124415Z","iopub.execute_input":"2024-06-26T09:43:58.125241Z","iopub.status.idle":"2024-06-26T09:43:58.148775Z","shell.execute_reply.started":"2024-06-26T09:43:58.125211Z","shell.execute_reply":"2024-06-26T09:43:58.147807Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING PROCESS DONE. DO NOT TOUCH ANY OF THE TRAINING LINES","metadata":{}},{"cell_type":"code","source":"################################\n### TRAINING OF THE NETWORKS ###\n################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################\n### CNN Training ###\n####################","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:01:23.701726Z","iopub.execute_input":"2024-06-26T10:01:23.702091Z","iopub.status.idle":"2024-06-26T10:01:23.706071Z","shell.execute_reply.started":"2024-06-26T10:01:23.702063Z","shell.execute_reply":"2024-06-26T10:01:23.705146Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"# Initialize CNN on MNIST\ncnn_mnist = CNN_MNIST()\ncnn_mnist_optimizer = optim.AdamW(cnn_mnist.parameters(), lr=1e-3)\ncnn_mnist_trainer = Trainer(model=cnn_mnist, optimizer=cnn_mnist_optimizer,\n                            train_loader=mnist_train_loader, val_loader=mnist_val_loader, test_loader=mnist_test_loader,\n                            num_epochs=25, save_checkpoints=True, path=f'CNN_MNIST()')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:59:57.960648Z","iopub.execute_input":"2024-06-26T09:59:57.961017Z","iopub.status.idle":"2024-06-26T09:59:57.979682Z","shell.execute_reply.started":"2024-06-26T09:59:57.960988Z","shell.execute_reply":"2024-06-26T09:59:57.978852Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"# Train CNN on MNIST\ncnn_mnist_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:36:04.602241Z","iopub.execute_input":"2024-06-26T09:36:04.602675Z","iopub.status.idle":"2024-06-26T09:41:35.421734Z","shell.execute_reply.started":"2024-06-26T09:36:04.602639Z","shell.execute_reply":"2024-06-26T09:41:35.420567Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"Epoch 1/25, Batch 0/422, Train Loss: 2.2912\nEpoch 1/25, Batch 100/422, Train Loss: 0.1524\nEpoch 1/25, Batch 200/422, Train Loss: 0.0506\nEpoch 1/25, Batch 300/422, Train Loss: 0.1027\nEpoch 1/25, Batch 400/422, Train Loss: 0.1112\nModel CNN_MNIST at epoch 1/25: Avg Train Loss: 0.2305, Avg Val Loss: 0.0511\nEpoch 2/25, Batch 0/422, Train Loss: 0.0475\nEpoch 2/25, Batch 100/422, Train Loss: 0.1308\nEpoch 2/25, Batch 200/422, Train Loss: 0.0907\nEpoch 2/25, Batch 300/422, Train Loss: 0.0455\nEpoch 2/25, Batch 400/422, Train Loss: 0.0501\nModel CNN_MNIST at epoch 2/25: Avg Train Loss: 0.0827, Avg Val Loss: 0.0396\nEpoch 3/25, Batch 0/422, Train Loss: 0.0398\nEpoch 3/25, Batch 100/422, Train Loss: 0.0854\nEpoch 3/25, Batch 200/422, Train Loss: 0.0420\nEpoch 3/25, Batch 300/422, Train Loss: 0.0608\nEpoch 3/25, Batch 400/422, Train Loss: 0.0717\nModel CNN_MNIST at epoch 3/25: Avg Train Loss: 0.0612, Avg Val Loss: 0.0315\nEpoch 4/25, Batch 0/422, Train Loss: 0.0229\nEpoch 4/25, Batch 100/422, Train Loss: 0.0346\nEpoch 4/25, Batch 200/422, Train Loss: 0.0791\nEpoch 4/25, Batch 300/422, Train Loss: 0.0272\nEpoch 4/25, Batch 400/422, Train Loss: 0.0629\nModel CNN_MNIST at epoch 4/25: Avg Train Loss: 0.0487, Avg Val Loss: 0.0327\nEpoch 5/25, Batch 0/422, Train Loss: 0.0187\nEpoch 5/25, Batch 100/422, Train Loss: 0.0124\nEpoch 5/25, Batch 200/422, Train Loss: 0.0308\nEpoch 5/25, Batch 300/422, Train Loss: 0.0106\nEpoch 5/25, Batch 400/422, Train Loss: 0.1296\nModel CNN_MNIST at epoch 5/25: Avg Train Loss: 0.0427, Avg Val Loss: 0.0299\nEpoch 6/25, Batch 0/422, Train Loss: 0.0184\nEpoch 6/25, Batch 100/422, Train Loss: 0.0171\nEpoch 6/25, Batch 200/422, Train Loss: 0.0399\nEpoch 6/25, Batch 300/422, Train Loss: 0.0927\nEpoch 6/25, Batch 400/422, Train Loss: 0.0246\nModel CNN_MNIST at epoch 6/25: Avg Train Loss: 0.0369, Avg Val Loss: 0.0348\nEpoch 7/25, Batch 0/422, Train Loss: 0.0372\nEpoch 7/25, Batch 100/422, Train Loss: 0.0188\nEpoch 7/25, Batch 200/422, Train Loss: 0.0238\nEpoch 7/25, Batch 300/422, Train Loss: 0.0293\nEpoch 7/25, Batch 400/422, Train Loss: 0.0219\nModel CNN_MNIST at epoch 7/25: Avg Train Loss: 0.0320, Avg Val Loss: 0.0304\nEpoch 8/25, Batch 0/422, Train Loss: 0.0266\nEpoch 8/25, Batch 100/422, Train Loss: 0.0321\nEpoch 8/25, Batch 200/422, Train Loss: 0.0531\nEpoch 8/25, Batch 300/422, Train Loss: 0.0240\nEpoch 8/25, Batch 400/422, Train Loss: 0.0257\nModel CNN_MNIST at epoch 8/25: Avg Train Loss: 0.0259, Avg Val Loss: 0.0310\nEpoch 9/25, Batch 0/422, Train Loss: 0.0215\nEpoch 9/25, Batch 100/422, Train Loss: 0.0725\nEpoch 9/25, Batch 200/422, Train Loss: 0.0244\nEpoch 9/25, Batch 300/422, Train Loss: 0.0471\nEpoch 9/25, Batch 400/422, Train Loss: 0.0474\nModel CNN_MNIST at epoch 9/25: Avg Train Loss: 0.0264, Avg Val Loss: 0.0307\nEpoch 10/25, Batch 0/422, Train Loss: 0.0087\nEpoch 10/25, Batch 100/422, Train Loss: 0.0276\nEpoch 10/25, Batch 200/422, Train Loss: 0.0017\nEpoch 10/25, Batch 300/422, Train Loss: 0.0023\nEpoch 10/25, Batch 400/422, Train Loss: 0.0135\nModel CNN_MNIST at epoch 10/25: Avg Train Loss: 0.0226, Avg Val Loss: 0.0334\nEpoch 11/25, Batch 0/422, Train Loss: 0.0009\nEpoch 11/25, Batch 100/422, Train Loss: 0.0058\nEpoch 11/25, Batch 200/422, Train Loss: 0.0083\nEpoch 11/25, Batch 300/422, Train Loss: 0.0188\nEpoch 11/25, Batch 400/422, Train Loss: 0.0372\nModel CNN_MNIST at epoch 11/25: Avg Train Loss: 0.0196, Avg Val Loss: 0.0344\nEpoch 12/25, Batch 0/422, Train Loss: 0.0156\nEpoch 12/25, Batch 100/422, Train Loss: 0.0078\nEpoch 12/25, Batch 200/422, Train Loss: 0.0255\nEpoch 12/25, Batch 300/422, Train Loss: 0.0118\nEpoch 12/25, Batch 400/422, Train Loss: 0.0057\nModel CNN_MNIST at epoch 12/25: Avg Train Loss: 0.0195, Avg Val Loss: 0.0327\nEpoch 13/25, Batch 0/422, Train Loss: 0.0226\nEpoch 13/25, Batch 100/422, Train Loss: 0.0099\nEpoch 13/25, Batch 200/422, Train Loss: 0.0098\nEpoch 13/25, Batch 300/422, Train Loss: 0.0053\nEpoch 13/25, Batch 400/422, Train Loss: 0.0171\nModel CNN_MNIST at epoch 13/25: Avg Train Loss: 0.0178, Avg Val Loss: 0.0295\nEpoch 14/25, Batch 0/422, Train Loss: 0.0139\nEpoch 14/25, Batch 100/422, Train Loss: 0.0218\nEpoch 14/25, Batch 200/422, Train Loss: 0.0066\nEpoch 14/25, Batch 300/422, Train Loss: 0.0080\nEpoch 14/25, Batch 400/422, Train Loss: 0.0031\nModel CNN_MNIST at epoch 14/25: Avg Train Loss: 0.0158, Avg Val Loss: 0.0329\nEpoch 15/25, Batch 0/422, Train Loss: 0.0140\nEpoch 15/25, Batch 100/422, Train Loss: 0.0032\nEpoch 15/25, Batch 200/422, Train Loss: 0.0012\nEpoch 15/25, Batch 300/422, Train Loss: 0.0059\nEpoch 15/25, Batch 400/422, Train Loss: 0.0281\nModel CNN_MNIST at epoch 15/25: Avg Train Loss: 0.0127, Avg Val Loss: 0.0366\nEpoch 16/25, Batch 0/422, Train Loss: 0.0029\nEpoch 16/25, Batch 100/422, Train Loss: 0.0226\nEpoch 16/25, Batch 200/422, Train Loss: 0.0011\nEpoch 16/25, Batch 300/422, Train Loss: 0.0020\nEpoch 16/25, Batch 400/422, Train Loss: 0.0092\nModel CNN_MNIST at epoch 16/25: Avg Train Loss: 0.0146, Avg Val Loss: 0.0302\nEpoch 17/25, Batch 0/422, Train Loss: 0.0022\nEpoch 17/25, Batch 100/422, Train Loss: 0.0009\nEpoch 17/25, Batch 200/422, Train Loss: 0.0053\nEpoch 17/25, Batch 300/422, Train Loss: 0.0125\nEpoch 17/25, Batch 400/422, Train Loss: 0.0166\nModel CNN_MNIST at epoch 17/25: Avg Train Loss: 0.0128, Avg Val Loss: 0.0341\nEpoch 18/25, Batch 0/422, Train Loss: 0.0008\nEpoch 18/25, Batch 100/422, Train Loss: 0.0033\nEpoch 18/25, Batch 200/422, Train Loss: 0.0359\nEpoch 18/25, Batch 300/422, Train Loss: 0.0113\nEpoch 18/25, Batch 400/422, Train Loss: 0.0343\nModel CNN_MNIST at epoch 18/25: Avg Train Loss: 0.0117, Avg Val Loss: 0.0318\nEpoch 19/25, Batch 0/422, Train Loss: 0.0448\nEpoch 19/25, Batch 100/422, Train Loss: 0.0055\nEpoch 19/25, Batch 200/422, Train Loss: 0.0096\nEpoch 19/25, Batch 300/422, Train Loss: 0.0292\nEpoch 19/25, Batch 400/422, Train Loss: 0.0006\nModel CNN_MNIST at epoch 19/25: Avg Train Loss: 0.0113, Avg Val Loss: 0.0427\nEpoch 20/25, Batch 0/422, Train Loss: 0.0013\nEpoch 20/25, Batch 100/422, Train Loss: 0.0175\nEpoch 20/25, Batch 200/422, Train Loss: 0.0238\nEpoch 20/25, Batch 300/422, Train Loss: 0.0035\nEpoch 20/25, Batch 400/422, Train Loss: 0.0264\nModel CNN_MNIST at epoch 20/25: Avg Train Loss: 0.0118, Avg Val Loss: 0.0442\nEpoch 21/25, Batch 0/422, Train Loss: 0.0047\nEpoch 21/25, Batch 100/422, Train Loss: 0.0009\nEpoch 21/25, Batch 200/422, Train Loss: 0.0073\nEpoch 21/25, Batch 300/422, Train Loss: 0.0020\nEpoch 21/25, Batch 400/422, Train Loss: 0.0198\nModel CNN_MNIST at epoch 21/25: Avg Train Loss: 0.0122, Avg Val Loss: 0.0384\nEpoch 22/25, Batch 0/422, Train Loss: 0.0538\nEpoch 22/25, Batch 100/422, Train Loss: 0.0094\nEpoch 22/25, Batch 200/422, Train Loss: 0.0311\nEpoch 22/25, Batch 300/422, Train Loss: 0.0217\nEpoch 22/25, Batch 400/422, Train Loss: 0.0026\nModel CNN_MNIST at epoch 22/25: Avg Train Loss: 0.0118, Avg Val Loss: 0.0341\nEpoch 23/25, Batch 0/422, Train Loss: 0.0075\nEpoch 23/25, Batch 100/422, Train Loss: 0.0085\nEpoch 23/25, Batch 200/422, Train Loss: 0.0007\nEpoch 23/25, Batch 300/422, Train Loss: 0.0002\nEpoch 23/25, Batch 400/422, Train Loss: 0.0470\nModel CNN_MNIST at epoch 23/25: Avg Train Loss: 0.0120, Avg Val Loss: 0.0362\nEpoch 24/25, Batch 0/422, Train Loss: 0.0006\nEpoch 24/25, Batch 100/422, Train Loss: 0.0211\nEpoch 24/25, Batch 200/422, Train Loss: 0.0013\nEpoch 24/25, Batch 300/422, Train Loss: 0.0130\nEpoch 24/25, Batch 400/422, Train Loss: 0.0021\nModel CNN_MNIST at epoch 24/25: Avg Train Loss: 0.0100, Avg Val Loss: 0.0493\nEpoch 25/25, Batch 0/422, Train Loss: 0.0078\nEpoch 25/25, Batch 100/422, Train Loss: 0.0023\nEpoch 25/25, Batch 200/422, Train Loss: 0.0013\nEpoch 25/25, Batch 300/422, Train Loss: 0.0003\nEpoch 25/25, Batch 400/422, Train Loss: 0.0006\nModel CNN_MNIST at epoch 25/25: Avg Train Loss: 0.0080, Avg Val Loss: 0.0353\nModel CNN_MNIST took 5.50 minutes to run on 25 epochs.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate CNN on MNIST\ncnn_mnist_trainer.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:43:04.237782Z","iopub.execute_input":"2024-06-26T09:43:04.238494Z","iopub.status.idle":"2024-06-26T09:43:06.318664Z","shell.execute_reply.started":"2024-06-26T09:43:04.238463Z","shell.execute_reply":"2024-06-26T09:43:06.317701Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"Test Accuracy for CNN_MNIST: 0.9922 - in 2.0747809410095215 seconds.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize CNN on cluterred MNIST\ncnn_clutter = CNN()\ncnn_clutter_optimizer = optim.AdamW(cnn_clutter.parameters(), lr=1e-3)\ncnn_clutter_trainer = Trainer(model=cnn_clutter, optimizer=cnn_clutter_optimizer,\n                            train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n                            num_epochs=50, save_checkpoints=True, path=f'CNN_CLUTTER()')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T19:39:38.365125Z","iopub.execute_input":"2024-06-25T19:39:38.365488Z","iopub.status.idle":"2024-06-25T19:39:38.562050Z","shell.execute_reply.started":"2024-06-25T19:39:38.365448Z","shell.execute_reply":"2024-06-25T19:39:38.561148Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Train CNN on cluttered MNIST\ncnn_clutter_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T19:39:38.564165Z","iopub.execute_input":"2024-06-25T19:39:38.564448Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50, Batch 0/422, Train Loss: 2.3025\nEpoch 1/50, Batch 100/422, Train Loss: 2.2885\nEpoch 1/50, Batch 200/422, Train Loss: 2.2716\nEpoch 1/50, Batch 300/422, Train Loss: 2.2654\nEpoch 1/50, Batch 400/422, Train Loss: 2.2453\nModel CNN at epoch 1/50: Avg Train Loss: 2.2562, Avg Val Loss: 2.1783\nEpoch 2/50, Batch 0/422, Train Loss: 2.1931\nEpoch 2/50, Batch 100/422, Train Loss: 2.1976\nEpoch 2/50, Batch 200/422, Train Loss: 2.1855\nEpoch 2/50, Batch 300/422, Train Loss: 2.1381\nEpoch 2/50, Batch 400/422, Train Loss: 2.1300\nModel CNN at epoch 2/50: Avg Train Loss: 2.1619, Avg Val Loss: 2.1139\nEpoch 3/50, Batch 0/422, Train Loss: 2.1377\nEpoch 3/50, Batch 100/422, Train Loss: 2.1492\nEpoch 3/50, Batch 200/422, Train Loss: 2.1892\nEpoch 3/50, Batch 300/422, Train Loss: 2.0057\nEpoch 3/50, Batch 400/422, Train Loss: 2.0529\nModel CNN at epoch 3/50: Avg Train Loss: 2.1032, Avg Val Loss: 2.0331\nEpoch 4/50, Batch 0/422, Train Loss: 2.1089\nEpoch 4/50, Batch 100/422, Train Loss: 2.0860\nEpoch 4/50, Batch 200/422, Train Loss: 2.0880\nEpoch 4/50, Batch 300/422, Train Loss: 2.1121\nEpoch 4/50, Batch 400/422, Train Loss: 2.0236\nModel CNN at epoch 4/50: Avg Train Loss: 2.0503, Avg Val Loss: 1.9969\nEpoch 5/50, Batch 0/422, Train Loss: 2.0407\nEpoch 5/50, Batch 100/422, Train Loss: 2.0177\nEpoch 5/50, Batch 200/422, Train Loss: 2.0224\nEpoch 5/50, Batch 300/422, Train Loss: 1.9719\nEpoch 5/50, Batch 400/422, Train Loss: 2.0061\nModel CNN at epoch 5/50: Avg Train Loss: 2.0000, Avg Val Loss: 1.9012\nEpoch 6/50, Batch 0/422, Train Loss: 2.0111\nEpoch 6/50, Batch 100/422, Train Loss: 1.9559\nEpoch 6/50, Batch 200/422, Train Loss: 1.8897\nEpoch 6/50, Batch 300/422, Train Loss: 1.8813\nEpoch 6/50, Batch 400/422, Train Loss: 1.9410\nModel CNN at epoch 6/50: Avg Train Loss: 1.9503, Avg Val Loss: 1.8419\nEpoch 7/50, Batch 0/422, Train Loss: 1.8812\nEpoch 7/50, Batch 100/422, Train Loss: 1.9310\nEpoch 7/50, Batch 200/422, Train Loss: 1.8610\nEpoch 7/50, Batch 300/422, Train Loss: 1.8907\nEpoch 7/50, Batch 400/422, Train Loss: 1.8470\nModel CNN at epoch 7/50: Avg Train Loss: 1.9096, Avg Val Loss: 1.8049\nEpoch 8/50, Batch 0/422, Train Loss: 1.8646\nEpoch 8/50, Batch 100/422, Train Loss: 1.8716\nEpoch 8/50, Batch 200/422, Train Loss: 1.9018\nEpoch 8/50, Batch 300/422, Train Loss: 1.8562\nEpoch 8/50, Batch 400/422, Train Loss: 1.8811\nModel CNN at epoch 8/50: Avg Train Loss: 1.8746, Avg Val Loss: 1.7625\nEpoch 9/50, Batch 0/422, Train Loss: 1.8850\nEpoch 9/50, Batch 100/422, Train Loss: 1.8585\nEpoch 9/50, Batch 200/422, Train Loss: 1.8724\nEpoch 9/50, Batch 300/422, Train Loss: 1.8472\nEpoch 9/50, Batch 400/422, Train Loss: 1.8152\nModel CNN at epoch 9/50: Avg Train Loss: 1.8436, Avg Val Loss: 1.7616\nEpoch 10/50, Batch 0/422, Train Loss: 1.8117\nEpoch 10/50, Batch 100/422, Train Loss: 1.8534\nEpoch 10/50, Batch 200/422, Train Loss: 1.8207\nEpoch 10/50, Batch 300/422, Train Loss: 1.7581\nEpoch 10/50, Batch 400/422, Train Loss: 1.7987\nModel CNN at epoch 10/50: Avg Train Loss: 1.8159, Avg Val Loss: 1.7065\nEpoch 11/50, Batch 0/422, Train Loss: 1.8728\nEpoch 11/50, Batch 100/422, Train Loss: 1.7165\nEpoch 11/50, Batch 200/422, Train Loss: 1.8046\nEpoch 11/50, Batch 300/422, Train Loss: 1.7902\nEpoch 11/50, Batch 400/422, Train Loss: 1.7870\nModel CNN at epoch 11/50: Avg Train Loss: 1.7940, Avg Val Loss: 1.7029\nEpoch 12/50, Batch 0/422, Train Loss: 1.7508\nEpoch 12/50, Batch 100/422, Train Loss: 1.8009\nEpoch 12/50, Batch 200/422, Train Loss: 1.7324\nEpoch 12/50, Batch 300/422, Train Loss: 1.7616\nEpoch 12/50, Batch 400/422, Train Loss: 1.7263\nModel CNN at epoch 12/50: Avg Train Loss: 1.7743, Avg Val Loss: 1.6857\nEpoch 13/50, Batch 0/422, Train Loss: 1.7966\nEpoch 13/50, Batch 100/422, Train Loss: 1.7708\nEpoch 13/50, Batch 200/422, Train Loss: 1.7443\nEpoch 13/50, Batch 300/422, Train Loss: 1.7586\nEpoch 13/50, Batch 400/422, Train Loss: 1.7966\nModel CNN at epoch 13/50: Avg Train Loss: 1.7590, Avg Val Loss: 1.6724\nEpoch 14/50, Batch 0/422, Train Loss: 1.7972\nEpoch 14/50, Batch 100/422, Train Loss: 1.7356\nEpoch 14/50, Batch 200/422, Train Loss: 1.7424\nEpoch 14/50, Batch 300/422, Train Loss: 1.8007\nEpoch 14/50, Batch 400/422, Train Loss: 1.7457\nModel CNN at epoch 14/50: Avg Train Loss: 1.7476, Avg Val Loss: 1.6571\nEpoch 15/50, Batch 0/422, Train Loss: 1.7861\nEpoch 15/50, Batch 100/422, Train Loss: 1.7495\nEpoch 15/50, Batch 200/422, Train Loss: 1.6927\nEpoch 15/50, Batch 300/422, Train Loss: 1.6929\nEpoch 15/50, Batch 400/422, Train Loss: 1.7823\nModel CNN at epoch 15/50: Avg Train Loss: 1.7368, Avg Val Loss: 1.6474\nEpoch 16/50, Batch 0/422, Train Loss: 1.7182\nEpoch 16/50, Batch 100/422, Train Loss: 1.7357\nEpoch 16/50, Batch 200/422, Train Loss: 1.6883\nEpoch 16/50, Batch 300/422, Train Loss: 1.6972\nEpoch 16/50, Batch 400/422, Train Loss: 1.7008\nModel CNN at epoch 16/50: Avg Train Loss: 1.7283, Avg Val Loss: 1.6427\nEpoch 17/50, Batch 0/422, Train Loss: 1.7371\nEpoch 17/50, Batch 100/422, Train Loss: 1.7458\nEpoch 17/50, Batch 200/422, Train Loss: 1.7353\nEpoch 17/50, Batch 300/422, Train Loss: 1.7881\nEpoch 17/50, Batch 400/422, Train Loss: 1.7299\nModel CNN at epoch 17/50: Avg Train Loss: 1.7211, Avg Val Loss: 1.6417\nEpoch 18/50, Batch 0/422, Train Loss: 1.6654\nEpoch 18/50, Batch 100/422, Train Loss: 1.7197\nEpoch 18/50, Batch 200/422, Train Loss: 1.7238\nEpoch 18/50, Batch 300/422, Train Loss: 1.6777\nEpoch 18/50, Batch 400/422, Train Loss: 1.6694\nModel CNN at epoch 18/50: Avg Train Loss: 1.7146, Avg Val Loss: 1.6260\nEpoch 19/50, Batch 0/422, Train Loss: 1.7573\nEpoch 19/50, Batch 100/422, Train Loss: 1.7420\nEpoch 19/50, Batch 200/422, Train Loss: 1.7121\nEpoch 19/50, Batch 300/422, Train Loss: 1.6324\nEpoch 19/50, Batch 400/422, Train Loss: 1.7123\nModel CNN at epoch 19/50: Avg Train Loss: 1.7056, Avg Val Loss: 1.6240\nEpoch 20/50, Batch 0/422, Train Loss: 1.7162\nEpoch 20/50, Batch 100/422, Train Loss: 1.7029\nEpoch 20/50, Batch 200/422, Train Loss: 1.7456\nEpoch 20/50, Batch 300/422, Train Loss: 1.7498\nEpoch 20/50, Batch 400/422, Train Loss: 1.6803\nModel CNN at epoch 20/50: Avg Train Loss: 1.7029, Avg Val Loss: 1.6197\nEpoch 21/50, Batch 0/422, Train Loss: 1.6757\nEpoch 21/50, Batch 100/422, Train Loss: 1.6887\nEpoch 21/50, Batch 200/422, Train Loss: 1.6882\nEpoch 21/50, Batch 300/422, Train Loss: 1.6913\nEpoch 21/50, Batch 400/422, Train Loss: 1.6934\nModel CNN at epoch 21/50: Avg Train Loss: 1.6950, Avg Val Loss: 1.6256\nEpoch 22/50, Batch 0/422, Train Loss: 1.7096\nEpoch 22/50, Batch 100/422, Train Loss: 1.6710\nEpoch 22/50, Batch 200/422, Train Loss: 1.6791\nEpoch 22/50, Batch 300/422, Train Loss: 1.6719\nEpoch 22/50, Batch 400/422, Train Loss: 1.7016\nModel CNN at epoch 22/50: Avg Train Loss: 1.6954, Avg Val Loss: 1.6156\nEpoch 23/50, Batch 0/422, Train Loss: 1.6853\nEpoch 23/50, Batch 100/422, Train Loss: 1.6754\nEpoch 23/50, Batch 200/422, Train Loss: 1.6631\nEpoch 23/50, Batch 300/422, Train Loss: 1.7451\nEpoch 23/50, Batch 400/422, Train Loss: 1.6407\nModel CNN at epoch 23/50: Avg Train Loss: 1.6895, Avg Val Loss: 1.6108\nEpoch 24/50, Batch 0/422, Train Loss: 1.6622\nEpoch 24/50, Batch 100/422, Train Loss: 1.6658\nEpoch 24/50, Batch 200/422, Train Loss: 1.7490\nEpoch 24/50, Batch 300/422, Train Loss: 1.7033\nEpoch 24/50, Batch 400/422, Train Loss: 1.7068\nModel CNN at epoch 24/50: Avg Train Loss: 1.6816, Avg Val Loss: 1.6113\nEpoch 25/50, Batch 0/422, Train Loss: 1.6959\nEpoch 25/50, Batch 100/422, Train Loss: 1.6354\nEpoch 25/50, Batch 200/422, Train Loss: 1.6606\nEpoch 25/50, Batch 300/422, Train Loss: 1.6792\nEpoch 25/50, Batch 400/422, Train Loss: 1.6201\nModel CNN at epoch 25/50: Avg Train Loss: 1.6807, Avg Val Loss: 1.6105\nEpoch 26/50, Batch 0/422, Train Loss: 1.6394\nEpoch 26/50, Batch 100/422, Train Loss: 1.6643\nEpoch 26/50, Batch 200/422, Train Loss: 1.6799\nEpoch 26/50, Batch 300/422, Train Loss: 1.6578\nEpoch 26/50, Batch 400/422, Train Loss: 1.6880\nModel CNN at epoch 26/50: Avg Train Loss: 1.6781, Avg Val Loss: 1.6077\nEpoch 27/50, Batch 0/422, Train Loss: 1.7098\nEpoch 27/50, Batch 100/422, Train Loss: 1.6986\nEpoch 27/50, Batch 200/422, Train Loss: 1.6151\nEpoch 27/50, Batch 300/422, Train Loss: 1.6847\nEpoch 27/50, Batch 400/422, Train Loss: 1.6539\nModel CNN at epoch 27/50: Avg Train Loss: 1.6764, Avg Val Loss: 1.6206\nEpoch 28/50, Batch 0/422, Train Loss: 1.6904\nEpoch 28/50, Batch 100/422, Train Loss: 1.6440\nEpoch 28/50, Batch 200/422, Train Loss: 1.6802\nEpoch 28/50, Batch 300/422, Train Loss: 1.6862\nEpoch 28/50, Batch 400/422, Train Loss: 1.6585\nModel CNN at epoch 28/50: Avg Train Loss: 1.6703, Avg Val Loss: 1.6144\nEpoch 29/50, Batch 0/422, Train Loss: 1.6620\nEpoch 29/50, Batch 100/422, Train Loss: 1.6725\nEpoch 29/50, Batch 200/422, Train Loss: 1.6583\nEpoch 29/50, Batch 300/422, Train Loss: 1.6223\nEpoch 29/50, Batch 400/422, Train Loss: 1.7196\nModel CNN at epoch 29/50: Avg Train Loss: 1.6704, Avg Val Loss: 1.5973\nEpoch 30/50, Batch 0/422, Train Loss: 1.6234\nEpoch 30/50, Batch 100/422, Train Loss: 1.6662\nEpoch 30/50, Batch 200/422, Train Loss: 1.6320\nEpoch 30/50, Batch 300/422, Train Loss: 1.6838\nEpoch 30/50, Batch 400/422, Train Loss: 1.6598\nModel CNN at epoch 30/50: Avg Train Loss: 1.6656, Avg Val Loss: 1.5964\nEpoch 31/50, Batch 0/422, Train Loss: 1.6846\nEpoch 31/50, Batch 100/422, Train Loss: 1.6343\nEpoch 31/50, Batch 200/422, Train Loss: 1.6592\nEpoch 31/50, Batch 300/422, Train Loss: 1.6278\nEpoch 31/50, Batch 400/422, Train Loss: 1.6970\nEpoch 32/50, Batch 300/422, Train Loss: 1.6333\nEpoch 32/50, Batch 400/422, Train Loss: 1.6459\nModel CNN at epoch 32/50: Avg Train Loss: 1.6623, Avg Val Loss: 1.5908\nEpoch 33/50, Batch 0/422, Train Loss: 1.6152\nEpoch 33/50, Batch 100/422, Train Loss: 1.6917\nEpoch 33/50, Batch 200/422, Train Loss: 1.6647\nEpoch 33/50, Batch 300/422, Train Loss: 1.6829\nEpoch 33/50, Batch 400/422, Train Loss: 1.6506\nModel CNN at epoch 33/50: Avg Train Loss: 1.6598, Avg Val Loss: 1.6010\nEpoch 34/50, Batch 0/422, Train Loss: 1.7300\nEpoch 34/50, Batch 100/422, Train Loss: 1.6420\nEpoch 34/50, Batch 200/422, Train Loss: 1.6977\nEpoch 34/50, Batch 300/422, Train Loss: 1.6885\nEpoch 34/50, Batch 400/422, Train Loss: 1.6492\nModel CNN at epoch 34/50: Avg Train Loss: 1.6595, Avg Val Loss: 1.5827\nEpoch 35/50, Batch 0/422, Train Loss: 1.6669\nEpoch 35/50, Batch 100/422, Train Loss: 1.6845\nEpoch 35/50, Batch 200/422, Train Loss: 1.6129\nEpoch 35/50, Batch 300/422, Train Loss: 1.5979\nEpoch 35/50, Batch 400/422, Train Loss: 1.6499\nModel CNN at epoch 35/50: Avg Train Loss: 1.6558, Avg Val Loss: 1.5898\nEpoch 36/50, Batch 0/422, Train Loss: 1.6660\nEpoch 36/50, Batch 100/422, Train Loss: 1.6567\nEpoch 36/50, Batch 200/422, Train Loss: 1.6865\nEpoch 36/50, Batch 300/422, Train Loss: 1.6595\nEpoch 36/50, Batch 400/422, Train Loss: 1.6654\nModel CNN at epoch 36/50: Avg Train Loss: 1.6533, Avg Val Loss: 1.5888\nEpoch 37/50, Batch 0/422, Train Loss: 1.6788\nEpoch 37/50, Batch 100/422, Train Loss: 1.6532\nEpoch 42/50, Batch 100/422, Train Loss: 1.5777\nEpoch 42/50, Batch 200/422, Train Loss: 1.6292\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate CNN on cluterred MNIST\ncnn_clutter_trainer.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:37:02.148177Z","iopub.execute_input":"2024-06-26T06:37:02.148857Z","iopub.status.idle":"2024-06-26T06:37:35.727284Z","shell.execute_reply.started":"2024-06-26T06:37:02.148826Z","shell.execute_reply":"2024-06-26T06:37:35.726251Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Test Accuracy for CNN: 0.9056 - in 33.57246661186218 seconds.\n","output_type":"stream"}]},{"cell_type":"code","source":"####################\n### VAN Training ###\n####################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize VAN on MNIST\nvan_mnist = VAN_MNIST(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4])\nvan_mnist_optimizer = optim.AdamW(van_mnist.parameters(), lr=1e-3)\nvan_mnist_trainer = Trainer(model=van_mnist, optimizer=van_mnist_optimizer,\n                            train_loader=mnist_train_loader, val_loader=mnist_val_loader, test_loader=mnist_test_loader,\n                            num_epochs=25, save_checkpoints=True, path=f'VAN_MNIST(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4])')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:59:37.421489Z","iopub.execute_input":"2024-06-26T09:59:37.422552Z","iopub.status.idle":"2024-06-26T09:59:37.441445Z","shell.execute_reply.started":"2024-06-26T09:59:37.422478Z","shell.execute_reply":"2024-06-26T09:59:37.440719Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"# Train VAN on MNIST\nvan_mnist_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:44:09.597943Z","iopub.execute_input":"2024-06-26T09:44:09.598296Z","iopub.status.idle":"2024-06-26T09:50:29.990727Z","shell.execute_reply.started":"2024-06-26T09:44:09.598268Z","shell.execute_reply":"2024-06-26T09:50:29.989749Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"Epoch 1/25, Batch 0/422, Train Loss: 2.3047\nEpoch 1/25, Batch 100/422, Train Loss: 0.2294\nEpoch 1/25, Batch 200/422, Train Loss: 0.0952\nEpoch 1/25, Batch 300/422, Train Loss: 0.1123\nEpoch 1/25, Batch 400/422, Train Loss: 0.1287\nModel VAN_MNIST at epoch 1/25: Avg Train Loss: 0.3081, Avg Val Loss: 0.1016\nEpoch 2/25, Batch 0/422, Train Loss: 0.0731\nEpoch 2/25, Batch 100/422, Train Loss: 0.0493\nEpoch 2/25, Batch 200/422, Train Loss: 0.0590\nEpoch 2/25, Batch 300/422, Train Loss: 0.0353\nEpoch 2/25, Batch 400/422, Train Loss: 0.0064\nModel VAN_MNIST at epoch 2/25: Avg Train Loss: 0.0634, Avg Val Loss: 0.0584\nEpoch 3/25, Batch 0/422, Train Loss: 0.0770\nEpoch 3/25, Batch 100/422, Train Loss: 0.0176\nEpoch 3/25, Batch 200/422, Train Loss: 0.0453\nEpoch 3/25, Batch 300/422, Train Loss: 0.0358\nEpoch 3/25, Batch 400/422, Train Loss: 0.0638\nModel VAN_MNIST at epoch 3/25: Avg Train Loss: 0.0452, Avg Val Loss: 0.0475\nEpoch 4/25, Batch 0/422, Train Loss: 0.0364\nEpoch 4/25, Batch 100/422, Train Loss: 0.0057\nEpoch 4/25, Batch 200/422, Train Loss: 0.0467\nEpoch 4/25, Batch 300/422, Train Loss: 0.0255\nEpoch 4/25, Batch 400/422, Train Loss: 0.0082\nModel VAN_MNIST at epoch 4/25: Avg Train Loss: 0.0361, Avg Val Loss: 0.0487\nEpoch 5/25, Batch 0/422, Train Loss: 0.0206\nEpoch 5/25, Batch 100/422, Train Loss: 0.0023\nEpoch 5/25, Batch 200/422, Train Loss: 0.0229\nEpoch 5/25, Batch 300/422, Train Loss: 0.0310\nEpoch 5/25, Batch 400/422, Train Loss: 0.0227\nModel VAN_MNIST at epoch 5/25: Avg Train Loss: 0.0290, Avg Val Loss: 0.0387\nEpoch 6/25, Batch 0/422, Train Loss: 0.0620\nEpoch 6/25, Batch 100/422, Train Loss: 0.0382\nEpoch 6/25, Batch 200/422, Train Loss: 0.0065\nEpoch 6/25, Batch 300/422, Train Loss: 0.0259\nEpoch 6/25, Batch 400/422, Train Loss: 0.0592\nModel VAN_MNIST at epoch 6/25: Avg Train Loss: 0.0243, Avg Val Loss: 0.0442\nEpoch 7/25, Batch 0/422, Train Loss: 0.0112\nEpoch 7/25, Batch 100/422, Train Loss: 0.0116\nEpoch 7/25, Batch 200/422, Train Loss: 0.0092\nEpoch 7/25, Batch 300/422, Train Loss: 0.0031\nEpoch 7/25, Batch 400/422, Train Loss: 0.0632\nModel VAN_MNIST at epoch 7/25: Avg Train Loss: 0.0223, Avg Val Loss: 0.0344\nEpoch 8/25, Batch 0/422, Train Loss: 0.0285\nEpoch 8/25, Batch 100/422, Train Loss: 0.0353\nEpoch 8/25, Batch 200/422, Train Loss: 0.0040\nEpoch 8/25, Batch 300/422, Train Loss: 0.0156\nEpoch 8/25, Batch 400/422, Train Loss: 0.0029\nModel VAN_MNIST at epoch 8/25: Avg Train Loss: 0.0188, Avg Val Loss: 0.0417\nEpoch 9/25, Batch 0/422, Train Loss: 0.0054\nEpoch 9/25, Batch 100/422, Train Loss: 0.0030\nEpoch 9/25, Batch 200/422, Train Loss: 0.0431\nEpoch 9/25, Batch 300/422, Train Loss: 0.0237\nEpoch 9/25, Batch 400/422, Train Loss: 0.0047\nModel VAN_MNIST at epoch 9/25: Avg Train Loss: 0.0182, Avg Val Loss: 0.0351\nEpoch 10/25, Batch 0/422, Train Loss: 0.0150\nEpoch 10/25, Batch 100/422, Train Loss: 0.0009\nEpoch 10/25, Batch 200/422, Train Loss: 0.0363\nEpoch 10/25, Batch 300/422, Train Loss: 0.0055\nEpoch 10/25, Batch 400/422, Train Loss: 0.0007\nModel VAN_MNIST at epoch 10/25: Avg Train Loss: 0.0158, Avg Val Loss: 0.0491\nEpoch 11/25, Batch 0/422, Train Loss: 0.0102\nEpoch 11/25, Batch 100/422, Train Loss: 0.0013\nEpoch 11/25, Batch 200/422, Train Loss: 0.0010\nEpoch 11/25, Batch 300/422, Train Loss: 0.0100\nEpoch 11/25, Batch 400/422, Train Loss: 0.0007\nModel VAN_MNIST at epoch 11/25: Avg Train Loss: 0.0158, Avg Val Loss: 0.0347\nEpoch 12/25, Batch 0/422, Train Loss: 0.0620\nEpoch 12/25, Batch 100/422, Train Loss: 0.0057\nEpoch 12/25, Batch 200/422, Train Loss: 0.0004\nEpoch 12/25, Batch 300/422, Train Loss: 0.0145\nEpoch 12/25, Batch 400/422, Train Loss: 0.0265\nModel VAN_MNIST at epoch 12/25: Avg Train Loss: 0.0138, Avg Val Loss: 0.0328\nEpoch 13/25, Batch 0/422, Train Loss: 0.0032\nEpoch 13/25, Batch 100/422, Train Loss: 0.0047\nEpoch 13/25, Batch 200/422, Train Loss: 0.0005\nEpoch 13/25, Batch 300/422, Train Loss: 0.0020\nEpoch 13/25, Batch 400/422, Train Loss: 0.0089\nModel VAN_MNIST at epoch 13/25: Avg Train Loss: 0.0110, Avg Val Loss: 0.0309\nEpoch 14/25, Batch 0/422, Train Loss: 0.0079\nEpoch 14/25, Batch 100/422, Train Loss: 0.0017\nEpoch 14/25, Batch 200/422, Train Loss: 0.0084\nEpoch 14/25, Batch 300/422, Train Loss: 0.0061\nEpoch 14/25, Batch 400/422, Train Loss: 0.0007\nModel VAN_MNIST at epoch 14/25: Avg Train Loss: 0.0136, Avg Val Loss: 0.0353\nEpoch 15/25, Batch 0/422, Train Loss: 0.0072\nEpoch 15/25, Batch 100/422, Train Loss: 0.0003\nEpoch 15/25, Batch 200/422, Train Loss: 0.0005\nEpoch 15/25, Batch 300/422, Train Loss: 0.0107\nEpoch 15/25, Batch 400/422, Train Loss: 0.0195\nModel VAN_MNIST at epoch 15/25: Avg Train Loss: 0.0118, Avg Val Loss: 0.0383\nEpoch 16/25, Batch 0/422, Train Loss: 0.0074\nEpoch 16/25, Batch 100/422, Train Loss: 0.0091\nEpoch 16/25, Batch 200/422, Train Loss: 0.0282\nEpoch 16/25, Batch 300/422, Train Loss: 0.0027\nEpoch 16/25, Batch 400/422, Train Loss: 0.0006\nModel VAN_MNIST at epoch 16/25: Avg Train Loss: 0.0098, Avg Val Loss: 0.0360\nEpoch 17/25, Batch 0/422, Train Loss: 0.0376\nEpoch 17/25, Batch 100/422, Train Loss: 0.0152\nEpoch 17/25, Batch 200/422, Train Loss: 0.0035\nEpoch 17/25, Batch 300/422, Train Loss: 0.0071\nEpoch 17/25, Batch 400/422, Train Loss: 0.0009\nModel VAN_MNIST at epoch 17/25: Avg Train Loss: 0.0103, Avg Val Loss: 0.0466\nEpoch 18/25, Batch 0/422, Train Loss: 0.0088\nEpoch 18/25, Batch 100/422, Train Loss: 0.0274\nEpoch 18/25, Batch 200/422, Train Loss: 0.0030\nEpoch 18/25, Batch 300/422, Train Loss: 0.0099\nEpoch 18/25, Batch 400/422, Train Loss: 0.0271\nModel VAN_MNIST at epoch 18/25: Avg Train Loss: 0.0105, Avg Val Loss: 0.0357\nEpoch 19/25, Batch 0/422, Train Loss: 0.0003\nEpoch 19/25, Batch 100/422, Train Loss: 0.0013\nEpoch 19/25, Batch 200/422, Train Loss: 0.0067\nEpoch 19/25, Batch 300/422, Train Loss: 0.0050\nEpoch 19/25, Batch 400/422, Train Loss: 0.0013\nModel VAN_MNIST at epoch 19/25: Avg Train Loss: 0.0097, Avg Val Loss: 0.0411\nEpoch 20/25, Batch 0/422, Train Loss: 0.0012\nEpoch 20/25, Batch 100/422, Train Loss: 0.0058\nEpoch 20/25, Batch 200/422, Train Loss: 0.0004\nEpoch 20/25, Batch 300/422, Train Loss: 0.0010\nEpoch 20/25, Batch 400/422, Train Loss: 0.0007\nModel VAN_MNIST at epoch 20/25: Avg Train Loss: 0.0075, Avg Val Loss: 0.0366\nEpoch 21/25, Batch 0/422, Train Loss: 0.0009\nEpoch 21/25, Batch 100/422, Train Loss: 0.0035\nEpoch 21/25, Batch 200/422, Train Loss: 0.0042\nEpoch 21/25, Batch 300/422, Train Loss: 0.0301\nEpoch 21/25, Batch 400/422, Train Loss: 0.0114\nModel VAN_MNIST at epoch 21/25: Avg Train Loss: 0.0087, Avg Val Loss: 0.0518\nEpoch 22/25, Batch 0/422, Train Loss: 0.0349\nEpoch 22/25, Batch 100/422, Train Loss: 0.0011\nEpoch 22/25, Batch 200/422, Train Loss: 0.0051\nEpoch 22/25, Batch 300/422, Train Loss: 0.0000\nEpoch 22/25, Batch 400/422, Train Loss: 0.0028\nModel VAN_MNIST at epoch 22/25: Avg Train Loss: 0.0077, Avg Val Loss: 0.0423\nEpoch 23/25, Batch 0/422, Train Loss: 0.0005\nEpoch 23/25, Batch 100/422, Train Loss: 0.0108\nEpoch 23/25, Batch 200/422, Train Loss: 0.0001\nEpoch 23/25, Batch 300/422, Train Loss: 0.0057\nEpoch 23/25, Batch 400/422, Train Loss: 0.0008\nModel VAN_MNIST at epoch 23/25: Avg Train Loss: 0.0089, Avg Val Loss: 0.0432\nEpoch 24/25, Batch 0/422, Train Loss: 0.0185\nEpoch 24/25, Batch 100/422, Train Loss: 0.0036\nEpoch 24/25, Batch 200/422, Train Loss: 0.0233\nEpoch 24/25, Batch 300/422, Train Loss: 0.0010\nEpoch 24/25, Batch 400/422, Train Loss: 0.0005\nModel VAN_MNIST at epoch 24/25: Avg Train Loss: 0.0070, Avg Val Loss: 0.0370\nEpoch 25/25, Batch 0/422, Train Loss: 0.0157\nEpoch 25/25, Batch 100/422, Train Loss: 0.0044\nEpoch 25/25, Batch 200/422, Train Loss: 0.0008\nEpoch 25/25, Batch 300/422, Train Loss: 0.0008\nEpoch 25/25, Batch 400/422, Train Loss: 0.0001\nModel VAN_MNIST at epoch 25/25: Avg Train Loss: 0.0026, Avg Val Loss: 0.0318\nModel VAN_MNIST took 6.33 minutes to run on 25 epochs.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate VAN on MNIST\nvan_mnist_trainer.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:50:29.992658Z","iopub.execute_input":"2024-06-26T09:50:29.993113Z","iopub.status.idle":"2024-06-26T09:50:32.194671Z","shell.execute_reply.started":"2024-06-26T09:50:29.993077Z","shell.execute_reply":"2024-06-26T09:50:32.193763Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Test Accuracy for VAN_MNIST: 0.9933 - in 2.1942741870880127 seconds.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize VAN on cluttered MNIST\nvan_clutter = VAN(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4])\nvan_clutter_optimizer = optim.AdamW(van_clutter.parameters(), lr=1e-3)\nvan_clutter_trainer = Trainer(model=van_clutter, optimizer=van_clutter_optimizer,\n                              train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n                              num_epochs=50, save_checkpoints=True, path=f'VAN_CLUTTER(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4])')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:37:18.488526Z","iopub.execute_input":"2024-06-26T08:37:18.488780Z","iopub.status.idle":"2024-06-26T08:37:18.503934Z","shell.execute_reply.started":"2024-06-26T08:37:18.488757Z","shell.execute_reply":"2024-06-26T08:37:18.503000Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# Train VAN on cluttered MNIST\nvan_clutter_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:37:18.506071Z","iopub.execute_input":"2024-06-26T08:37:18.506445Z","iopub.status.idle":"2024-06-26T09:31:57.929104Z","shell.execute_reply.started":"2024-06-26T08:37:18.506411Z","shell.execute_reply":"2024-06-26T09:31:57.927278Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"Epoch 1/50, Batch 0/422, Train Loss: 2.3034\nEpoch 1/50, Batch 100/422, Train Loss: 2.0539\nEpoch 1/50, Batch 200/422, Train Loss: 1.8242\nEpoch 1/50, Batch 300/422, Train Loss: 1.7326\nEpoch 1/50, Batch 400/422, Train Loss: 1.8243\nModel VAN at epoch 1/50: Avg Train Loss: 1.9348, Avg Val Loss: 1.7710\nEpoch 2/50, Batch 0/422, Train Loss: 1.7565\nEpoch 2/50, Batch 100/422, Train Loss: 1.6875\nEpoch 2/50, Batch 200/422, Train Loss: 1.6846\nEpoch 2/50, Batch 300/422, Train Loss: 1.6520\nEpoch 2/50, Batch 400/422, Train Loss: 1.6071\nModel VAN at epoch 2/50: Avg Train Loss: 1.6884, Avg Val Loss: 1.6376\nEpoch 3/50, Batch 0/422, Train Loss: 1.6008\nEpoch 3/50, Batch 100/422, Train Loss: 1.6259\nEpoch 3/50, Batch 200/422, Train Loss: 1.6096\nEpoch 3/50, Batch 300/422, Train Loss: 1.6231\nEpoch 3/50, Batch 400/422, Train Loss: 1.6198\nModel VAN at epoch 3/50: Avg Train Loss: 1.6246, Avg Val Loss: 1.6107\nEpoch 4/50, Batch 0/422, Train Loss: 1.5894\nEpoch 4/50, Batch 100/422, Train Loss: 1.6221\nEpoch 4/50, Batch 200/422, Train Loss: 1.6057\nEpoch 4/50, Batch 300/422, Train Loss: 1.5887\nEpoch 4/50, Batch 400/422, Train Loss: 1.5543\nModel VAN at epoch 4/50: Avg Train Loss: 1.6085, Avg Val Loss: 1.6054\nEpoch 5/50, Batch 0/422, Train Loss: 1.5904\nEpoch 5/50, Batch 100/422, Train Loss: 1.6388\nEpoch 5/50, Batch 200/422, Train Loss: 1.5730\nEpoch 5/50, Batch 300/422, Train Loss: 1.5814\nEpoch 5/50, Batch 400/422, Train Loss: 1.5242\nModel VAN at epoch 5/50: Avg Train Loss: 1.5836, Avg Val Loss: 1.5498\nEpoch 6/50, Batch 0/422, Train Loss: 1.5164\nEpoch 6/50, Batch 100/422, Train Loss: 1.5046\nEpoch 6/50, Batch 200/422, Train Loss: 1.4978\nEpoch 6/50, Batch 300/422, Train Loss: 1.5149\nEpoch 6/50, Batch 400/422, Train Loss: 1.4885\nModel VAN at epoch 6/50: Avg Train Loss: 1.5169, Avg Val Loss: 1.5199\nEpoch 7/50, Batch 0/422, Train Loss: 1.5144\nEpoch 7/50, Batch 100/422, Train Loss: 1.4971\nEpoch 7/50, Batch 200/422, Train Loss: 1.4863\nEpoch 7/50, Batch 300/422, Train Loss: 1.5445\nEpoch 7/50, Batch 400/422, Train Loss: 1.5384\nModel VAN at epoch 7/50: Avg Train Loss: 1.5078, Avg Val Loss: 1.5124\nEpoch 8/50, Batch 0/422, Train Loss: 1.5137\nEpoch 8/50, Batch 100/422, Train Loss: 1.4847\nEpoch 8/50, Batch 200/422, Train Loss: 1.5135\nEpoch 8/50, Batch 300/422, Train Loss: 1.4801\nEpoch 8/50, Batch 400/422, Train Loss: 1.5002\nModel VAN at epoch 8/50: Avg Train Loss: 1.5046, Avg Val Loss: 1.5001\nEpoch 9/50, Batch 0/422, Train Loss: 1.5009\nEpoch 9/50, Batch 100/422, Train Loss: 1.4833\nEpoch 9/50, Batch 200/422, Train Loss: 1.4905\nEpoch 9/50, Batch 300/422, Train Loss: 1.5010\nEpoch 9/50, Batch 400/422, Train Loss: 1.5014\nModel VAN at epoch 9/50: Avg Train Loss: 1.4988, Avg Val Loss: 1.4983\nEpoch 10/50, Batch 0/422, Train Loss: 1.5021\nEpoch 10/50, Batch 100/422, Train Loss: 1.5171\nEpoch 10/50, Batch 200/422, Train Loss: 1.5064\nEpoch 10/50, Batch 300/422, Train Loss: 1.5051\nEpoch 10/50, Batch 400/422, Train Loss: 1.4774\nModel VAN at epoch 10/50: Avg Train Loss: 1.4989, Avg Val Loss: 1.4974\nEpoch 11/50, Batch 0/422, Train Loss: 1.4996\nEpoch 11/50, Batch 100/422, Train Loss: 1.4923\nEpoch 11/50, Batch 200/422, Train Loss: 1.5107\nEpoch 11/50, Batch 300/422, Train Loss: 1.5292\nEpoch 11/50, Batch 400/422, Train Loss: 1.5214\nModel VAN at epoch 11/50: Avg Train Loss: 1.4969, Avg Val Loss: 1.4999\nEpoch 12/50, Batch 0/422, Train Loss: 1.5215\nEpoch 12/50, Batch 100/422, Train Loss: 1.5009\nEpoch 12/50, Batch 200/422, Train Loss: 1.4894\nEpoch 12/50, Batch 300/422, Train Loss: 1.4871\nEpoch 12/50, Batch 400/422, Train Loss: 1.4898\nModel VAN at epoch 12/50: Avg Train Loss: 1.4972, Avg Val Loss: 1.4984\nEpoch 13/50, Batch 0/422, Train Loss: 1.4892\nEpoch 13/50, Batch 100/422, Train Loss: 1.4795\nEpoch 13/50, Batch 200/422, Train Loss: 1.4854\nEpoch 13/50, Batch 300/422, Train Loss: 1.4925\nEpoch 13/50, Batch 400/422, Train Loss: 1.5012\nModel VAN at epoch 13/50: Avg Train Loss: 1.4943, Avg Val Loss: 1.4866\nEpoch 14/50, Batch 0/422, Train Loss: 1.4812\nEpoch 14/50, Batch 100/422, Train Loss: 1.4850\nEpoch 14/50, Batch 200/422, Train Loss: 1.4939\nEpoch 14/50, Batch 300/422, Train Loss: 1.5069\nEpoch 14/50, Batch 400/422, Train Loss: 1.4760\nModel VAN at epoch 14/50: Avg Train Loss: 1.4933, Avg Val Loss: 1.4920\nEpoch 15/50, Batch 0/422, Train Loss: 1.4788\nEpoch 15/50, Batch 100/422, Train Loss: 1.4867\nEpoch 15/50, Batch 200/422, Train Loss: 1.4920\nEpoch 15/50, Batch 300/422, Train Loss: 1.4696\nEpoch 15/50, Batch 400/422, Train Loss: 1.5139\nModel VAN at epoch 15/50: Avg Train Loss: 1.4916, Avg Val Loss: 1.5035\nEpoch 16/50, Batch 0/422, Train Loss: 1.4837\nEpoch 16/50, Batch 100/422, Train Loss: 1.4824\nEpoch 16/50, Batch 200/422, Train Loss: 1.4637\nEpoch 16/50, Batch 300/422, Train Loss: 1.4642\nEpoch 16/50, Batch 400/422, Train Loss: 1.5088\nModel VAN at epoch 16/50: Avg Train Loss: 1.4914, Avg Val Loss: 1.4901\nEpoch 17/50, Batch 0/422, Train Loss: 1.4689\nEpoch 17/50, Batch 100/422, Train Loss: 1.4811\nEpoch 17/50, Batch 200/422, Train Loss: 1.4993\nEpoch 17/50, Batch 300/422, Train Loss: 1.5034\nEpoch 17/50, Batch 400/422, Train Loss: 1.4766\nModel VAN at epoch 17/50: Avg Train Loss: 1.4907, Avg Val Loss: 1.4983\nEpoch 18/50, Batch 0/422, Train Loss: 1.4951\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[113], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train VAN on cluttered MNIST\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvan_clutter_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m running_loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00\u001b[39m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[1;32m     46\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), target\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mRandomCropAndCombine.__call__\u001b[0;34m(self, canvas)\u001b[0m\n\u001b[1;32m     22\u001b[0m   patch \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mRandomCrop((\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m9\u001b[39m))(img)\n\u001b[1;32m     23\u001b[0m   canvas[:, x:x\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m9\u001b[39m, y:y\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m9\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m patch\n\u001b[0;32m---> 24\u001b[0m   canvas \u001b[38;5;241m=\u001b[39m \u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m canvas\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Evaluate VAN on cluttered MNIST\nvan_clutter_trainer.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:32:02.872772Z","iopub.execute_input":"2024-06-26T09:32:02.873126Z","iopub.status.idle":"2024-06-26T09:32:34.384146Z","shell.execute_reply.started":"2024-06-26T09:32:02.873096Z","shell.execute_reply":"2024-06-26T09:32:34.383169Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Test Accuracy for VAN: 0.9699 - in 31.503678798675537 seconds.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# #DO NOT TOUCH ANY CODE FROM ABOVE, EXCEPT COMMENTS!","metadata":{}},{"cell_type":"code","source":"# Get information about current runtime and package versions.\n\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\n\n# Get CUDA device count\ncuda_device_count = torch.cuda.device_count() if cuda_available else 0\n\n# Get current CUDA device index\ncuda_device_index = torch.cuda.current_device() if cuda_available else None\n\n# Get name of current CUDA device\ncuda_device_name = torch.cuda.get_device_name(cuda_device_index) if cuda_available else None\n\n# Get CUDA capability of the device\ncuda_capability = torch.cuda.get_device_capability(cuda_device_index) if cuda_available else None\n\n# Get CUDA version\ncuda_version = torch.version.cuda if cuda_available else None\n\n# Get cuDNN version\ncudnn_version = torch.backends.cudnn.version() if cuda_available else None\n\n# Get PyTorch version\npytorch_version = torch.__version__\n\n# Get OS information\nos_info = platform.platform()\n\npython_version = platform.python_version()\n\n# Print the information\nenvironment_dict = {\"OS:\", os_info,\n                    \"GPU:\", cuda_device_name,\n                    \"PyTorch:\", pytorch_version,\n                    \"CUDA:\", cuda_version,\n                    \"cudnn:\", cudnn_version,\n                    \"Python Version:\", python_version\n                   }\nprint(\"OS:\", os_info)\nprint(\"GPU:\", cuda_device_name)\nprint(\"PyTorch:\", pytorch_version)\nprint(\"CUDA:\", cuda_version)\nprint(\"cudnn:\", cudnn_version)\nprint(\"Python Version:\", python_version)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:30:31.802624Z","iopub.execute_input":"2024-06-26T07:30:31.803571Z","iopub.status.idle":"2024-06-26T07:30:31.878658Z","shell.execute_reply.started":"2024-06-26T07:30:31.803531Z","shell.execute_reply":"2024-06-26T07:30:31.877683Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"OS: Linux-5.15.133+-x86_64-with-glibc2.31\nGPU: Tesla P100-PCIE-16GB\nPyTorch: 2.1.2\nCUDA: 12.1\ncudnn: 8900\nPython Version: 3.10.13\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip freeze ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:30:44.306299Z","iopub.execute_input":"2024-06-26T07:30:44.306693Z","iopub.status.idle":"2024-06-26T07:30:47.614988Z","shell.execute_reply.started":"2024-06-26T07:30:44.306659Z","shell.execute_reply":"2024-06-26T07:30:47.613861Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"absl-py==1.4.0\naccelerate==0.27.2\naccess==1.1.9\naffine==2.4.0\naiobotocore==2.11.2\naiofiles==22.1.0\naiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1701099469104/work\naiohttp-cors==0.7.0\naioitertools==0.11.0\naiorwlock==1.3.0\naiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work\naiosqlite==0.19.0\nalbumentations==1.4.0\nalembic==1.13.1\naltair==5.2.0\nannotated-types @ file:///home/conda/feedstock_root/build_artifacts/annotated-types_1696634205638/work\nannoy==1.17.3\nanyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1702909220329/work\napache-beam==2.46.0\naplus==0.11.0\nappdirs==1.4.4\narchspec @ file:///home/conda/feedstock_root/build_artifacts/archspec_1699370045702/work\nargon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1692818318753/work\nargon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1695386546427/work\narray-record==0.5.0\narrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1696128962909/work\narviz==0.17.0\nastroid==3.0.3\nastropy==6.0.0\nastropy-iers-data==0.2024.2.19.0.28.47\nasttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work\nastunparse==1.6.3\nasync-lru==2.0.4\nasync-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work\nattrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1704011227531/work\naudioread==3.0.1\nautopep8==2.0.4\nBabel==2.14.0\nbackoff==2.2.1\nbayesian-optimization==1.4.3\nbayespy==0.5.28\nbeatrix_jupyterlab @ file:///home/kbuilder/miniconda3/conda-bld/dlenv-tf-2-15-gpu_1704941576253/work/packages/beatrix_jupyterlab-2023.128.151533.tar.gz#sha256=8c6941d08ce18f5b9ea7719574d611c18163074ff8254e0734342014eb064a48\nbeautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work\nbidict==0.23.1\nbiopython==1.83\nblake3==0.2.1\nbleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1696630167146/work\nblessed==1.20.0\nblinker==1.7.0\nblis @ file:///home/conda/feedstock_root/build_artifacts/cython-blis_1696148805003/work\nblosc2==2.5.1\nbokeh @ file:///home/conda/feedstock_root/build_artifacts/bokeh_1706215790147/work\nboltons @ file:///home/conda/feedstock_root/build_artifacts/boltons_1703154663129/work\nBoruta==0.3\nboto3==1.26.100\nbotocore==1.34.34\nbq_helper==0.4.1\nbqplot==0.12.43\nbranca==0.7.1\nbrewer2mpl==1.4.1\nBrotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1687884021435/work\nbrotlipy==0.7.0\ncached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work\ncachetools==4.2.4\nCartopy @ file:///home/conda/feedstock_root/build_artifacts/cartopy_1698172724393/work\ncatalogue @ file:///home/conda/feedstock_root/build_artifacts/catalogue_1695626339626/work\ncatalyst @ git+https://github.com/Philmod/catalyst.git@9420384a98c4b9d3b17b959e66f845b98457b545\ncatboost==1.2.2\ncategory-encoders==2.6.3\ncertifi @ file:///home/conda/feedstock_root/build_artifacts/certifi_1707022139797/work/certifi\ncesium==0.12.1\ncffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1696001684923/work\ncharset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1698833585322/work\nchex==0.1.85\ncleverhans==4.0.0\nclick @ file:///home/conda/feedstock_root/build_artifacts/click_1692311806742/work\nclick-plugins==1.1.1\ncligj==0.7.2\ncloud-tpu-client==0.10\ncloud-tpu-profiler==2.4.0\ncloudpathlib @ file:///home/conda/feedstock_root/build_artifacts/cloudpathlib-meta_1697837790453/work\ncloudpickle==2.2.1\ncmdstanpy==1.2.1\ncmudict==1.0.18\ncolorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work\ncolorcet==3.0.1\ncolorful==0.5.6\ncolorlog==6.8.2\ncolorlover==0.3.0\ncomm @ file:///home/conda/feedstock_root/build_artifacts/comm_1704278392174/work\nconda @ file:///home/conda/feedstock_root/build_artifacts/conda_1694556045812/work\nconda-libmamba-solver @ file:///home/conda/feedstock_root/build_artifacts/conda-libmamba-solver_1690880668143/work/src\nconda-package-handling @ file:///home/conda/feedstock_root/build_artifacts/conda-package-handling_1691048088238/work\nconda_package_streaming @ file:///home/conda/feedstock_root/build_artifacts/conda-package-streaming_1691009212940/work\nconfection @ file:///home/conda/feedstock_root/build_artifacts/confection_1701179074719/work\ncontextily==1.5.0\ncontourpy @ file:///home/conda/feedstock_root/build_artifacts/contourpy_1699041363598/work\nconvertdate==2.4.0\ncrcmod==1.7\ncryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1701563205069/work\ncuda-python @ file:///opt/conda/conda-bld/cuda-python_1696638333144/work\ncudf @ file:///opt/conda/conda-bld/work/python/cudf\ncufflinks==0.17.3\ncuml @ file:///opt/conda/conda-bld/work/python\ncupy @ file:///home/conda/feedstock_root/build_artifacts/cupy-split_1707093121318/work\nCVXcanon==0.1.2\ncycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1696677705766/work\ncymem @ file:///home/conda/feedstock_root/build_artifacts/cymem_1695443485440/work\ncysignals==1.11.4\nCython==3.0.8\ncytoolz @ file:///home/conda/feedstock_root/build_artifacts/cytoolz_1706897049115/work\ndaal==2024.1.0\ndaal4py==2024.1.0\ndacite==1.8.1\ndask==2024.2.0\ndask-cuda @ file:///opt/conda/conda-bld/work\ndask-cudf @ file:///opt/conda/conda-bld/work/python/dask_cudf\ndataclasses-json==0.6.4\ndataproc_jupyter_plugin==0.1.66\ndatasets==2.1.0\ndatashader==0.16.0\ndatatile==1.0.3\ndb-dtypes==1.2.0\ndeap==1.4.1\ndebugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1695534290310/work\ndecorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\ndeepdiff==6.7.1\ndefusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work\nDelorean==1.0.0\nDeprecated==1.2.14\ndeprecation==2.1.0\ndescartes==1.1.0\ndill==0.3.8\ndipy==1.8.0\ndistlib==0.3.8\ndistributed @ file:///home/conda/feedstock_root/build_artifacts/distributed_1689891044039/work\ndistro @ file:///home/conda/feedstock_root/build_artifacts/distro_1704321475663/work\ndm-tree==0.1.8\ndocker==7.0.0\ndocker-pycreds==0.4.0\ndocopt==0.6.2\ndocstring-parser==0.15\ndocstring-to-markdown==0.15\ndocutils==0.20.1\nearthengine-api==0.1.391\neasydict==1.12\neasyocr==1.7.1\necos==2.0.13\neli5==0.13.0\nemoji==2.10.1\nen-core-web-lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl#sha256=ab70aeb6172cde82508f7739f35ebc9918a3d07debeed637403c8f794ba3d3dc\nen-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\nentrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work\nephem==4.1.5\nesda==2.5.1\nessentia==2.1b6.dev1110\net-xmlfile==1.1.0\netils==1.6.0\nexceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1704921103267/work\nexecuting @ file:///home/conda/feedstock_root/build_artifacts/executing_1698579936712/work\nexplainable-ai-sdk==1.3.3\nFarama-Notifications==0.0.4\nfastai==2.7.14\nfastapi==0.108.0\nfastavro==1.9.3\nfastcore==1.5.29\nfastdownload==0.0.7\nfasteners==0.19\nfastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1703780968325/work/dist\nfastprogress==1.0.3\nfastrlock @ file:///home/conda/feedstock_root/build_artifacts/fastrlock_1702696298817/work\nfasttext==0.9.2\nfbpca==1.0\nfeather-format==0.4.1\nfeaturetools==1.29.0\nfilelock==3.13.1\nfiona==1.9.5\nfitter==1.7.0\nflake8==7.0.0\nflashtext==2.7\nFlask==3.0.2\nflatbuffers==23.5.26\nflax==0.8.1\nfolium==0.15.1\nfonttools==4.47.0\nfqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist\nfrozendict==2.4.0\nfrozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1702645481127/work\nfsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1707102468451/work\nfuncy==2.0\nfury==0.9.0\nfuture==1.0.0\nfuzzywuzzy==0.18.0\ngast==0.5.4\ngatspy==0.3\ngcsfs==2023.12.2.post1\ngensim==4.3.2\ngeographiclib==2.0\nGeohash==1.0\ngeojson==3.1.0\ngeopandas==0.14.3\ngeoplot==0.5.1\ngeopy==2.4.1\ngeoviews==1.11.1\nggplot @ https://github.com/hbasria/ggpy/archive/0.11.5.zip#sha256=7df947ba3fd86d3757686afec264785ad8df38dc50ffb2d2d31064fb355f69b1\ngiddy==2.3.5\ngitdb==4.0.11\nGitPython==3.1.41\ngoogle-ai-generativelanguage==0.4.0\ngoogle-api-core==2.11.1\ngoogle-api-python-client==2.118.0\ngoogle-apitools==0.5.31\ngoogle-auth==2.26.1\ngoogle-auth-httplib2==0.1.1\ngoogle-auth-oauthlib==1.2.0\ngoogle-cloud-aiplatform==0.6.0a1\ngoogle-cloud-artifact-registry==1.10.0\ngoogle-cloud-automl==1.0.1\ngoogle-cloud-bigquery==2.34.4\ngoogle-cloud-bigtable==1.7.3\ngoogle-cloud-core==2.4.1\ngoogle-cloud-datastore==2.19.0\ngoogle-cloud-dlp==3.14.0\ngoogle-cloud-jupyter-config==0.0.5\ngoogle-cloud-language==2.13.1\ngoogle-cloud-monitoring==2.18.0\ngoogle-cloud-pubsub==2.19.0\ngoogle-cloud-pubsublite==1.9.0\ngoogle-cloud-recommendations-ai==0.7.1\ngoogle-cloud-resource-manager==1.11.0\ngoogle-cloud-spanner==3.40.1\ngoogle-cloud-storage==1.44.0\ngoogle-cloud-translate==3.12.1\ngoogle-cloud-videointelligence==2.13.1\ngoogle-cloud-vision==2.8.0\ngoogle-crc32c==1.5.0\ngoogle-generativeai==0.3.2\ngoogle-pasta==0.2.0\ngoogle-resumable-media==2.7.0\ngoogleapis-common-protos==1.62.0\ngplearn==0.4.2\ngpustat==1.0.0\ngpxpy==1.6.2\ngraphviz==0.20.1\ngreenlet==3.0.3\ngrpc-google-iam-v1==0.12.7\ngrpcio @ file:///home/conda/feedstock_root/build_artifacts/grpc-split_1677499296072/work\ngrpcio-status @ file:///home/conda/feedstock_root/build_artifacts/grpcio-status_1662108958711/work\ngviz-api==1.10.0\ngym==0.26.2\ngym-notices==0.0.8\ngymnasium==0.29.0\nh11==0.14.0\nh2o==3.44.0.3\nh5netcdf==1.3.0\nh5py==3.10.0\nhaversine==2.8.1\nhdfs==2.7.3\nhep-ml==0.7.2\nhijri-converter==2.3.1\nhmmlearn==0.3.0\nholidays==0.24\nholoviews==1.18.3\nhpsklearn==0.1.0\nhtml5lib==1.1\nhtmlmin==0.1.12\nhttpcore==1.0.4\nhttplib2==0.21.0\nhttptools==0.6.1\nhttpx==0.27.0\nhuggingface-hub==0.20.3\nhumanize==4.9.0\nhunspell==0.5.5\nhusl==4.0.3\nhydra-slayer==0.5.0\nhyperopt==0.2.7\nhypertools==0.8.0\nidna @ file:///home/conda/feedstock_root/build_artifacts/idna_1701026962277/work\nigraph==0.11.4\nimagecodecs==2024.1.1\nImageHash==4.3.1\nimageio==2.33.1\nimbalanced-learn==0.12.0\nimgaug==0.4.0\nimportlib-metadata==6.11.0\nimportlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1699364556997/work\ninequality==1.0.1\niniconfig==2.0.0\nipydatawidgets==4.3.5\nipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1703631723894/work\nipyleaflet==0.18.2\nipympl==0.7.0\nipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1704718870316/work\nipython-genutils==0.2.0\nipython-sql==0.5.0\nipyvolume==0.6.3\nipyvue==1.10.1\nipyvuetify==1.8.10\nipywebrtc==0.6.0\nipywidgets==7.7.1\nisoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist\nisort==5.13.2\nisoweek==1.3.3\nitsdangerous==2.1.2\nJanome==0.5.0\njaraco.classes==3.3.0\njax==0.4.23\njax-jumpy==1.0.0\njaxlib @ file:///tmp/jax/jaxlib-0.4.23.dev20240116-cp310-cp310-manylinux2014_x86_64.whl#sha256=2adde6b0fff8a64af0b461e617ac514b80d8ee4aa52f1b1cf9a9139f427be8ba\njedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work\njeepney==0.8.0\njieba==0.42.1\nJinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work\njmespath==1.0.1\njoblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1691577114857/work\njson5==0.9.14\njsonpatch @ file:///home/conda/feedstock_root/build_artifacts/jsonpatch_1695536281965/work\njsonpointer @ file:///home/conda/feedstock_root/build_artifacts/jsonpointer_1695397238043/work\njsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1700159890288/work\njsonschema-specifications @ file:///tmp/tmpkv1z7p57/src\njupyter-console==6.6.3\njupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1699285872613/work\njupyter-http-over-ws==0.0.8\njupyter-lsp==1.5.1\njupyter-server-mathjax==0.2.6\njupyter-ydoc==0.2.5\njupyter_client==7.4.9\njupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1704727030956/work\njupyter_server==2.12.5\njupyter_server_fileid==0.9.1\njupyter_server_proxy==4.1.0\njupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1703611053195/work\njupyter_server_ydoc==0.8.0\njupyterlab==4.1.2\njupyterlab-lsp==5.0.3\njupyterlab-widgets==3.0.9\njupyterlab_git==0.44.0\njupyterlab_pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1700744013163/work\njupyterlab_server==2.25.2\njupytext==1.16.0\nkaggle==1.6.6\nkaggle-environments==1.14.3\nkagglehub==0.1.9\nkeras==3.0.5\nkeras-cv==0.8.2\nkeras-nlp==0.8.1\nkeras-tuner==1.4.6\nkernels-mixer==0.0.7\nkeyring==24.3.0\nkeyrings.google-artifactregistry-auth==1.1.2\nkfp==2.5.0\nkfp-pipeline-spec==0.2.2\nkfp-server-api==2.0.5\nkiwisolver @ file:///home/conda/feedstock_root/build_artifacts/kiwisolver_1695379902431/work\nkmapper==2.0.1\nkmodes==0.12.2\nkorean-lunar-calendar==0.3.1\nkornia==0.7.1\nkt-legacy==1.0.5\nkubernetes==26.1.0\nlangcodes @ file:///home/conda/feedstock_root/build_artifacts/langcodes_1636741340529/work\nlangid==1.1.6\nlazy_loader==0.3\nlearntools @ git+https://github.com/Kaggle/learntools@183cdad0530e7c898cd4658a63b579c54e91f056\nleven==1.0.4\nLevenshtein==0.25.0\nlibclang==16.0.6\nlibmambapy @ file:///home/conda/feedstock_root/build_artifacts/mamba-split_1692866066721/work/libmambapy\nlibpysal==4.9.2\nlibrosa==0.10.1\nlightgbm @ file:///tmp/lightgbm/lightgbm-4.2.0-py3-none-manylinux_2_31_x86_64.whl#sha256=26ed21477c12bb26edc4d6d51336cd43d5a8f7daf55ebbe27b0faf50ce96db23\nlightning-utilities==0.10.1\nlime==0.2.0.1\nline-profiler==4.1.2\nlinkify-it-py==2.0.3\nllvmlite==0.41.1\nlml==0.1.0\nlocket @ file:///home/conda/feedstock_root/build_artifacts/locket_1650660393415/work\nloguru==0.7.2\nLunarCalendar==0.0.9\nlxml==5.1.0\nlz4 @ file:///home/conda/feedstock_root/build_artifacts/lz4_1704831084136/work\nMako==1.3.2\nmamba @ file:///home/conda/feedstock_root/build_artifacts/mamba-split_1692866066721/work/mamba\nmapclassify==2.6.1\nmarisa-trie==1.1.0\nMarkdown==3.5.2\nmarkdown-it-py @ file:///home/conda/feedstock_root/build_artifacts/markdown-it-py_1686175045316/work\nmarkovify==0.9.4\nMarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1695367434228/work\nmarshmallow==3.20.2\nmatplotlib==3.7.5\nmatplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\nmatplotlib-venn==0.11.10\nmccabe==0.7.0\nmdit-py-plugins==0.4.0\nmdurl @ file:///home/conda/feedstock_root/build_artifacts/mdurl_1704317613764/work\nmemory-profiler==0.61.0\nmenuinst @ file:///home/conda/feedstock_root/build_artifacts/menuinst_1702317041727/work\nmercantile==1.2.1\nmgwr==2.2.1\nmissingno==0.5.2\nmistune==0.8.4\nmizani==0.11.0\nml-dtypes==0.2.0\nmlcrate==0.2.0\nmlens==0.2.3\nmlxtend==0.23.1\nmmh3==4.1.0\nmne==1.6.1\nmnist==0.2.2\nmock==5.1.0\nmomepy==0.7.0\nmore-itertools==10.2.0\nmpld3==0.5.10\nmpmath==1.3.0\nmsgpack @ file:///home/conda/feedstock_root/build_artifacts/msgpack-python_1700926504817/work\nmsgpack-numpy==0.4.8\nmultidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1696716075096/work\nmultimethod==1.10\nmultipledispatch==1.0.0\nmultiprocess==0.70.16\nmunkres==1.1.4\nmurmurhash @ file:///home/conda/feedstock_root/build_artifacts/murmurhash_1695449783955/work\nmypy-extensions==1.0.0\nnamex==0.0.7\nnb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1699980974206/work\nnb_conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1704789357480/work\nnbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1683202081046/work\nnbclient==0.5.13\nnbconvert==6.4.5\nnbdime==3.2.0\nnbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work\nndindex==1.8\nnest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1697083700168/work\nnetworkx==3.2.1\nnibabel==5.2.0\nnilearn==0.10.3\nninja==1.11.1.1\nnltk==3.2.4\nnose==1.3.7\nnotebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1680870634737/work\nnotebook_executor @ file:///home/kbuilder/miniconda3/conda-bld/dlenv-tf-2-15-gpu_1704941576253/work/packages/notebook_executor\nnotebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work\nnumba==0.58.1\nnumexpr==2.9.0\nnumpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1707225380409/work/dist/numpy-1.26.4-cp310-cp310-linux_x86_64.whl#sha256=51131fd8fc130cd168aecaf1bc0ea85f92e8ffebf211772ceb16ac2e7f10d7ca\nnvidia-ml-py==11.495.46\nnvtx @ file:///home/conda/feedstock_root/build_artifacts/nvtx_1708093799817/work\noauth2client==4.1.3\noauthlib==3.2.2\nobjsize==0.6.1\nodfpy==1.4.1\nolefile==0.47\nonnx==1.15.0\nopencensus==0.11.4\nopencensus-context==0.1.3\nopencv-contrib-python==4.9.0.80\nopencv-python==4.9.0.80\nopencv-python-headless==4.9.0.80\nopenpyxl==3.1.2\nopenslide-python==1.3.1\nopentelemetry-api==1.22.0\nopentelemetry-exporter-otlp==1.22.0\nopentelemetry-exporter-otlp-proto-common==1.22.0\nopentelemetry-exporter-otlp-proto-grpc==1.22.0\nopentelemetry-exporter-otlp-proto-http==1.22.0\nopentelemetry-proto==1.22.0\nopentelemetry-sdk==1.22.0\nopentelemetry-semantic-conventions==0.43b0\nopt-einsum==3.3.0\noptax==0.1.9\noptuna==3.5.0\norbax-checkpoint==0.5.3\nordered-set==4.1.0\norderedmultidict==1.0.1\norjson==3.9.10\nortools==9.4.1874\nosmnx==1.9.1\noverrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work\npackaging==21.3\npandas==2.1.4\npandas-datareader==0.10.0\npandas-profiling==3.6.6\npandas-summary==0.2.0\npandasql==0.7.3\npandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work\npanel==1.3.8\npapermill==2.5.0\nparam==2.0.2\nparso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\npartd @ file:///home/conda/feedstock_root/build_artifacts/partd_1695667515973/work\npath==16.10.0\npath.py==12.5.0\npathos==0.3.2\npathy @ file:///croot/pathy_1703688110387/work\npatsy==0.5.6\npdf2image==1.17.0\npettingzoo==1.24.0\npexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work\nphik==0.12.4\npickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\nPillow==9.5.0\npkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1694617248815/work\nplatformdirs==4.2.0\nplotly==5.18.0\nplotly-express==0.4.1\nplotnine==0.13.0\npluggy @ file:///home/conda/feedstock_root/build_artifacts/pluggy_1693086607691/work\npointpats==2.4.0\npolars==0.20.10\npolyglot==16.7.4\npooch==1.8.1\npox==0.3.4\nppca==0.0.4\nppft==1.7.6.8\npreprocessing==0.1.13\npreshed @ file:///home/conda/feedstock_root/build_artifacts/preshed_1695644760607/work\nprettytable==3.9.0\nprogressbar2==4.3.2\nprometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1700579315247/work\npromise==2.3\nprompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1702399386289/work\npronouncing==0.2.0\nprophet==1.1.1\nproto-plus @ file:///home/conda/feedstock_root/build_artifacts/proto-plus_1702003338643/work\nprotobuf==3.20.3\npsutil==5.9.3\nptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\npudb==2024.1\nPuLP==2.8.0\npure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\npy-cpuinfo==9.0.0\npy-spy==0.3.14\npy4j==0.10.9.7\npyaml==23.12.0\nPyArabic==0.6.15\npyarrow==11.0.0\npyasn1 @ file:///home/conda/feedstock_root/build_artifacts/pyasn1_1701287008248/work\npyasn1-modules @ file:///home/conda/feedstock_root/build_artifacts/pyasn1-modules_1695107857548/work\nPyAstronomy==0.20.0\npybind11==2.11.1\npyclipper==1.3.0.post5\npycodestyle==2.11.1\npycosat @ file:///home/conda/feedstock_root/build_artifacts/pycosat_1696355758174/work\npycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\npycryptodome==3.20.0\npyct==0.5.0\npycuda==2024.1\npydantic==2.5.3\npydantic_core==2.14.6\npydegensac==0.1.2\npydicom==2.4.4\npydocstyle==6.3.0\npydot==1.4.2\npydub==0.25.1\npyemd==1.0.0\npyerfa==2.0.1.1\npyexcel-io==0.6.6\npyexcel-ods==0.6.0\npyfasttext==0.4.6\npyflakes==3.2.0\npygltflib==1.16.1\nPygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1700607939962/work\nPyJWT==2.8.0\npykalman==0.9.5\npyLDAvis==3.4.1\npylibraft @ file:///opt/conda/conda-bld/work/python/pylibraft\npylint==3.0.3\npymc3==3.11.4\nPyMeeus==0.5.12\npymongo==3.13.0\nPympler==1.0.1\npynndescent==0.5.11\npynvml @ file:///home/conda/feedstock_root/build_artifacts/pynvml_1639061605391/work\npynvrtc==9.2\npyocr==0.8.5\npyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1698795453264/work\npyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1690737849915/work\npypdf==4.0.2\npyproj @ file:///home/conda/feedstock_root/build_artifacts/pyproj_1702028071709/work\npysal==24.1\npyshp @ file:///home/conda/feedstock_root/build_artifacts/pyshp_1659002966020/work\nPySocks @ file:///home/builder/ci_310/pysocks_1640793678128/work\npytesseract==0.3.10\npytest==8.0.1\npython-bidi==0.4.2\npython-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work\npython-dotenv==1.0.0\npython-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work\npython-Levenshtein==0.25.0\npython-louvain==0.16\npython-lsp-jsonrpc==1.1.2\npython-lsp-server==1.10.0\npython-slugify==8.0.4\npython-utils==3.8.2\npythreejs==2.4.2\npytoolconfig==1.3.1\npytools==2023.1.1\npytorch-ignite==0.4.13\npytorch-lightning==2.2.0.post0\npytz==2023.3.post1\npyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work\nPyUpSet==0.1.1.post7\npyviz_comms==3.0.1\nPyWavelets==1.5.0\nPyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1695373428874/work\npyzmq==24.0.1\nqgrid==1.3.1\nqtconsole==5.5.1\nQtPy==2.4.1\nquantecon==0.7.1\nquantities==0.15.0\nqudida==0.0.4\nraft-dask @ file:///opt/conda/conda-bld/work/python/raft-dask\nrapidfuzz==3.6.1\nrasterio==1.3.9\nrasterstats==0.19.0\nray==2.9.0\nray-cpp==2.9.0\nreferencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1704489226496/work\nregex==2023.12.25\nrequests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work\nrequests-oauthlib==1.3.1\nrequests-toolbelt==0.10.1\nresponses==0.18.0\nretrying==1.3.3\nrfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work\nrfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work\nrgf-python==3.12.0\nrich @ file:///home/conda/feedstock_root/build_artifacts/rich-split_1700160075651/work/dist\nrich-click==1.7.3\nrmm @ file:///opt/conda/conda-bld/work/python\nrope==1.12.0\nrpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1703822618592/work\nrsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work\nRtree==1.2.0\nruamel-yaml-conda @ file:///home/builder/ci_310/ruamel_yaml_1640794439226/work\nruamel.yaml @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml_1698138615000/work\nruamel.yaml.clib @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml.clib_1695996839082/work\ns2sphere==0.2.5\ns3fs==2024.2.0\ns3transfer==0.6.2\nsafetensors==0.4.2\nscattertext==0.1.19\nscikit-image==0.22.0\nscikit-learn==1.2.2\nscikit-learn-intelex==2024.1.0\nscikit-multilearn==0.2.0\nscikit-optimize==0.9.0\nscikit-plot==0.3.7\nscikit-surprise==1.1.3\nscipy==1.11.4\nseaborn==0.12.2\nSecretStorage==3.3.3\nsegment_anything @ git+https://github.com/facebookresearch/segment-anything.git@6fdee8f2727f4506cfbbe553e23b895e27956588\nsegregation==2.5\nsemver==3.0.2\nSend2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work\nsentencepiece==0.2.0\nsentry-sdk==1.40.5\nsetproctitle==1.3.3\nsetuptools-git==1.2\nsetuptools-scm==8.0.4\nshap==0.44.1\nShapely==1.8.5.post1\nshellingham @ file:///home/conda/feedstock_root/build_artifacts/shellingham_1698144360966/work\nShimmy==1.3.0\nsimpervisor==1.0.0\nSimpleITK==2.3.1\nsimplejson==3.19.2\nsix @ file:///tmp/build/80754af9/six_1644875935023/work\nsklearn-pandas==2.2.0\nslicer==0.0.7\nsmart-open @ file:///home/conda/feedstock_root/build_artifacts/smart_open_split_1694066705667/work/dist\nsmhasher==0.150.1\nsmmap==5.0.1\nsniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work\nsnowballstemmer==2.2.0\nsnuggs==1.4.7\nsortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work\nsoundfile==0.12.1\nsoupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1693929250441/work\nsoxr==0.3.7\nspacy @ file:///home/conda/feedstock_root/build_artifacts/spacy_1699194962107/work\nspacy-legacy @ file:///home/conda/feedstock_root/build_artifacts/spacy-legacy_1674550301837/work\nspacy-loggers @ file:///home/conda/feedstock_root/build_artifacts/spacy-loggers_1694527114282/work\nspaghetti==1.7.5.post1\nspectral==0.23.1\nspglm==1.1.0\nsphinx-rtd-theme==0.2.4\nspint==1.0.7\nsplot==1.1.5.post1\nspopt==0.6.0\nspreg==1.4.2\nspvcm==0.3.0\nSQLAlchemy==2.0.25\nsqlparse==0.4.4\nsquarify==0.4.3\nsrsly @ file:///home/conda/feedstock_root/build_artifacts/srsly_1695653949688/work\nstable-baselines3==2.1.0\nstack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work\nstanio==0.3.0\nstarlette==0.32.0.post1\nstatsmodels==0.14.1\nstemming==1.0.1\nstop-words==2018.7.23\nstopit==1.1.2\nstumpy==1.12.0\nsympy==1.12\ntables==3.9.2\ntabulate==0.9.0\ntangled-up-in-unicode==0.2.0\ntbb==2021.11.0\ntblib @ file:///home/conda/feedstock_root/build_artifacts/tblib_1702066284995/work\ntenacity==8.2.3\ntensorboard==2.15.1\ntensorboard-data-server==0.7.2\ntensorboard-plugin-profile==2.15.0\ntensorboardX==2.6.2.2\ntensorflow==2.15.0\ntensorflow-cloud==0.1.16\ntensorflow-datasets==4.9.4\ntensorflow-decision-forests==1.8.1\ntensorflow-estimator==2.15.0\ntensorflow-hub==0.16.1\ntensorflow-io==0.35.0\ntensorflow-io-gcs-filesystem==0.35.0\ntensorflow-metadata==0.14.0\ntensorflow-probability==0.23.0\ntensorflow-serving-api==2.14.1\ntensorflow-text==2.15.0\ntensorflow-transform==0.14.0\ntensorpack==0.11\ntensorstore==0.1.53\ntermcolor==2.4.0\nterminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1699810101464/work\ntestpath==0.6.0\ntext-unidecode==1.3\ntextblob==0.18.0.post0\ntexttable==1.7.0\ntf-keras==2.15.0\ntfp-nightly @ git+https://github.com/tensorflow/probability.git@fbc5ebe9b1d343113fb917010096cfd88b32eecf\nTheano==1.0.5\nTheano-PyMC==1.1.2\nthinc @ file:///home/conda/feedstock_root/build_artifacts/thinc_1703842165913/work\nthreadpoolctl==3.2.0\ntifffile==2023.12.9\ntimm==0.9.16\ntinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work\ntobler==0.11.2\ntokenizers==0.15.2\ntoml==0.10.2\ntomli==2.0.1\ntomlkit==0.12.3\ntoolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1706112571092/work\ntorch @ file:///tmp/torch/torch-2.1.2-cp310-cp310-linux_x86_64.whl#sha256=ae3259980b8d6551608b32fde2695baca64c72ed15ab2332023a248c113815a8\ntorchaudio @ file:///tmp/torch/torchaudio-2.1.2-cp310-cp310-linux_x86_64.whl#sha256=10966b20361b49bc41b6c6ba842d3ea842320fb8c589823b4120f24a98013b4a\ntorchdata==0.7.1\ntorchinfo==1.8.0\ntorchmetrics==1.3.1\ntorchtext @ file:///tmp/torch/torchtext-0.16.2-cp310-cp310-linux_x86_64.whl#sha256=a2a382655a08e1f6eeab6a307d0c8d78139cfa04cc329a7dc15a3f7c1e6e7a19\ntorchvision @ file:///tmp/torch/torchvision-0.16.2-cp310-cp310-linux_x86_64.whl#sha256=105901a20924f652ee62df0bb57580c67725eb21f11a349658952c4be2050d94\ntornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1695373560918/work\nTPOT==0.12.1\ntqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1691671248568/work\ntraceml==1.0.8\ntraitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work\ntraittypes==0.2.1\ntransformers==4.38.1\ntreelite==3.2.0\ntreelite-runtime==3.2.0\ntrueskill==0.4.5\ntruststore @ file:///home/conda/feedstock_root/build_artifacts/truststore_1694154605758/work\ntrx-python==0.2.9\ntsfresh==0.20.2\ntypeguard==4.1.5\ntyper @ file:///home/conda/feedstock_root/build_artifacts/typer_1683029246636/work\ntypes-python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/types-python-dateutil_1704512562698/work\ntyping-inspect==0.9.0\ntyping-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work\ntyping_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1702176139754/work\ntzdata==2023.4\ntzlocal==5.2\nuc-micro-py==1.0.3\nucx-py @ file:///opt/conda/conda-bld/work\nujson==5.9.0\numap-learn==0.5.5\nunicodedata2 @ file:///home/conda/feedstock_root/build_artifacts/unicodedata2_1695847980273/work\nUnidecode==1.3.8\nupdate-checker==0.18.0\nuri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist\nuritemplate==3.0.1\nurllib3==1.26.18\nurwid==2.6.4\nurwid_readline==0.13\nuvicorn==0.25.0\nuvloop==0.19.0\nvaex==4.17.0\nvaex-astro==0.9.3\nvaex-core==4.17.1\nvaex-hdf5==0.14.1\nvaex-jupyter==0.8.2\nvaex-ml==0.18.3\nvaex-server==0.9.0\nvaex-viz==0.5.4\nvec_noise==1.1.4\nvecstack==0.4.0\nvirtualenv==20.21.0\nvisions==0.7.5\nvowpalwabbit==9.9.0\nvtk==9.3.0\nWand==0.6.13\nwandb==0.16.3\nwasabi @ file:///home/conda/feedstock_root/build_artifacts/wasabi_1686131297168/work\nwatchfiles==0.21.0\nwavio==0.0.8\nwcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1704731205417/work\nweasel @ file:///home/conda/feedstock_root/build_artifacts/weasel_1699295455892/work\nwebcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work\nwebencodings @ file:///home/conda/feedstock_root/build_artifacts/webencodings_1694681268211/work\nwebsocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1701630677416/work\nwebsockets==12.0\nWerkzeug==3.0.1\nwfdb==4.1.2\nwhatthepatch==1.0.5\nwidgetsnbextension==3.6.6\nwitwidget==1.8.1\nwoodwork==0.28.0\nwordcloud==1.9.3\nwordsegment==1.3.1\nwrapt==1.14.1\nxarray==2024.2.0\nxarray-einstats==0.7.0\nxgboost==2.0.3\nxvfbwrapper==0.2.9\nxxhash==3.4.1\nxyzservices @ file:///home/conda/feedstock_root/build_artifacts/xyzservices_1698325309404/work\ny-py==0.6.2\nyapf==0.40.2\nyarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1701168553642/work\nydata-profiling==4.6.4\nyellowbrick==1.5\nypy-websocket==0.8.4\nzict @ file:///home/conda/feedstock_root/build_artifacts/zict_1681770155528/work\nzipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work\nzstandard==0.22.0\n","output_type":"stream"}]},{"cell_type":"code","source":"###Import function for checkpoints!###","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper funtion to import models from a checkpoint\ndef import_model_from_ckpt(model:str, dataset:str ,path:str):\n    '''\n    Function to import models from a checkpoint.\n    The model and the dataset the model was trained on as well as the path to the checkpoint\n    need to be specified. Returns the model in the checkpoint state.\n    '''\n    ckpt = torch.load(path)\n    if (model == 'VAN' and dataset == 'MNIST'):\n        ckpt_model = VAN_MNIST(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4])\n    if (model == 'VAN' and dataset == 'cluttered_MNIST'):\n        ckpt_model = VAN(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4]) \n    if (model == 'CNN' and dataset == 'MNIST'):\n        ckpt_model = CNN_MNIST()\n    if (model == 'CNN' and dataset == 'cluttered_MNIST'):\n        ckpt_model = CNN()\n    ckpt_model.load_state_dict(ckpt['model_state_dict'])\n    \n    return ckpt_model","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:33:14.043656Z","iopub.execute_input":"2024-06-26T07:33:14.044488Z","iopub.status.idle":"2024-06-26T07:33:14.052000Z","shell.execute_reply.started":"2024-06-26T07:33:14.044456Z","shell.execute_reply":"2024-06-26T07:33:14.050997Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Import models for the upcoming comparisons to ensure reproducability as good as possible.\nvan_MNIST = import_model_from_ckpt('VAN', 'MNIST', '/kaggle/input/van/pytorch/mnist/1/VAN_MNIST(channels64 128 stages2 l1 1 expansion_ratio2 4).pth')\n#cnn_MNIST = import_model_from_ckpt('CNN', 'MNIST', '/kaggle/working/CNN_MNIST().pth')\nvan = import_model_from_ckpt('VAN', 'cluttered_MNIST', '/kaggle/input/van/pytorch/cluttered_mnist/1/VAN_CLUTTER(channels64 128 stages2 l1 1 expansion_ratio2 4).pth')\n#cnn = import_model_from_ckpt('CNN', 'cluttered_MNIST', '/kaggle/working/CNN_CLUTTER().pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:45:48.114163Z","iopub.execute_input":"2024-06-26T10:45:48.114875Z","iopub.status.idle":"2024-06-26T10:45:48.183127Z","shell.execute_reply.started":"2024-06-26T10:45:48.114845Z","shell.execute_reply":"2024-06-26T10:45:48.182150Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"ck = torch.load('/kaggle/input/van/pytorch/mnist/1/VAN_MNIST(channels64 128 stages2 l1 1 expansion_ratio2 4).pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:47:31.379706Z","iopub.execute_input":"2024-06-26T10:47:31.380827Z","iopub.status.idle":"2024-06-26T10:47:31.410098Z","shell.execute_reply.started":"2024-06-26T10:47:31.380782Z","shell.execute_reply":"2024-06-26T10:47:31.409144Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"ck","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:47:47.161199Z","iopub.execute_input":"2024-06-26T10:47:47.162089Z","iopub.status.idle":"2024-06-26T10:47:47.790453Z","shell.execute_reply.started":"2024-06-26T10:47:47.162056Z","shell.execute_reply":"2024-06-26T10:47:47.789433Z"},"trusted":true},"execution_count":189,"outputs":[{"execution_count":189,"output_type":"execute_result","data":{"text/plain":"{'epoch': 25,\n 'model_state_dict': OrderedDict([('classifier.weight',\n               tensor([[-0.0936,  0.0393,  0.1019,  ..., -0.0349, -0.1024, -0.0966],\n                       [-0.0023, -0.0567,  0.0541,  ..., -0.1542, -0.0797,  0.0845],\n                       [ 0.0623,  0.1232,  0.0160,  ...,  0.1001, -0.0820, -0.0374],\n                       ...,\n                       [-0.0544, -0.0014, -0.1475,  ...,  0.0800,  0.1824,  0.1495],\n                       [ 0.0360,  0.0500,  0.0715,  ...,  0.0070,  0.0653, -0.1381],\n                       [ 0.0034, -0.0825, -0.0078,  ...,  0.0604, -0.0488,  0.1439]],\n                      device='cuda:0')),\n              ('classifier.bias',\n               tensor([-0.0437,  0.0567,  0.0042, -0.0845, -0.0943,  0.0342, -0.0658, -0.0169,\n                        0.0051,  0.0407], device='cuda:0')),\n              ('downsampler_1.conv.weight',\n               tensor([[[[-0.1430, -0.3738, -0.0936],\n                         [ 0.2290, -0.0578, -0.4322],\n                         [-0.0756, -0.0173, -0.1486]]],\n               \n               \n                       [[[ 0.0490, -0.0833, -0.0379],\n                         [ 0.0950,  0.3317,  0.2263],\n                         [-0.1147,  0.0089, -0.3501]]],\n               \n               \n                       [[[ 0.1159, -0.1758,  0.0369],\n                         [-0.0262,  0.2601, -0.3773],\n                         [ 0.2910,  0.0305, -0.0100]]],\n               \n               \n                       [[[-0.0015, -0.2476,  0.1468],\n                         [ 0.0310, -0.0659,  0.1073],\n                         [ 0.2184, -0.2544, -0.0649]]],\n               \n               \n                       [[[ 0.2296,  0.0742,  0.2424],\n                         [-0.1889,  0.0654, -0.3091],\n                         [-0.1996, -0.2006,  0.1864]]],\n               \n               \n                       [[[ 0.2270, -0.1691,  0.1730],\n                         [-0.3260,  0.1698,  0.2198],\n                         [-0.1072, -0.2956, -0.2435]]],\n               \n               \n                       [[[ 0.0050,  0.2975, -0.1859],\n                         [-0.1544,  0.2272,  0.0786],\n                         [ 0.0228, -0.2999,  0.0582]]],\n               \n               \n                       [[[ 0.1394, -0.2566, -0.0086],\n                         [-0.1651,  0.0261, -0.0256],\n                         [ 0.0637,  0.3117, -0.2444]]],\n               \n               \n                       [[[ 0.1982,  0.2171, -0.2716],\n                         [-0.1707, -0.1886, -0.2337],\n                         [-0.2395, -0.2013,  0.0127]]],\n               \n               \n                       [[[ 0.2538, -0.0772,  0.0398],\n                         [-0.0423, -0.3048, -0.1098],\n                         [ 0.2840, -0.0579,  0.1350]]],\n               \n               \n                       [[[ 0.0035, -0.0893,  0.2456],\n                         [-0.3327, -0.1986,  0.0757],\n                         [ 0.3358,  0.2162,  0.0902]]],\n               \n               \n                       [[[ 0.2647,  0.0193, -0.2165],\n                         [-0.2184, -0.0073,  0.1877],\n                         [ 0.2446, -0.0799,  0.3413]]],\n               \n               \n                       [[[-0.1540, -0.0464,  0.0428],\n                         [-0.0871,  0.0526,  0.1504],\n                         [ 0.2696, -0.3196, -0.0669]]],\n               \n               \n                       [[[-0.3765,  0.0252, -0.2112],\n                         [-0.0234, -0.0873,  0.0056],\n                         [-0.2372,  0.3718, -0.0701]]],\n               \n               \n                       [[[ 0.0787,  0.0520, -0.1713],\n                         [ 0.0851,  0.0246,  0.3599],\n                         [-0.3145, -0.0821, -0.0325]]],\n               \n               \n                       [[[-0.2043,  0.2489,  0.0334],\n                         [ 0.2557,  0.2087, -0.0779],\n                         [ 0.1672,  0.0030, -0.2904]]],\n               \n               \n                       [[[-0.0869,  0.0932,  0.1480],\n                         [-0.2169,  0.0111,  0.1976],\n                         [-0.0615,  0.1955, -0.2488]]],\n               \n               \n                       [[[ 0.1878,  0.2105, -0.0443],\n                         [ 0.1668,  0.3260, -0.1222],\n                         [-0.0363,  0.1046, -0.1007]]],\n               \n               \n                       [[[-0.0829,  0.1029,  0.2768],\n                         [-0.3419, -0.3321,  0.1335],\n                         [-0.1581,  0.2544,  0.1792]]],\n               \n               \n                       [[[ 0.2062, -0.0197, -0.3285],\n                         [ 0.1748, -0.2148,  0.0877],\n                         [-0.0887, -0.1456,  0.0297]]],\n               \n               \n                       [[[ 0.1685, -0.0890, -0.1060],\n                         [ 0.0405, -0.1704, -0.0525],\n                         [ 0.0047, -0.4099,  0.1232]]],\n               \n               \n                       [[[-0.0413,  0.2438, -0.1976],\n                         [ 0.4707,  0.1729,  0.2322],\n                         [ 0.2537, -0.0529, -0.1957]]],\n               \n               \n                       [[[ 0.2706, -0.1386,  0.1850],\n                         [ 0.1208, -0.0489,  0.0170],\n                         [-0.3904, -0.0324,  0.0884]]],\n               \n               \n                       [[[-0.1619, -0.1733, -0.2179],\n                         [ 0.2116,  0.2973,  0.0590],\n                         [-0.1422,  0.1132,  0.3538]]],\n               \n               \n                       [[[-0.1843, -0.1155, -0.2914],\n                         [ 0.1117, -0.1823, -0.0299],\n                         [ 0.1271,  0.1720,  0.2807]]],\n               \n               \n                       [[[-0.2154,  0.0029,  0.3183],\n                         [-0.0945, -0.1494,  0.0147],\n                         [ 0.0598,  0.2854,  0.1276]]],\n               \n               \n                       [[[-0.0907,  0.0627,  0.2188],\n                         [-0.0124, -0.1878,  0.1483],\n                         [-0.2338, -0.0870,  0.1813]]],\n               \n               \n                       [[[-0.1379,  0.3234,  0.0876],\n                         [-0.1880, -0.1557,  0.2664],\n                         [-0.0966, -0.0895, -0.1792]]],\n               \n               \n                       [[[ 0.1776,  0.0118, -0.1858],\n                         [-0.1710, -0.0537, -0.2097],\n                         [ 0.1258,  0.0961,  0.1258]]],\n               \n               \n                       [[[-0.1294, -0.0663,  0.1150],\n                         [-0.2594,  0.2440,  0.0790],\n                         [ 0.1469,  0.1052, -0.3100]]],\n               \n               \n                       [[[ 0.0508, -0.2167,  0.0984],\n                         [-0.3645,  0.2807,  0.1893],\n                         [ 0.1326, -0.2190, -0.0662]]],\n               \n               \n                       [[[ 0.0558, -0.3499, -0.2647],\n                         [-0.1549, -0.2830, -0.1382],\n                         [ 0.0950,  0.3619,  0.1227]]],\n               \n               \n                       [[[-0.1025, -0.2220,  0.1810],\n                         [ 0.0071,  0.3271,  0.1927],\n                         [ 0.0685, -0.3401,  0.1564]]],\n               \n               \n                       [[[-0.1638,  0.1644,  0.2102],\n                         [-0.1646, -0.1157, -0.1230],\n                         [ 0.2877, -0.2019,  0.2234]]],\n               \n               \n                       [[[-0.2979, -0.1578, -0.0733],\n                         [ 0.1297, -0.0999,  0.2302],\n                         [ 0.3014,  0.1241,  0.0254]]],\n               \n               \n                       [[[-0.2189, -0.0055,  0.1029],\n                         [ 0.2927, -0.2012, -0.3372],\n                         [-0.2373,  0.2033,  0.0759]]],\n               \n               \n                       [[[-0.1528,  0.3495,  0.1037],\n                         [-0.2058, -0.2471, -0.1450],\n                         [ 0.3112, -0.1721,  0.1376]]],\n               \n               \n                       [[[ 0.0611, -0.3504, -0.0955],\n                         [-0.2209,  0.2043,  0.3310],\n                         [ 0.1626,  0.0634, -0.2164]]],\n               \n               \n                       [[[-0.1533,  0.3252,  0.0677],\n                         [ 0.1123, -0.1742, -0.0566],\n                         [-0.0135, -0.2195, -0.1226]]],\n               \n               \n                       [[[-0.1194,  0.0537, -0.1307],\n                         [ 0.1322,  0.3128,  0.0943],\n                         [-0.1646,  0.1591,  0.2679]]],\n               \n               \n                       [[[ 0.0963, -0.1239,  0.0947],\n                         [-0.2197, -0.0843,  0.2692],\n                         [-0.0797, -0.1590,  0.0940]]],\n               \n               \n                       [[[-0.2500, -0.3168, -0.1839],\n                         [-0.1442, -0.0060, -0.2023],\n                         [-0.1960,  0.0371,  0.2642]]],\n               \n               \n                       [[[-0.0069,  0.2201,  0.1454],\n                         [-0.2340,  0.4284,  0.1010],\n                         [ 0.1331, -0.2057,  0.2100]]],\n               \n               \n                       [[[-0.3191, -0.0397,  0.0436],\n                         [-0.0900,  0.4120, -0.0184],\n                         [ 0.2530,  0.2918, -0.2376]]],\n               \n               \n                       [[[ 0.0818,  0.0864, -0.0142],\n                         [-0.1441,  0.2408, -0.0887],\n                         [-0.2105, -0.1691,  0.2175]]],\n               \n               \n                       [[[-0.0657,  0.0646,  0.1383],\n                         [ 0.3275,  0.0076, -0.2632],\n                         [ 0.0692, -0.3200, -0.2290]]],\n               \n               \n                       [[[-0.2015,  0.0088, -0.1516],\n                         [ 0.1131,  0.1358, -0.1626],\n                         [-0.1773,  0.2471,  0.1773]]],\n               \n               \n                       [[[ 0.1711, -0.2419,  0.2531],\n                         [ 0.2347, -0.3521, -0.1609],\n                         [ 0.0310, -0.1583,  0.1453]]],\n               \n               \n                       [[[-0.2304, -0.0037,  0.1132],\n                         [-0.0037, -0.3427, -0.3371],\n                         [-0.2170,  0.1637, -0.2769]]],\n               \n               \n                       [[[-0.1871,  0.0746, -0.1589],\n                         [-0.2527, -0.2248,  0.1027],\n                         [-0.3825, -0.2000, -0.0960]]],\n               \n               \n                       [[[ 0.3342, -0.2033,  0.0342],\n                         [-0.0596, -0.1970,  0.3034],\n                         [-0.1428,  0.0679,  0.1993]]],\n               \n               \n                       [[[ 0.2694, -0.2770,  0.0313],\n                         [-0.1800,  0.2407, -0.1546],\n                         [ 0.2910, -0.2317, -0.4315]]],\n               \n               \n                       [[[ 0.0129,  0.0050, -0.0340],\n                         [ 0.2050,  0.2387, -0.2007],\n                         [ 0.2630,  0.1536, -0.0426]]],\n               \n               \n                       [[[-0.0299,  0.4090, -0.0625],\n                         [-0.0751, -0.3301, -0.0603],\n                         [-0.0946, -0.1881, -0.0807]]],\n               \n               \n                       [[[-0.2668,  0.0637,  0.2964],\n                         [ 0.2652,  0.2172, -0.1454],\n                         [-0.1333, -0.3237,  0.1121]]],\n               \n               \n                       [[[ 0.0606,  0.2638,  0.1861],\n                         [ 0.0555, -0.0663, -0.2923],\n                         [ 0.0960, -0.0340, -0.0653]]],\n               \n               \n                       [[[-0.1403,  0.1385, -0.1234],\n                         [ 0.0235,  0.0739, -0.5856],\n                         [ 0.0454, -0.0417,  0.0859]]],\n               \n               \n                       [[[ 0.1178, -0.1039, -0.1570],\n                         [ 0.0960, -0.2932,  0.3431],\n                         [ 0.0663, -0.0476, -0.0941]]],\n               \n               \n                       [[[-0.2155, -0.2890,  0.1416],\n                         [ 0.0364,  0.0964,  0.0425],\n                         [-0.0842, -0.2438, -0.1539]]],\n               \n               \n                       [[[-0.2787,  0.0338, -0.1192],\n                         [ 0.3282,  0.3582, -0.0022],\n                         [-0.2286,  0.0423, -0.1948]]],\n               \n               \n                       [[[ 0.1102,  0.1703,  0.1721],\n                         [-0.0240, -0.0205, -0.2824],\n                         [-0.1351,  0.0464, -0.0233]]],\n               \n               \n                       [[[-0.1882, -0.2182,  0.2518],\n                         [ 0.1444,  0.2927, -0.0461],\n                         [ 0.0566, -0.2790, -0.0364]]],\n               \n               \n                       [[[ 0.2077,  0.2951, -0.1728],\n                         [ 0.0124, -0.0323,  0.0561],\n                         [ 0.1896, -0.1694, -0.1988]]],\n               \n               \n                       [[[ 0.1535,  0.1609,  0.2902],\n                         [ 0.0641,  0.0186, -0.1085],\n                         [ 0.3544,  0.0800, -0.1233]]]], device='cuda:0')),\n              ('block_1.0.batch_norm1.weight',\n               tensor([0.7326, 0.8615, 0.9740, 0.9444, 0.7695, 0.9202, 1.0030, 0.8316, 0.7031,\n                       0.7907, 0.7919, 0.7996, 0.8581, 0.7515, 0.9198, 0.8746, 0.8794, 0.8147,\n                       0.9843, 0.8716, 0.7966, 0.6529, 0.7579, 0.8946, 0.8993, 0.8199, 0.8808,\n                       0.8842, 0.8740, 0.9839, 0.9790, 0.8347, 0.7722, 0.8439, 0.8196, 0.7029,\n                       0.9811, 0.9921, 0.7472, 0.7105, 0.9998, 0.7118, 0.6142, 0.8480, 0.9117,\n                       0.7924, 0.9416, 0.9352, 0.7284, 0.7375, 0.7794, 0.6945, 0.9016, 0.7709,\n                       0.9456, 0.8634, 0.7861, 0.8966, 0.6992, 0.8818, 0.8335, 0.9516, 0.8735,\n                       0.7545], device='cuda:0')),\n              ('block_1.0.batch_norm1.bias',\n               tensor([-0.1495, -0.0666,  0.0475,  0.0418, -0.0716, -0.0982, -0.0572,  0.0022,\n                       -0.1030,  0.0419,  0.0682,  0.1221, -0.0746, -0.0594,  0.0088,  0.1344,\n                        0.0068,  0.1596, -0.0874, -0.0815,  0.0095, -0.0456, -0.0153, -0.0440,\n                       -0.0460, -0.0177,  0.1270,  0.0055,  0.0651, -0.0445,  0.0078, -0.0591,\n                       -0.1497, -0.1470, -0.0523, -0.1297,  0.0727, -0.0732,  0.2144,  0.0539,\n                       -0.0532, -0.0877,  0.0671, -0.0242, -0.1519,  0.0731,  0.0277, -0.0321,\n                        0.0140, -0.0786, -0.0682,  0.1225,  0.0782,  0.0153, -0.0747, -0.0226,\n                        0.0642,  0.1023, -0.1486, -0.0366,  0.0438, -0.1111,  0.1843,  0.0565],\n                      device='cuda:0')),\n              ('block_1.0.batch_norm1.running_mean',\n               tensor([-0.0168, -0.0019,  0.0110,  0.0046,  0.0115,  0.0003, -0.0008, -0.0023,\n                       -0.0017,  0.0212,  0.0052,  0.0102, -0.0036, -0.0350, -0.0064,  0.0091,\n                       -0.0063,  0.0196, -0.0082,  0.0041,  0.0057,  0.0199,  0.0088, -0.0193,\n                       -0.0155, -0.0041, -0.0046, -0.0046,  0.0041, -0.0093, -0.0075, -0.0164,\n                       -0.0053,  0.0055, -0.0114, -0.0077,  0.0080, -0.0116,  0.0058, -0.0108,\n                       -0.0044, -0.0392,  0.0071, -0.0135, -0.0038,  0.0143, -0.0176,  0.0185,\n                       -0.0158, -0.0319,  0.0080,  0.0122,  0.0137,  0.0038, -0.0012,  0.0217,\n                       -0.0049,  0.0036, -0.0182, -0.0161,  0.0119, -0.0039,  0.0216,  0.0349],\n                      device='cuda:0')),\n              ('block_1.0.batch_norm1.running_var',\n               tensor([0.9187, 0.1060, 0.1327, 0.0504, 0.1519, 0.2352, 0.0497, 0.0580, 0.7138,\n                       0.0759, 0.1973, 0.2209, 0.0502, 0.3045, 0.0737, 0.2739, 0.0827, 0.4964,\n                       0.2942, 0.1796, 0.2907, 0.7954, 0.1057, 0.3016, 0.2425, 0.2339, 0.1475,\n                       0.1506, 0.0765, 0.0676, 0.0670, 0.5198, 0.1451, 0.0578, 0.2230, 0.0963,\n                       0.0616, 0.0803, 0.1322, 0.3951, 0.1214, 0.8574, 0.5289, 0.3076, 0.0593,\n                       0.2820, 0.0862, 0.1013, 0.8833, 1.2526, 0.1423, 0.3363, 0.4008, 0.3163,\n                       0.0973, 0.1392, 0.2767, 0.0441, 0.3100, 0.0796, 0.0771, 0.0524, 0.1899,\n                       0.6102], device='cuda:0')),\n              ('block_1.0.batch_norm1.num_batches_tracked',\n               tensor(10550, device='cuda:0')),\n              ('block_1.0.conv1.weight',\n               tensor([[[[ 0.0102]],\n               \n                        [[ 0.0200]],\n               \n                        [[-0.1201]],\n               \n                        ...,\n               \n                        [[ 0.0508]],\n               \n                        [[ 0.0228]],\n               \n                        [[ 0.0380]]],\n               \n               \n                       [[[-0.0434]],\n               \n                        [[ 0.0143]],\n               \n                        [[-0.0370]],\n               \n                        ...,\n               \n                        [[-0.0427]],\n               \n                        [[-0.0730]],\n               \n                        [[ 0.0125]]],\n               \n               \n                       [[[ 0.1587]],\n               \n                        [[-0.0929]],\n               \n                        [[-0.0544]],\n               \n                        ...,\n               \n                        [[-0.0587]],\n               \n                        [[ 0.0909]],\n               \n                        [[ 0.0104]]],\n               \n               \n                       ...,\n               \n               \n                       [[[-0.0236]],\n               \n                        [[-0.1434]],\n               \n                        [[-0.0260]],\n               \n                        ...,\n               \n                        [[-0.2393]],\n               \n                        [[-0.0983]],\n               \n                        [[ 0.0823]]],\n               \n               \n                       [[[ 0.1280]],\n               \n                        [[ 0.0449]],\n               \n                        [[ 0.1444]],\n               \n                        ...,\n               \n                        [[-0.0235]],\n               \n                        [[ 0.0772]],\n               \n                        [[-0.0418]]],\n               \n               \n                       [[[ 0.1359]],\n               \n                        [[ 0.0485]],\n               \n                        [[ 0.1490]],\n               \n                        ...,\n               \n                        [[-0.0363]],\n               \n                        [[-0.0862]],\n               \n                        [[ 0.0082]]]], device='cuda:0')),\n              ('block_1.0.conv1.bias',\n               tensor([-0.0249,  0.0908,  0.0265, -0.1157, -0.2583, -0.2022, -0.0593, -0.1408,\n                        0.0077, -0.0666,  0.0473, -0.0167,  0.0067,  0.0726,  0.0484, -0.1223,\n                       -0.2167, -0.0669, -0.1480, -0.1881, -0.1210, -0.0309, -0.1319, -0.2419,\n                       -0.1197,  0.0948,  0.0285,  0.0070, -0.0515,  0.1416,  0.1109, -0.1295,\n                       -0.0621,  0.1030,  0.0129, -0.0785, -0.2832,  0.1133,  0.0218, -0.2016,\n                        0.0523, -0.1823, -0.0389, -0.0587, -0.0688,  0.0251,  0.1485,  0.1639,\n                       -0.1624, -0.1490, -0.0347, -0.1533, -0.3000, -0.1079, -0.1799,  0.0309,\n                       -0.1493, -0.0550, -0.0991, -0.0701,  0.0111, -0.3105, -0.0421,  0.0311],\n                      device='cuda:0')),\n              ('block_1.0.LKA.conv1.weight',\n               tensor([[[[ 0.0861, -0.1707,  0.0439,  0.1171,  0.1520],\n                         [-0.0080, -0.0756, -0.1463,  0.2600, -0.1493],\n                         [ 0.2346, -0.1271,  0.1476, -0.1221,  0.0474],\n                         [ 0.0432,  0.0992,  0.1339,  0.1237, -0.0293],\n                         [-0.0463,  0.0294, -0.0551, -0.1430,  0.0121]]],\n               \n               \n                       [[[ 0.3149,  0.1004, -0.0095,  0.1374,  0.1740],\n                         [-0.0048, -0.1835, -0.1676, -0.0188, -0.1325],\n                         [ 0.1035, -0.0766, -0.2341, -0.0968, -0.1507],\n                         [ 0.2204,  0.1342,  0.1446, -0.0061,  0.0767],\n                         [-0.0510,  0.1438, -0.0164,  0.0345, -0.1594]]],\n               \n               \n                       [[[ 0.2439, -0.1410, -0.2634,  0.2667,  0.1248],\n                         [-0.2669,  0.1373, -0.1188,  0.2180, -0.2143],\n                         [-0.0590,  0.0505, -0.0643, -0.0387,  0.0885],\n                         [-0.1257,  0.0982, -0.0692, -0.1929,  0.1169],\n                         [ 0.0963,  0.0350,  0.1132, -0.1077,  0.2038]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.2277, -0.0459,  0.0054,  0.1560,  0.0618],\n                         [-0.2096, -0.0546, -0.0123,  0.0813,  0.0464],\n                         [ 0.0746, -0.2810, -0.0432, -0.0694,  0.0058],\n                         [-0.2097,  0.0887,  0.0164, -0.0342,  0.1644],\n                         [-0.2058, -0.0218,  0.2049,  0.1276,  0.2624]]],\n               \n               \n                       [[[-0.0540,  0.2494, -0.0812,  0.2940,  0.2846],\n                         [-0.0099, -0.0510,  0.0089, -0.0529, -0.1135],\n                         [-0.1880, -0.0555, -0.0649, -0.0601, -0.1423],\n                         [-0.0991,  0.1739, -0.0044, -0.1470, -0.2435],\n                         [-0.0368,  0.0845, -0.2161, -0.0530, -0.1277]]],\n               \n               \n                       [[[ 0.2207,  0.1029, -0.1443,  0.0787,  0.1287],\n                         [ 0.3598, -0.0467,  0.0379, -0.1419, -0.1683],\n                         [ 0.1460,  0.2069, -0.0345,  0.0988,  0.0814],\n                         [ 0.1530, -0.1078, -0.0846,  0.2074,  0.2287],\n                         [ 0.0981, -0.2507, -0.0839,  0.0722, -0.0236]]]], device='cuda:0')),\n              ('block_1.0.LKA.conv1.bias',\n               tensor([ 0.1406, -0.0163, -0.0551,  0.2079, -0.2731, -0.1768, -0.1721, -0.0496,\n                       -0.0105,  0.1877,  0.0488, -0.0516,  0.2398,  0.1532,  0.1910,  0.1389,\n                       -0.0411, -0.1574, -0.1593, -0.0468,  0.0160,  0.0300,  0.1964,  0.2260,\n                       -0.0020,  0.0228, -0.1755, -0.1665, -0.0397, -0.0929, -0.1272, -0.1024,\n                        0.1985, -0.1742, -0.1074, -0.0093,  0.3527,  0.0076, -0.0159,  0.1476,\n                        0.1599, -0.1839, -0.1145,  0.0622, -0.2307,  0.0935, -0.0384, -0.0250,\n                       -0.1479, -0.3073,  0.0488, -0.2091, -0.0874,  0.1834,  0.2588,  0.1553,\n                        0.0953, -0.0011,  0.1053, -0.0166,  0.0685,  0.0483,  0.2046,  0.0786],\n                      device='cuda:0')),\n              ('block_1.0.LKA.conv2.weight',\n               tensor([[[[-0.0471, -0.0205, -0.0229, -0.2103,  0.0118],\n                         [-0.0297,  0.0107, -0.0576,  0.1562, -0.0696],\n                         [ 0.2010, -0.0081, -0.1007, -0.2047, -0.1274],\n                         [ 0.0041, -0.2589,  0.0116, -0.0396,  0.1802],\n                         [ 0.1431,  0.2597, -0.1623,  0.1336,  0.1175]]],\n               \n               \n                       [[[ 0.0094,  0.1050,  0.0359,  0.1054,  0.0646],\n                         [-0.1567, -0.2137,  0.2297, -0.2053,  0.1578],\n                         [-0.0526, -0.0550, -0.1782, -0.1614, -0.1804],\n                         [ 0.0614, -0.1814,  0.0695,  0.1071,  0.1378],\n                         [ 0.0687, -0.0765,  0.0046,  0.1319, -0.2421]]],\n               \n               \n                       [[[ 0.2129, -0.0509,  0.0768, -0.1165,  0.0226],\n                         [ 0.1040,  0.0869, -0.0163,  0.2063, -0.0493],\n                         [ 0.0052, -0.1327, -0.1742, -0.2568, -0.1373],\n                         [ 0.2355, -0.0916, -0.0663,  0.1261, -0.0607],\n                         [-0.2747,  0.1146, -0.0767,  0.2236, -0.0927]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.0532,  0.0183,  0.2482,  0.0124,  0.1332],\n                         [ 0.2309,  0.0264, -0.2600,  0.1206,  0.0119],\n                         [ 0.1176, -0.1038,  0.2192, -0.0993,  0.1343],\n                         [ 0.0658,  0.0929,  0.0850,  0.1679,  0.0272],\n                         [-0.1279,  0.1272,  0.1647,  0.0277, -0.2021]]],\n               \n               \n                       [[[-0.1412, -0.1610,  0.0024,  0.2048,  0.0569],\n                         [-0.2617, -0.0810,  0.0892, -0.0398, -0.1185],\n                         [-0.0546,  0.1320,  0.1896, -0.0020, -0.2779],\n                         [ 0.0369, -0.1507, -0.2230,  0.0791,  0.1825],\n                         [-0.1830, -0.2783, -0.0778,  0.2279,  0.1420]]],\n               \n               \n                       [[[ 0.0812,  0.1209, -0.0217, -0.1085,  0.0585],\n                         [ 0.1681,  0.0574,  0.2566,  0.0963, -0.1996],\n                         [ 0.0381, -0.2559,  0.2285, -0.0309, -0.0183],\n                         [-0.1081,  0.2893, -0.0486,  0.2706, -0.1644],\n                         [ 0.0395,  0.2090, -0.1067, -0.0271, -0.0712]]]], device='cuda:0')),\n              ('block_1.0.LKA.conv2.bias',\n               tensor([ 1.8007e-01,  4.5781e-02, -1.4989e-02,  1.9439e-01, -2.1667e-01,\n                       -2.5209e-01, -8.1944e-02,  1.1843e-01,  1.5683e-01, -2.0788e-01,\n                       -7.9347e-03, -3.1063e-01, -1.5635e-01, -9.9585e-02, -2.7003e-02,\n                       -1.1320e-01, -9.3621e-02,  3.3988e-02,  8.4090e-02,  1.9153e-01,\n                       -6.3190e-02,  1.5657e-01,  4.2847e-02, -1.1621e-01, -1.3725e-01,\n                        6.9174e-02,  4.7219e-02,  1.1155e-01, -1.1386e-01, -1.3284e-01,\n                       -5.4980e-02,  3.7570e-02, -3.2177e-02, -5.8116e-02,  2.2012e-01,\n                       -4.8309e-02,  2.9716e-04, -1.7984e-01,  5.6944e-02,  1.1596e-01,\n                       -1.0704e-01,  1.1944e-01, -6.4776e-02, -1.5139e-01,  4.1104e-02,\n                        7.3496e-02, -2.9105e-02, -1.7494e-01, -8.5987e-02,  2.1842e-01,\n                       -1.4877e-02,  2.3930e-01, -1.0742e-01,  1.8919e-01, -4.2036e-02,\n                       -7.4749e-02,  1.7627e-01,  1.5037e-01,  1.2976e-01,  8.8327e-02,\n                        2.1796e-01,  3.2530e-02,  4.5643e-02,  1.6354e-01], device='cuda:0')),\n              ('block_1.0.LKA.conv3.weight',\n               tensor([[[[-0.0589]],\n               \n                        [[ 0.1466]],\n               \n                        [[-0.0180]],\n               \n                        ...,\n               \n                        [[ 0.1468]],\n               \n                        [[ 0.0213]],\n               \n                        [[-0.0517]]],\n               \n               \n                       [[[-0.0336]],\n               \n                        [[ 0.0695]],\n               \n                        [[-0.0651]],\n               \n                        ...,\n               \n                        [[-0.0234]],\n               \n                        [[-0.1667]],\n               \n                        [[ 0.0241]]],\n               \n               \n                       [[[-0.0533]],\n               \n                        [[ 0.1206]],\n               \n                        [[-0.1634]],\n               \n                        ...,\n               \n                        [[ 0.1804]],\n               \n                        [[ 0.0448]],\n               \n                        [[ 0.0344]]],\n               \n               \n                       ...,\n               \n               \n                       [[[-0.1294]],\n               \n                        [[ 0.1196]],\n               \n                        [[ 0.0328]],\n               \n                        ...,\n               \n                        [[ 0.0318]],\n               \n                        [[ 0.0516]],\n               \n                        [[-0.0893]]],\n               \n               \n                       [[[-0.0717]],\n               \n                        [[ 0.1025]],\n               \n                        [[-0.2582]],\n               \n                        ...,\n               \n                        [[ 0.1062]],\n               \n                        [[ 0.0378]],\n               \n                        [[-0.2192]]],\n               \n               \n                       [[[ 0.0973]],\n               \n                        [[ 0.1364]],\n               \n                        [[-0.1930]],\n               \n                        ...,\n               \n                        [[ 0.0868]],\n               \n                        [[-0.0663]],\n               \n                        [[ 0.2969]]]], device='cuda:0')),\n              ('block_1.0.LKA.conv3.bias',\n               tensor([-3.8785e-02,  2.7587e-01,  3.7122e-02, -1.2771e-02, -2.2044e-01,\n                       -1.4194e-01,  1.2260e-01,  1.4263e-01,  5.4171e-02, -9.0706e-02,\n                        2.9798e-02,  6.5255e-02, -2.1846e-01,  1.8683e-01, -1.5701e-01,\n                        8.0199e-02, -2.3548e-01, -9.4161e-02, -1.0866e-02,  2.8608e-01,\n                       -2.5998e-02, -5.1745e-02, -2.3819e-01, -9.0070e-02, -1.2836e-01,\n                        5.6805e-02,  4.0680e-02,  7.5262e-02, -5.7378e-02, -2.5034e-01,\n                       -3.7923e-02, -1.1322e-01, -2.3725e-01, -2.2398e-01,  3.0983e-02,\n                        1.3082e-01, -8.0536e-02, -1.1981e-01, -3.6004e-02, -2.5093e-02,\n                       -6.0020e-02, -9.5468e-02,  1.1508e-01,  1.0690e-01,  5.1011e-05,\n                       -1.4890e-01,  1.4296e-01, -3.0798e-01, -8.3707e-02, -1.0908e-01,\n                       -8.8043e-02,  1.7122e-01,  5.7516e-02, -2.3891e-01, -3.9497e-02,\n                        1.1304e-01, -2.5199e-02, -1.1932e-02,  1.3237e-02,  9.3709e-03,\n                       -1.3717e-01, -2.2685e-02, -8.7454e-02,  1.6327e-01], device='cuda:0')),\n              ('block_1.0.batch_norm2.weight',\n               tensor([1.0005, 0.8726, 0.9532, 0.8899, 0.9661, 0.9237, 1.0181, 1.0293, 0.8344,\n                       0.8564, 0.8724, 1.0084, 0.8924, 0.8716, 0.8905, 1.1050, 0.9639, 0.9973,\n                       0.9585, 0.9639, 1.0619, 0.9857, 1.0029, 1.0278, 0.9926, 0.9360, 0.8710,\n                       0.9129, 0.9925, 0.9333, 0.9162, 1.0199, 1.0031, 0.8651, 0.9562, 0.8934,\n                       0.9472, 0.8540, 0.8734, 0.8889, 0.9310, 0.8582, 0.9517, 0.9460, 0.9133,\n                       0.7891, 0.9461, 0.9687, 1.0196, 0.9962, 1.0031, 0.8767, 0.9132, 0.9872,\n                       0.9029, 0.8368, 0.9493, 0.8237, 0.8290, 0.9399, 0.9650, 0.9739, 1.0081,\n                       0.8767], device='cuda:0')),\n              ('block_1.0.batch_norm2.bias',\n               tensor([-5.6460e-02,  1.2660e-01,  3.2447e-02, -1.0101e-01, -3.8880e-02,\n                        4.9102e-03, -1.7437e-04,  2.7420e-02, -1.3233e-02, -1.5449e-02,\n                        6.3629e-02, -1.8448e-02,  2.7451e-02, -2.7930e-02, -4.2492e-02,\n                        8.8176e-02, -5.9367e-02, -2.9035e-02, -7.8885e-02,  2.4830e-02,\n                        4.8797e-02,  7.0499e-03, -5.6542e-02, -2.8786e-02, -3.4722e-02,\n                        8.5370e-02,  2.6335e-02, -4.7352e-02,  2.3291e-02,  1.2942e-02,\n                        4.5368e-02, -6.4708e-02,  6.8626e-02,  2.5021e-02, -1.5167e-02,\n                        7.1885e-02, -4.3977e-04,  3.9909e-02,  3.4985e-03,  4.1129e-02,\n                        1.1201e-01, -1.4960e-02, -4.0575e-02,  2.5068e-02,  4.4136e-02,\n                        7.8095e-03,  5.8492e-02, -3.0138e-02,  5.2940e-03,  2.9821e-02,\n                        6.3137e-02,  2.0882e-02, -6.4843e-02, -5.2305e-02, -1.8288e-01,\n                       -6.5173e-02,  2.0382e-02,  4.4821e-02, -8.1010e-03,  6.6067e-02,\n                        6.9047e-02,  5.3416e-03, -1.9750e-02, -8.6888e-02], device='cuda:0')),\n              ('block_1.0.batch_norm2.running_mean',\n               tensor([-0.0889,  0.4446,  0.0295, -0.2783, -0.0308, -0.1149,  0.1878,  0.1518,\n                        0.1554,  0.0898, -0.0141,  0.0599, -0.2879,  0.2680, -0.2851,  0.2934,\n                       -0.0416, -0.1639, -0.0960,  0.2014,  0.0778, -0.1173, -0.2044, -0.0887,\n                       -0.0697,  0.2874, -0.1655,  0.0281, -0.0692, -0.6650,  0.0984, -0.1697,\n                       -0.0957, -0.5160, -0.0578,  0.2270, -0.1148, -0.2064,  0.0177,  0.0915,\n                        0.1336, -0.0775,  0.1024,  0.1893,  0.0767,  0.0560,  0.0263, -0.7123,\n                        0.0972, -0.1425, -0.1368,  0.3091,  0.0083, -0.1210, -0.1005,  0.0985,\n                       -0.0996, -0.0868, -0.0301,  0.1729, -0.0942,  0.0805, -0.0464,  0.1564],\n                      device='cuda:0')),\n              ('block_1.0.batch_norm2.running_var',\n               tensor([0.5218, 2.6248, 0.3155, 1.4156, 0.3212, 0.6011, 1.5738, 0.6563, 1.1292,\n                       0.3033, 0.4244, 0.2805, 1.1354, 0.0840, 1.5680, 2.1705, 0.3761, 0.1103,\n                       0.9450, 0.7061, 0.5627, 0.4751, 0.6485, 0.3297, 0.4510, 1.6410, 0.5488,\n                       0.4211, 0.4075, 0.5002, 0.9170, 1.0915, 0.4592, 0.2674, 0.3153, 1.3327,\n                       0.6056, 0.1217, 0.8111, 0.5770, 1.0559, 0.3156, 0.2786, 0.2532, 0.8896,\n                       0.7166, 0.8178, 0.5342, 0.6713, 0.5226, 0.6090, 1.7915, 0.6277, 0.2808,\n                       0.5806, 0.0625, 0.5074, 0.8043, 0.6001, 1.2250, 1.3387, 0.4662, 0.5778,\n                       0.1210], device='cuda:0')),\n              ('block_1.0.batch_norm2.num_batches_tracked',\n               tensor(10550, device='cuda:0')),\n              ('block_1.0.FFN.conv1.weight',\n               tensor([[[[ 0.2810]],\n               \n                        [[ 0.0888]],\n               \n                        [[ 0.2259]],\n               \n                        ...,\n               \n                        [[-0.2191]],\n               \n                        [[ 0.0306]],\n               \n                        [[-0.0064]]],\n               \n               \n                       [[[-0.1113]],\n               \n                        [[ 0.2310]],\n               \n                        [[-0.0603]],\n               \n                        ...,\n               \n                        [[-0.0341]],\n               \n                        [[ 0.0187]],\n               \n                        [[ 0.0082]]],\n               \n               \n                       [[[ 0.0335]],\n               \n                        [[-0.0419]],\n               \n                        [[ 0.0684]],\n               \n                        ...,\n               \n                        [[-0.0109]],\n               \n                        [[ 0.0984]],\n               \n                        [[ 0.0687]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.0554]],\n               \n                        [[-0.0420]],\n               \n                        [[-0.0483]],\n               \n                        ...,\n               \n                        [[-0.0224]],\n               \n                        [[-0.0853]],\n               \n                        [[ 0.1364]]],\n               \n               \n                       [[[ 0.0805]],\n               \n                        [[-0.0288]],\n               \n                        [[-0.0337]],\n               \n                        ...,\n               \n                        [[-0.0678]],\n               \n                        [[ 0.0371]],\n               \n                        [[-0.0284]]],\n               \n               \n                       [[[ 0.0237]],\n               \n                        [[-0.0990]],\n               \n                        [[-0.0130]],\n               \n                        ...,\n               \n                        [[-0.1404]],\n               \n                        [[ 0.0422]],\n               \n                        [[ 0.0209]]]], device='cuda:0')),\n              ('block_1.0.FFN.conv1.bias',\n               tensor([-1.1375e-01,  2.1637e-01, -1.9720e-01, -2.7128e-01, -1.3235e-01,\n                        3.8400e-02,  2.0128e-02, -4.8511e-02, -5.2391e-02, -1.9596e-01,\n                        1.4722e-01, -2.3726e-01,  2.3472e-02,  6.0826e-02, -1.8705e-01,\n                        1.7200e-01,  1.9284e-01,  1.9004e-01, -2.6506e-01,  1.3012e-01,\n                        1.5276e-01, -2.3174e-01,  4.6369e-02, -7.0254e-02, -1.7953e-01,\n                       -5.7959e-02,  3.9951e-02,  1.0308e-01, -1.6949e-01,  1.4184e-01,\n                       -9.9628e-02,  2.2425e-01,  2.5249e-02,  9.0104e-02, -2.5185e-01,\n                        4.0318e-02, -1.9710e-01, -2.7838e-01, -1.3514e-01,  7.9541e-02,\n                       -2.1936e-01, -1.2055e-01,  2.0378e-01,  2.4039e-01,  1.0887e-01,\n                       -2.4995e-02,  6.9709e-02,  2.8073e-01,  1.8379e-01,  6.1003e-03,\n                       -1.7418e-02, -1.3262e-01, -6.0075e-02,  4.8433e-02, -6.6653e-02,\n                        1.4153e-01, -1.0504e-01,  1.5471e-01, -1.8126e-02, -8.7449e-02,\n                       -1.0506e-01, -2.8735e-02, -1.3853e-01,  9.6591e-02, -1.4968e-01,\n                        5.2459e-02, -5.7912e-03, -1.3817e-01, -1.1486e-01,  1.5290e-01,\n                       -5.9608e-03,  1.2598e-01,  2.2786e-02,  7.5238e-02,  1.6120e-02,\n                        1.6992e-01, -1.2034e-02, -4.2531e-02,  8.8011e-05,  9.5775e-02,\n                        1.5621e-01,  9.1927e-02, -1.0538e-01, -7.2943e-03, -5.7469e-03,\n                        5.9695e-02,  3.2648e-02, -6.2764e-02, -3.2319e-03, -7.5484e-02,\n                        1.3969e-01, -3.4416e-02, -3.7521e-02,  2.4133e-01,  1.6261e-01,\n                       -6.0301e-02, -2.1977e-01,  1.2076e-01,  1.8136e-01, -8.1445e-02,\n                        7.4794e-02, -1.6781e-01, -4.8071e-02,  1.2326e-01, -1.3473e-01,\n                       -2.5239e-01,  2.4557e-01, -4.1710e-02,  6.2615e-02,  2.0673e-01,\n                        1.3721e-01,  1.8096e-01,  1.9574e-02,  1.8200e-01,  1.7240e-02,\n                       -1.1456e-01, -5.1043e-02,  3.6538e-02,  7.0020e-03, -1.0384e-01,\n                       -2.0009e-01,  1.3718e-01, -7.3489e-02, -3.9110e-02, -1.7162e-01,\n                       -7.2652e-02,  3.6275e-03, -1.4318e-01], device='cuda:0')),\n              ('block_1.0.FFN.conv2.weight',\n               tensor([[[[ 0.0743, -0.0164,  0.0779],\n                         [-0.1166,  0.3630,  0.2668],\n                         [ 0.3078,  0.1118,  0.0236]]],\n               \n               \n                       [[[-0.3516, -0.3938, -0.0272],\n                         [-0.0876, -0.2867,  0.2180],\n                         [ 0.2506,  0.1530, -0.2156]]],\n               \n               \n                       [[[ 0.0763, -0.1447, -0.2039],\n                         [ 0.4023,  0.1370,  0.0295],\n                         [ 0.2585,  0.2002, -0.2388]]],\n               \n               \n                       ...,\n               \n               \n                       [[[-0.0501, -0.3332,  0.3057],\n                         [ 0.2003, -0.0230,  0.0144],\n                         [ 0.2520, -0.1632,  0.0081]]],\n               \n               \n                       [[[ 0.0233,  0.1735,  0.1750],\n                         [ 0.0217,  0.2267,  0.1991],\n                         [ 0.3241,  0.0573,  0.2514]]],\n               \n               \n                       [[[ 0.0426, -0.2138, -0.1344],\n                         [-0.0818,  0.1097,  0.0551],\n                         [ 0.1404,  0.2833,  0.0762]]]], device='cuda:0')),\n              ('block_1.0.FFN.conv2.bias',\n               tensor([-0.0320, -0.0102,  0.1889, -0.1644, -0.3693, -0.0771, -0.2772,  0.0661,\n                        0.1825, -0.2521, -0.0818, -0.1066, -0.0259, -0.1776, -0.2006,  0.0064,\n                        0.1083,  0.0131, -0.0527, -0.3102, -0.1348, -0.3312, -0.2855, -0.3581,\n                       -0.0517, -0.3029,  0.1659, -0.3487, -0.0459,  0.1420, -0.1244, -0.1852,\n                        0.0215, -0.2247, -0.2768, -0.2048, -0.0749,  0.0038,  0.0565, -0.2486,\n                       -0.1021, -0.0932,  0.0423, -0.0808,  0.0593,  0.1758, -0.2998, -0.2550,\n                       -0.4635,  0.0273,  0.0558, -0.0017, -0.1865, -0.0760, -0.0852, -0.0425,\n                        0.2571,  0.0988, -0.1922,  0.0275, -0.0190, -0.0934, -0.0645, -0.0640,\n                        0.0973, -0.2421, -0.2315,  0.1589, -0.4076, -0.0628,  0.1577, -0.2755,\n                       -0.1586, -0.1970,  0.2839, -0.3110, -0.2406, -0.3546, -0.2564, -0.1485,\n                       -0.0409, -0.2203, -0.0695, -0.3572,  0.1023, -0.1394, -0.0331, -0.0418,\n                       -0.1313, -0.0203, -0.1080, -0.0703, -0.0222, -0.0992,  0.1640,  0.0870,\n                       -0.3396, -0.2084,  0.0574,  0.0810, -0.1186,  0.1801, -0.2694, -0.0688,\n                        0.1024, -0.0804,  0.0030, -0.1953, -0.3186, -0.1872,  0.0941,  0.1465,\n                       -0.2072,  0.0812, -0.3081, -0.0711, -0.1034,  0.2264, -0.2529, -0.0875,\n                       -0.1284, -0.3126, -0.1257,  0.1541, -0.1343, -0.3045, -0.1099, -0.0203],\n                      device='cuda:0')),\n              ('block_1.0.FFN.conv3.weight',\n               tensor([[[[ 0.1030]],\n               \n                        [[-0.1691]],\n               \n                        [[-0.1475]],\n               \n                        ...,\n               \n                        [[-0.1662]],\n               \n                        [[-0.0234]],\n               \n                        [[-0.1025]]],\n               \n               \n                       [[[ 0.0782]],\n               \n                        [[-0.1614]],\n               \n                        [[-0.1612]],\n               \n                        ...,\n               \n                        [[-0.0248]],\n               \n                        [[-0.1102]],\n               \n                        [[-0.0962]]],\n               \n               \n                       [[[ 0.0116]],\n               \n                        [[-0.0099]],\n               \n                        [[-0.0869]],\n               \n                        ...,\n               \n                        [[-0.1190]],\n               \n                        [[-0.0410]],\n               \n                        [[ 0.0730]]],\n               \n               \n                       ...,\n               \n               \n                       [[[-0.2249]],\n               \n                        [[ 0.0655]],\n               \n                        [[-0.1346]],\n               \n                        ...,\n               \n                        [[ 0.0352]],\n               \n                        [[-0.0040]],\n               \n                        [[-0.0951]]],\n               \n               \n                       [[[-0.0595]],\n               \n                        [[ 0.0574]],\n               \n                        [[ 0.1806]],\n               \n                        ...,\n               \n                        [[ 0.2360]],\n               \n                        [[ 0.0159]],\n               \n                        [[ 0.1602]]],\n               \n               \n                       [[[-0.0645]],\n               \n                        [[ 0.0314]],\n               \n                        [[-0.0992]],\n               \n                        ...,\n               \n                        [[-0.0317]],\n               \n                        [[ 0.0359]],\n               \n                        [[-0.0971]]]], device='cuda:0')),\n              ('block_1.0.FFN.conv3.bias',\n               tensor([-0.0434,  0.0515, -0.0123,  0.0336, -0.0798,  0.0741, -0.0282, -0.0678,\n                        0.0838, -0.1460, -0.0099, -0.1302, -0.0688,  0.0407,  0.0962,  0.0217,\n                       -0.0627, -0.0215, -0.0460,  0.1043,  0.0300,  0.0106,  0.0677,  0.0235,\n                       -0.0180,  0.0236,  0.0396, -0.0404, -0.0023,  0.0646,  0.0060,  0.1327,\n                        0.0593,  0.0523,  0.0149, -0.0455,  0.0152,  0.0546, -0.1139, -0.0475,\n                       -0.0038,  0.0665, -0.0065, -0.0337, -0.0213, -0.1192, -0.0074,  0.0323,\n                        0.0025,  0.0114, -0.0051,  0.0574, -0.0058, -0.0253,  0.0136, -0.0485,\n                        0.1010, -0.0824,  0.0064, -0.0238,  0.1373,  0.0977,  0.0196,  0.0557],\n                      device='cuda:0')),\n              ('downsampler_2.conv.weight',\n               tensor([[[[ 3.7926e-02,  3.1581e-02,  9.8286e-02],\n                         [ 5.6160e-02,  1.0276e-01,  1.6220e-01],\n                         [ 9.6492e-02,  1.0017e-01,  8.2399e-02]],\n               \n                        [[ 1.5355e-02,  6.7424e-02,  5.0038e-02],\n                         [ 1.2066e-01,  1.1342e-01, -1.8427e-02],\n                         [ 5.6784e-02,  9.2440e-02,  1.3450e-01]],\n               \n                        [[-3.6754e-03,  6.3163e-02,  1.1536e-01],\n                         [ 7.3142e-02,  2.0633e-01,  1.2334e-01],\n                         [ 1.0137e-01,  2.0445e-01,  1.2516e-01]],\n               \n                        ...,\n               \n                        [[-2.2897e-02,  5.6016e-02, -9.8190e-03],\n                         [ 1.3223e-01,  5.8622e-02,  1.0753e-01],\n                         [-3.7762e-02,  3.4445e-03, -2.4443e-02]],\n               \n                        [[-1.0564e-02, -9.4803e-02,  2.1400e-02],\n                         [-7.4595e-02, -1.4271e-01,  1.6705e-03],\n                         [-8.2002e-02, -1.2225e-01, -3.1270e-02]],\n               \n                        [[-4.5699e-02, -3.3978e-02, -4.1027e-02],\n                         [-7.0646e-02,  4.2973e-03, -5.5616e-02],\n                         [ 2.4030e-03, -1.7215e-01, -7.3797e-02]]],\n               \n               \n                       [[[-5.3159e-02, -8.4872e-02,  1.2272e-01],\n                         [-2.8662e-02,  1.5317e-02,  2.9255e-02],\n                         [-6.2572e-02,  5.3574e-02, -3.7565e-03]],\n               \n                        [[-8.1347e-03,  4.4624e-02,  7.9128e-02],\n                         [ 3.9256e-02, -4.8293e-04,  1.8072e-01],\n                         [-9.2706e-02, -1.4098e-02,  1.0899e-01]],\n               \n                        [[-1.0017e-01, -2.2831e-02,  1.0032e-01],\n                         [ 5.1335e-03,  6.7856e-02,  1.4899e-01],\n                         [-9.9100e-03,  1.4319e-02,  6.6450e-02]],\n               \n                        ...,\n               \n                        [[-9.0247e-03,  1.0329e-02, -2.9901e-02],\n                         [-4.1606e-02, -2.6319e-03,  8.3187e-02],\n                         [ 8.2434e-02,  1.0569e-01,  7.2695e-02]],\n               \n                        [[-3.7850e-02, -2.5448e-02, -6.8299e-02],\n                         [ 3.9077e-03, -9.4328e-02,  4.5830e-02],\n                         [ 3.9230e-02,  1.0244e-01,  6.7610e-02]],\n               \n                        [[ 5.9014e-02, -3.9745e-02,  1.0606e-02],\n                         [-2.7805e-02,  4.6357e-02, -1.3847e-01],\n                         [-1.7659e-02,  9.4006e-04, -3.8218e-04]]],\n               \n               \n                       [[[-1.4098e-01, -6.0962e-02, -1.2549e-01],\n                         [-1.6259e-01, -1.4044e-01, -1.8899e-01],\n                         [-1.4514e-01, -1.3828e-01, -6.1937e-02]],\n               \n                        [[ 1.8247e-02,  1.3395e-02, -2.0034e-02],\n                         [-7.2297e-02, -9.0113e-03, -5.9020e-02],\n                         [ 1.4688e-02, -4.2935e-02, -6.0538e-02]],\n               \n                        [[-4.9984e-02, -4.2418e-02, -1.2031e-01],\n                         [-5.3225e-02, -2.6099e-01, -1.1438e-01],\n                         [-5.3678e-02, -1.4947e-01, -8.3759e-02]],\n               \n                        ...,\n               \n                        [[ 7.5619e-03,  2.3511e-02,  9.5324e-02],\n                         [-1.4049e-01,  2.6862e-03, -2.8693e-02],\n                         [ 2.1352e-02,  4.1684e-02, -1.8909e-02]],\n               \n                        [[ 7.5165e-02,  3.9195e-02,  7.6217e-03],\n                         [ 5.7578e-02,  1.7096e-01,  2.1409e-02],\n                         [ 4.3060e-02,  1.1667e-01,  1.9361e-02]],\n               \n                        [[ 1.0620e-03,  7.7967e-02,  1.1018e-01],\n                         [ 9.6194e-02, -1.8608e-02,  1.4886e-01],\n                         [ 2.4770e-02,  1.4744e-01,  3.3076e-02]]],\n               \n               \n                       ...,\n               \n               \n                       [[[-8.6904e-02, -9.7985e-02, -1.0595e-01],\n                         [ 1.8032e-02,  1.4246e-01,  1.3508e-01],\n                         [-4.1484e-02,  1.0525e-01, -5.0754e-03]],\n               \n                        [[-5.0395e-02,  1.5078e-02, -9.4879e-02],\n                         [ 1.2953e-01,  1.6405e-01,  1.1712e-01],\n                         [ 6.9538e-02,  1.3598e-01,  1.3459e-01]],\n               \n                        [[ 2.2388e-02,  5.6009e-02, -5.3144e-02],\n                         [-6.8571e-02,  6.3737e-02, -9.8975e-03],\n                         [ 4.7304e-02, -1.5276e-01,  2.6774e-03]],\n               \n                        ...,\n               \n                        [[ 8.6126e-02,  8.0232e-03,  2.7336e-02],\n                         [-2.6002e-02,  1.0957e-01, -4.6730e-02],\n                         [ 5.7467e-02,  1.9642e-02,  9.9708e-02]],\n               \n                        [[ 7.7120e-02,  6.4573e-02,  1.1290e-01],\n                         [-1.5411e-01, -1.5238e-01, -6.7142e-02],\n                         [-7.2146e-02, -1.6291e-01, -7.6860e-02]],\n               \n                        [[ 1.2823e-01,  5.5068e-02,  5.6306e-02],\n                         [ 5.8080e-02,  2.0422e-01,  1.9241e-01],\n                         [-4.4379e-02,  1.8097e-01,  4.0617e-02]]],\n               \n               \n                       [[[ 8.8246e-02, -9.9385e-02,  3.3920e-02],\n                         [ 1.7348e-02,  3.1704e-02,  1.1842e-01],\n                         [ 8.1615e-02,  8.8342e-02,  1.1974e-01]],\n               \n                        [[ 7.0796e-02,  4.1829e-02,  1.5462e-01],\n                         [ 1.4050e-01,  1.1236e-01,  1.3426e-01],\n                         [ 9.5525e-02,  8.7298e-02,  1.1901e-01]],\n               \n                        [[-1.2332e-01, -1.5917e-01, -1.0719e-01],\n                         [-6.0568e-02, -8.6021e-03, -7.1744e-02],\n                         [-5.4239e-02,  7.7160e-02,  1.3832e-01]],\n               \n                        ...,\n               \n                        [[ 2.6500e-02, -1.0238e-02,  7.3250e-02],\n                         [ 1.5034e-01,  1.6793e-02,  1.7548e-01],\n                         [ 3.4751e-02,  1.6964e-01,  1.1387e-01]],\n               \n                        [[-4.1861e-02, -3.9665e-02,  6.0297e-02],\n                         [-1.0013e-01, -9.9841e-02, -6.0779e-02],\n                         [-1.4354e-01, -1.0104e-01, -1.9809e-01]],\n               \n                        [[-5.7950e-02, -3.5473e-02, -6.6118e-02],\n                         [-3.4945e-02, -1.8534e-02, -1.5415e-02],\n                         [ 1.0760e-01, -3.2335e-02,  1.9605e-02]]],\n               \n               \n                       [[[-2.6229e-03, -1.5977e-01,  9.6119e-03],\n                         [ 9.8752e-02, -7.7874e-02, -2.8447e-02],\n                         [-1.5650e-01, -6.9296e-02, -1.0172e-01]],\n               \n                        [[-3.9715e-02,  4.6446e-02,  3.7243e-03],\n                         [ 3.5576e-02, -1.3441e-01,  4.9395e-02],\n                         [-8.5025e-02, -8.2643e-03, -4.6332e-02]],\n               \n                        [[ 3.3774e-02, -6.8382e-05,  3.7692e-02],\n                         [-1.2680e-01, -9.9626e-02, -1.2906e-01],\n                         [ 1.5727e-01, -1.3932e-01,  2.7809e-01]],\n               \n                        ...,\n               \n                        [[ 2.3895e-02, -9.8147e-02,  3.4542e-02],\n                         [-1.7771e-01, -1.0929e-01,  4.3614e-03],\n                         [ 7.0138e-03,  7.7701e-02,  1.5575e-01]],\n               \n                        [[ 3.0664e-02,  2.1464e-02,  8.1261e-02],\n                         [-4.6185e-02,  8.9159e-02, -7.9625e-02],\n                         [-4.0537e-02, -2.6899e-02, -8.9230e-02]],\n               \n                        [[-7.5520e-04, -1.7683e-02,  1.0400e-01],\n                         [-8.1824e-03,  8.7610e-03,  9.0733e-03],\n                         [-1.1448e-01, -7.6061e-02, -1.3698e-02]]]], device='cuda:0')),\n              ('block_2.0.batch_norm1.weight',\n               tensor([0.8648, 0.9311, 0.9001, 0.8663, 1.0307, 0.9863, 0.9244, 1.0724, 0.9468,\n                       0.8870, 0.8528, 0.8510, 0.8529, 0.9436, 0.9789, 1.0279, 0.9061, 0.9471,\n                       0.8922, 0.8376, 0.8339, 0.8833, 1.0226, 0.9206, 0.9487, 0.9638, 0.8881,\n                       0.8535, 0.9422, 1.0225, 0.8422, 0.8660, 0.9007, 1.0368, 0.7900, 0.9269,\n                       0.8935, 0.9043, 0.9872, 0.9520, 0.9040, 0.9934, 0.8304, 0.8915, 0.8307,\n                       0.8952, 0.8906, 1.0129, 0.8220, 0.9338, 0.8467, 0.9016, 1.0669, 0.9312,\n                       1.0251, 0.9670, 0.9263, 1.0071, 0.9831, 0.8898, 1.0220, 0.9996, 1.0539,\n                       1.0394, 0.9852, 1.1723, 0.9093, 0.9538, 0.8693, 0.9835, 0.8958, 0.9556,\n                       1.0056, 1.0526, 1.0531, 0.9383, 1.0170, 0.9365, 0.9278, 0.9166, 0.9391,\n                       0.8139, 0.9747, 0.9830, 0.9481, 0.8898, 0.9206, 0.9129, 0.9552, 0.9313,\n                       0.8610, 0.8251, 0.8776, 0.8570, 0.9197, 0.9779, 0.8964, 0.8758, 0.9253,\n                       1.0131, 0.8381, 0.8363, 0.8271, 0.8340, 0.9052, 0.9834, 0.9258, 1.0261,\n                       0.8765, 1.0369, 0.9327, 0.9085, 0.9485, 0.9757, 0.9714, 0.9909, 0.9240,\n                       0.9643, 0.9924, 0.9393, 0.9822, 0.9562, 0.8190, 0.9436, 0.9384, 1.0237,\n                       0.9323, 1.0343], device='cuda:0')),\n              ('block_2.0.batch_norm1.bias',\n               tensor([-1.1576e-01,  2.1919e-02,  5.3331e-02,  3.5714e-02, -9.2620e-04,\n                        8.5480e-03, -3.3914e-03,  9.1824e-02, -4.0804e-02,  1.0806e-01,\n                       -1.0305e-01,  5.7626e-02, -1.9024e-02, -6.7248e-02, -8.6823e-03,\n                       -6.0999e-02,  1.6505e-01,  1.4959e-02,  2.2384e-02, -1.9573e-02,\n                        4.7200e-02, -1.7527e-01, -8.5595e-02, -9.6505e-02,  1.8464e-02,\n                        8.5816e-03, -7.4465e-02,  1.1960e-02, -5.8709e-02, -1.1869e-01,\n                        8.1039e-02,  1.2629e-01, -1.5889e-02, -1.9505e-02, -1.0631e-01,\n                       -4.9824e-02,  8.5377e-03, -8.5366e-03,  6.8137e-02, -2.0946e-02,\n                        1.1747e-01,  7.5044e-02,  2.2936e-01, -2.7335e-02,  3.0889e-03,\n                        5.0257e-03,  1.4026e-04, -1.1308e-01,  1.1413e-01, -8.7756e-02,\n                        3.9994e-02,  2.0106e-02,  9.9160e-03, -1.5446e-01, -7.4502e-02,\n                        9.4732e-02, -2.7684e-02, -4.4726e-02,  7.3805e-03, -2.5566e-02,\n                       -2.1807e-01,  6.6836e-02,  3.0959e-02,  2.5485e-02, -7.1570e-02,\n                       -5.6879e-02, -9.6391e-02, -4.2950e-02,  3.2677e-02,  4.6860e-02,\n                       -1.6566e-01, -3.9840e-02,  1.1993e-02, -4.6662e-02,  3.1095e-03,\n                       -6.0559e-02,  9.6440e-02,  9.4378e-02,  6.9120e-02, -1.5330e-02,\n                        1.4972e-01,  4.2727e-02,  2.3422e-01, -2.2281e-03, -2.6578e-02,\n                       -1.5991e-02, -4.5561e-02,  7.3846e-02,  2.4897e-02, -3.0600e-02,\n                       -6.4131e-02,  5.8134e-02, -2.8141e-02,  3.9374e-02, -9.0969e-03,\n                        1.3489e-01, -1.6336e-01,  1.0310e-01,  3.2508e-03, -5.8015e-02,\n                       -1.3021e-01,  1.1290e-01, -1.5391e-01, -3.8540e-02,  1.0848e-01,\n                       -3.1070e-02,  3.3661e-02, -5.1896e-04, -7.1008e-02, -3.8983e-02,\n                       -9.0728e-02, -3.8810e-02,  6.3748e-02,  1.5368e-01, -3.4566e-02,\n                        2.1979e-02,  1.2012e-01, -4.9596e-02,  3.7595e-02, -1.0750e-01,\n                        1.7993e-03, -1.2505e-01,  7.6581e-02, -8.9296e-02, -6.9315e-02,\n                       -2.6643e-02, -4.2674e-02, -1.0356e-02], device='cuda:0')),\n              ('block_2.0.batch_norm1.running_mean',\n               tensor([-2.9240,  1.2024,  2.2477, -0.7250,  0.0865,  0.6511,  0.5051, -0.0566,\n                       -0.1964,  2.9892, -1.9104,  3.3007,  0.3661,  0.0036,  1.1114, -0.5180,\n                       -1.2099,  0.5824, -1.2741, -2.1942, -3.1885, -2.4337, -0.1144,  2.3861,\n                        0.6225,  0.1049, -0.2154,  1.4962,  2.3631,  0.5482, -0.6177,  1.0025,\n                        0.2900,  1.4457, -2.6070,  0.5654,  3.1542,  0.3176,  0.3234,  0.4221,\n                        0.9979, -0.6011,  3.1501,  1.6399,  1.4941,  0.6358,  0.8295, -1.7193,\n                        2.7825, -1.8879,  2.5668, -2.1104, -0.0503, -2.5852, -0.0446,  0.2019,\n                        0.2928, -0.6478, -0.1842, -2.4587, -0.5549, -0.4637,  0.4733, -1.4886,\n                       -0.3487,  0.6077, -2.6740, -0.3833,  2.5181,  1.6206,  0.2064,  0.6441,\n                        0.4822,  0.6173, -0.5629, -1.6460, -0.3452, -1.3831,  0.4109, -0.9964,\n                        2.6874, -3.2641,  0.2096,  0.7723,  2.3829,  0.7311, -2.6083, -2.2024,\n                       -2.8983,  1.8400,  1.7520, -2.8490, -1.4569, -3.4758,  2.6672,  0.6116,\n                       -0.9693, -1.7685, -0.6095,  1.3145, -2.5694, -1.0155, -3.4995,  3.2104,\n                        1.0441, -0.6575,  0.4827, -1.0196, -1.9618, -0.2846, -2.8596, -3.2737,\n                        0.6310,  1.5604,  1.0989,  0.2645,  1.9347, -0.4843, -0.4646, -0.3126,\n                        0.1910, -1.4691, -1.8174, -2.0094,  0.4496, -0.3714, -3.1868,  0.4775],\n                      device='cuda:0')),\n              ('block_2.0.batch_norm1.running_var',\n               tensor([38.3102, 14.9806, 31.1342, 26.7006,  7.7889, 10.2666, 18.0640,  9.5750,\n                       18.4366, 37.0094, 24.9895, 25.8650, 27.3278, 17.0225, 31.7064, 14.3841,\n                       19.8499, 13.2073, 26.1691, 25.1965, 39.2374, 26.6712, 17.5737, 35.6491,\n                       27.1948, 11.3492, 16.7407, 33.4397, 38.4679, 14.1708, 11.4164, 18.9770,\n                       37.1628, 11.6775, 22.9442, 26.2618, 32.0959, 26.9200, 25.5886, 20.9314,\n                       19.9404, 16.0095, 31.7590, 26.1405, 23.4237, 12.4952, 15.7337, 22.3559,\n                       26.3033, 18.5691, 26.8071, 33.7909, 16.5143, 33.2857, 23.5530, 11.5937,\n                       17.3130,  7.3047, 22.4657, 19.8774, 16.7234, 12.1421, 16.4401, 21.4041,\n                        7.8099, 11.1791, 34.6133, 14.1773, 22.3357, 25.9324, 16.3002, 27.6701,\n                       20.5393, 20.6328, 10.1183, 17.6017, 20.6611, 27.5644, 19.9130, 26.8262,\n                       26.7375, 46.3901, 12.9984, 16.5534, 43.4416, 29.2906, 29.6645, 33.3440,\n                       27.8063, 33.7916, 21.8222, 29.9809, 31.4767, 43.8118, 29.2673, 10.7521,\n                       18.4059, 34.5929, 20.9689, 19.7722, 36.9692, 24.8774, 29.6167, 32.7461,\n                       25.2279, 12.3931, 12.9274, 26.8914, 26.6513, 22.3840, 24.9935, 35.7158,\n                       14.5544, 18.4964, 30.0158, 23.2649, 17.3385, 18.8398, 11.7313, 11.3289,\n                        7.1515, 19.3068, 33.6266, 25.8876, 25.4465, 14.3564, 33.6489, 12.1371],\n                      device='cuda:0')),\n              ('block_2.0.batch_norm1.num_batches_tracked',\n               tensor(10550, device='cuda:0')),\n              ('block_2.0.conv1.weight',\n               tensor([[[[ 0.1482]],\n               \n                        [[-0.1084]],\n               \n                        [[-0.0651]],\n               \n                        ...,\n               \n                        [[-0.0081]],\n               \n                        [[ 0.0868]],\n               \n                        [[-0.1482]]],\n               \n               \n                       [[[ 0.0276]],\n               \n                        [[ 0.1428]],\n               \n                        [[-0.1203]],\n               \n                        ...,\n               \n                        [[ 0.0580]],\n               \n                        [[ 0.0918]],\n               \n                        [[-0.0336]]],\n               \n               \n                       [[[-0.2921]],\n               \n                        [[-0.0495]],\n               \n                        [[ 0.2140]],\n               \n                        ...,\n               \n                        [[-0.0446]],\n               \n                        [[-0.2089]],\n               \n                        [[ 0.1996]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.0589]],\n               \n                        [[ 0.0328]],\n               \n                        [[ 0.0293]],\n               \n                        ...,\n               \n                        [[-0.0206]],\n               \n                        [[-0.0669]],\n               \n                        [[ 0.1325]]],\n               \n               \n                       [[[-0.0561]],\n               \n                        [[-0.2377]],\n               \n                        [[ 0.0805]],\n               \n                        ...,\n               \n                        [[-0.0264]],\n               \n                        [[-0.0833]],\n               \n                        [[-0.0876]]],\n               \n               \n                       [[[ 0.0385]],\n               \n                        [[ 0.0571]],\n               \n                        [[-0.0160]],\n               \n                        ...,\n               \n                        [[-0.1573]],\n               \n                        [[-0.0721]],\n               \n                        [[-0.0803]]]], device='cuda:0')),\n              ('block_2.0.conv1.bias',\n               tensor([-2.1816e-01,  3.4911e-02,  1.2969e-01,  1.0440e-01,  4.0085e-02,\n                        4.0784e-02,  8.2031e-02,  1.6821e-01, -4.1754e-02, -6.5656e-03,\n                        1.7878e-01,  1.0758e-01, -4.8935e-02, -9.0418e-02,  9.1139e-03,\n                        1.2756e-01,  2.5938e-02,  5.6078e-02, -3.4730e-02, -1.4180e-01,\n                        5.6712e-02,  1.3262e-02,  6.4062e-02,  1.9139e-02,  1.5989e-01,\n                        1.2959e-02, -7.9427e-02,  5.4034e-02, -2.4476e-03, -1.2048e-01,\n                       -1.2149e-01, -4.8283e-03,  2.8413e-02,  8.0280e-02, -3.4858e-02,\n                        7.0636e-02, -6.7334e-02,  5.3897e-02,  7.9892e-02,  2.0713e-02,\n                       -2.1423e-01,  1.6486e-02,  2.1373e-01,  1.2722e-01,  5.5634e-02,\n                       -1.0529e-01, -1.5498e-01,  2.5449e-02, -1.8111e-02,  6.5585e-02,\n                       -2.3789e-02,  1.0299e-01, -2.5435e-02,  4.2283e-02, -6.6818e-02,\n                       -7.4209e-02, -1.4841e-01, -4.1388e-02,  1.4995e-01,  2.0629e-01,\n                        2.0721e-02,  7.1683e-02, -7.1082e-02, -1.6177e-02,  2.2536e-01,\n                       -4.8462e-02,  1.4010e-01,  1.1006e-01, -3.5163e-02,  5.6677e-02,\n                       -2.6057e-02,  5.7696e-02,  1.2010e-01,  2.5867e-03,  1.4191e-01,\n                        1.1961e-01, -4.2950e-03,  1.0167e-01,  1.9020e-01, -3.7746e-02,\n                        6.4010e-02,  2.7200e-01, -9.2234e-02,  6.8919e-02,  2.0981e-01,\n                        5.4399e-02,  1.9024e-01,  1.0634e-01, -2.1837e-02,  3.1245e-01,\n                       -3.7383e-02, -1.2015e-01, -5.1666e-05,  3.0930e-01,  2.1266e-02,\n                        1.9562e-01, -8.7019e-02,  1.1442e-01,  5.9973e-02,  5.9964e-02,\n                        6.0193e-02, -5.8839e-02, -2.0242e-02, -5.6077e-02,  8.4238e-04,\n                        2.4624e-01,  1.2836e-01, -2.9563e-01,  1.5038e-02,  2.0339e-01,\n                        4.2071e-02,  2.2443e-02,  3.3001e-02, -8.9846e-03,  1.8601e-01,\n                       -2.2155e-01, -1.4977e-01,  6.0876e-02, -3.1064e-02,  1.7616e-01,\n                       -2.3848e-01,  1.4649e-01, -2.2472e-01, -4.1420e-02,  3.8905e-02,\n                        7.9126e-02,  1.2535e-01,  2.0212e-01], device='cuda:0')),\n              ('block_2.0.LKA.conv1.weight',\n               tensor([[[[-0.1584,  0.1272,  0.0999, -0.2542, -0.0194],\n                         [ 0.0452, -0.1606, -0.1710, -0.1349, -0.1286],\n                         [ 0.1461,  0.1118,  0.0747,  0.0235, -0.0421],\n                         [ 0.1454,  0.0502, -0.1271,  0.0104,  0.1365],\n                         [ 0.1348,  0.0800,  0.2806,  0.0066,  0.0008]]],\n               \n               \n                       [[[-0.1580,  0.1511, -0.1322, -0.1543, -0.0586],\n                         [ 0.1671,  0.1028, -0.0459, -0.0418, -0.0249],\n                         [-0.0107, -0.1133, -0.0157,  0.0927,  0.0594],\n                         [ 0.0903,  0.1085,  0.3178, -0.1318, -0.3964],\n                         [-0.1942,  0.1010, -0.1379, -0.0446,  0.0276]]],\n               \n               \n                       [[[-0.0547, -0.0236,  0.0247,  0.0483, -0.2738],\n                         [ 0.0335, -0.2899, -0.0640,  0.1450,  0.2802],\n                         [-0.1716,  0.1587,  0.3327,  0.1526, -0.1182],\n                         [ 0.1826,  0.2195, -0.1768, -0.0540, -0.2373],\n                         [ 0.1397, -0.0520,  0.0098,  0.0701, -0.1512]]],\n               \n               \n                       ...,\n               \n               \n                       [[[-0.2045, -0.1315,  0.0447,  0.0240, -0.3312],\n                         [ 0.0169, -0.1219, -0.1116, -0.1276, -0.2496],\n                         [ 0.1432,  0.2104, -0.1423, -0.1449, -0.0717],\n                         [ 0.3038,  0.2949,  0.1407, -0.0570,  0.0167],\n                         [-0.0313,  0.0120, -0.0391, -0.0818, -0.1038]]],\n               \n               \n                       [[[-0.1890,  0.0720,  0.2503,  0.2423, -0.0820],\n                         [ 0.2138, -0.2128,  0.0564, -0.0573, -0.0653],\n                         [-0.0897, -0.0094, -0.1382, -0.0958, -0.0350],\n                         [ 0.1299,  0.2271,  0.2845, -0.1588, -0.0519],\n                         [-0.1693, -0.0810, -0.0530,  0.1764,  0.3334]]],\n               \n               \n                       [[[ 0.0345,  0.0318, -0.1764, -0.1916, -0.2361],\n                         [ 0.2046,  0.1867,  0.0624,  0.1802,  0.1817],\n                         [ 0.2097, -0.1270, -0.1574, -0.2603, -0.1110],\n                         [ 0.0559, -0.1258,  0.1192,  0.1704,  0.3156],\n                         [ 0.2186,  0.0498,  0.0189,  0.0461,  0.2064]]]], device='cuda:0')),\n              ('block_2.0.LKA.conv1.bias',\n               tensor([-0.1218, -0.2027, -0.0224,  0.0892, -0.0456, -0.2356, -0.1154,  0.3200,\n                        0.1872, -0.1128,  0.1015, -0.1483, -0.0993,  0.0107,  0.0385,  0.0609,\n                        0.0889, -0.0860, -0.2458, -0.2088,  0.0780, -0.0725,  0.1426, -0.0170,\n                        0.0872, -0.2346,  0.1208, -0.2328, -0.0974, -0.2707, -0.0488, -0.0408,\n                       -0.1540, -0.0024, -0.1897, -0.1982, -0.1616, -0.0473,  0.0113,  0.1718,\n                       -0.1310,  0.0236,  0.0803,  0.0059, -0.0026, -0.0843,  0.1443,  0.0511,\n                       -0.1445, -0.1023,  0.2183,  0.0337, -0.0888,  0.0253, -0.2001,  0.1132,\n                        0.0100,  0.2008,  0.0175,  0.2478,  0.1083,  0.0331,  0.1137, -0.0095,\n                        0.0720, -0.0100,  0.1236, -0.0415,  0.1778,  0.1011,  0.1492,  0.0340,\n                        0.0810, -0.0260,  0.2313,  0.1045,  0.2715, -0.2064, -0.0434,  0.1021,\n                       -0.1835, -0.1577,  0.0082, -0.0946, -0.1957, -0.1662, -0.1859, -0.0944,\n                        0.1461,  0.2670,  0.1410, -0.0978,  0.0147, -0.0866, -0.1405, -0.2372,\n                       -0.1760, -0.2464, -0.0772, -0.2545, -0.2838, -0.1430, -0.0614,  0.0795,\n                        0.0961,  0.1713, -0.1335, -0.1853, -0.0087,  0.0279,  0.2530,  0.0053,\n                       -0.1926,  0.1709, -0.1216, -0.0355,  0.1878,  0.1128, -0.0978, -0.0493,\n                        0.1562,  0.2456,  0.1723,  0.1185, -0.0265,  0.1057, -0.1669,  0.1613],\n                      device='cuda:0')),\n              ('block_2.0.LKA.conv2.weight',\n               tensor([[[[ 0.1549, -0.0802, -0.0885, -0.1157, -0.0142],\n                         [ 0.1707, -0.0148,  0.1307,  0.1789, -0.0439],\n                         [-0.0411, -0.1028,  0.1499,  0.0555, -0.0024],\n                         [-0.1548, -0.1605,  0.1799,  0.3379,  0.0433],\n                         [ 0.1591, -0.1072, -0.0453,  0.1332,  0.0132]]],\n               \n               \n                       [[[-0.0363,  0.1130, -0.0763,  0.0441,  0.1568],\n                         [-0.0550,  0.0979,  0.3015,  0.0806,  0.0545],\n                         [-0.0541, -0.0660, -0.0158, -0.0820, -0.0754],\n                         [ 0.0704,  0.3135, -0.1014,  0.1590,  0.0940],\n                         [ 0.1511,  0.1135, -0.0406,  0.1286,  0.1714]]],\n               \n               \n                       [[[ 0.0358, -0.0088, -0.0623,  0.0005, -0.1639],\n                         [-0.1179, -0.4484,  0.1127, -0.1310,  0.1237],\n                         [-0.0971, -0.0612,  0.0092,  0.1813,  0.0136],\n                         [-0.1569,  0.0156, -0.3035, -0.2358, -0.0241],\n                         [-0.1175, -0.1589,  0.0365,  0.0252,  0.1398]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.0075, -0.1654, -0.1245, -0.0100,  0.0920],\n                         [-0.0498, -0.0268,  0.0304,  0.0293,  0.1095],\n                         [-0.1173,  0.2746,  0.1961,  0.0392, -0.0065],\n                         [-0.1124,  0.0696,  0.0329, -0.0427, -0.0978],\n                         [ 0.1347,  0.0171, -0.0099, -0.1089, -0.0231]]],\n               \n               \n                       [[[-0.0740,  0.1018, -0.1121, -0.1305,  0.0892],\n                         [ 0.1791, -0.0262, -0.3192,  0.2588, -0.1121],\n                         [ 0.1143,  0.1608,  0.1526,  0.1438,  0.0135],\n                         [ 0.0992, -0.1821,  0.2428,  0.2654, -0.0270],\n                         [ 0.1571,  0.0577,  0.1103, -0.1667, -0.1508]]],\n               \n               \n                       [[[ 0.0670, -0.0652,  0.1609,  0.0389,  0.0211],\n                         [-0.1777,  0.0471, -0.2636, -0.1175, -0.0171],\n                         [-0.1276, -0.0088, -0.0455,  0.0851,  0.1577],\n                         [ 0.1251, -0.0030, -0.1228, -0.2939, -0.0124],\n                         [-0.0502,  0.1324, -0.0251, -0.0042,  0.0611]]]], device='cuda:0')),\n              ('block_2.0.LKA.conv2.bias',\n               tensor([ 0.0379,  0.0216,  0.0334,  0.1622, -0.0176,  0.0061,  0.0388, -0.0980,\n                       -0.0090, -0.0666, -0.0745,  0.0125,  0.1051, -0.0131, -0.1083,  0.0235,\n                        0.0711, -0.0034,  0.0026, -0.0338,  0.0017, -0.1524,  0.0524,  0.0707,\n                       -0.0667, -0.0653, -0.0567,  0.0181,  0.0508, -0.0467, -0.0194, -0.0401,\n                       -0.0081,  0.0245, -0.0863,  0.0011,  0.0020,  0.0673, -0.0016,  0.0257,\n                       -0.0329,  0.0982,  0.0515,  0.0810,  0.0491,  0.0350,  0.0248, -0.0097,\n                       -0.0164,  0.0127, -0.0582,  0.0348, -0.0235,  0.0164, -0.0399, -0.0159,\n                        0.1079, -0.0507,  0.1115,  0.1326,  0.1145, -0.0369,  0.0757, -0.0438,\n                        0.0941,  0.0390, -0.0772, -0.0439, -0.0328,  0.0049, -0.0541,  0.1058,\n                       -0.0843, -0.1325, -0.0974, -0.0117,  0.0867, -0.0145, -0.0167,  0.0861,\n                       -0.0982, -0.0371, -0.0924, -0.0299, -0.0637, -0.0441,  0.0222, -0.0474,\n                        0.0584,  0.0940, -0.1121, -0.0809,  0.0724,  0.0245, -0.0039, -0.0310,\n                       -0.0032,  0.0478, -0.0640,  0.0155,  0.0227,  0.0490,  0.0435,  0.0719,\n                       -0.0362, -0.1042, -0.1486,  0.0118, -0.0095, -0.0426,  0.0801,  0.0225,\n                       -0.0698,  0.1119,  0.0246,  0.0165, -0.0749,  0.1147, -0.0148, -0.0200,\n                        0.0157, -0.0503,  0.0324, -0.0474, -0.0046,  0.0081, -0.0771, -0.0211],\n                      device='cuda:0')),\n              ('block_2.0.LKA.conv3.weight',\n               tensor([[[[-0.1719]],\n               \n                        [[-0.0884]],\n               \n                        [[-0.0704]],\n               \n                        ...,\n               \n                        [[ 0.2109]],\n               \n                        [[-0.0938]],\n               \n                        [[ 0.0213]]],\n               \n               \n                       [[[-0.0505]],\n               \n                        [[-0.0449]],\n               \n                        [[-0.0966]],\n               \n                        ...,\n               \n                        [[-0.0417]],\n               \n                        [[ 0.0670]],\n               \n                        [[ 0.1003]]],\n               \n               \n                       [[[-0.0982]],\n               \n                        [[ 0.0509]],\n               \n                        [[ 0.0465]],\n               \n                        ...,\n               \n                        [[ 0.1232]],\n               \n                        [[ 0.0576]],\n               \n                        [[-0.0304]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.0822]],\n               \n                        [[ 0.1182]],\n               \n                        [[ 0.0614]],\n               \n                        ...,\n               \n                        [[ 0.0463]],\n               \n                        [[ 0.1595]],\n               \n                        [[ 0.1475]]],\n               \n               \n                       [[[ 0.0958]],\n               \n                        [[ 0.0156]],\n               \n                        [[ 0.1483]],\n               \n                        ...,\n               \n                        [[-0.0781]],\n               \n                        [[ 0.1219]],\n               \n                        [[ 0.0603]]],\n               \n               \n                       [[[-0.0230]],\n               \n                        [[ 0.1748]],\n               \n                        [[ 0.0739]],\n               \n                        ...,\n               \n                        [[ 0.0197]],\n               \n                        [[ 0.1312]],\n               \n                        [[-0.0082]]]], device='cuda:0')),\n              ('block_2.0.LKA.conv3.bias',\n               tensor([-0.1343, -0.0725, -0.0582,  0.0200,  0.1920,  0.0289, -0.0979, -0.0802,\n                        0.0032,  0.1175, -0.0530,  0.0268, -0.0251, -0.0017,  0.1252, -0.0008,\n                       -0.0662,  0.0624,  0.0097, -0.0424, -0.0665, -0.1627,  0.0367,  0.0143,\n                       -0.0397, -0.0654, -0.0036,  0.0553,  0.0682,  0.1554,  0.0779, -0.0274,\n                       -0.1025,  0.1763,  0.0233, -0.0552, -0.0586,  0.0773, -0.1369,  0.1775,\n                        0.0864, -0.1059, -0.0847, -0.0346,  0.0615,  0.0401,  0.0550, -0.2288,\n                       -0.0110, -0.0330,  0.0342,  0.1072,  0.0429, -0.1315,  0.1336,  0.0197,\n                       -0.1676,  0.0285, -0.0158,  0.0328, -0.0275,  0.1223, -0.0429, -0.0313,\n                       -0.2205,  0.0688,  0.0107, -0.0202, -0.1575, -0.0922, -0.1087, -0.0969,\n                        0.0143,  0.0815,  0.0166, -0.1093,  0.1084,  0.0538,  0.0061,  0.0553,\n                        0.1414,  0.0063, -0.0835, -0.0069, -0.1729, -0.0009,  0.0136,  0.0882,\n                       -0.0585,  0.0277, -0.0515, -0.0466, -0.0741, -0.0921, -0.0345, -0.0683,\n                       -0.0270, -0.0044, -0.0506,  0.0783,  0.0569,  0.0553,  0.1218, -0.0473,\n                        0.0844,  0.0045, -0.0277,  0.0895,  0.0192,  0.0731,  0.0801,  0.0290,\n                       -0.1057, -0.0922,  0.0729, -0.2159,  0.1156,  0.0092, -0.0012, -0.0408,\n                       -0.1526,  0.0401,  0.1791, -0.0897, -0.0173,  0.0411, -0.0992,  0.0194],\n                      device='cuda:0')),\n              ('block_2.0.batch_norm2.weight',\n               tensor([0.9861, 1.0041, 0.9197, 1.0170, 0.8315, 0.8994, 0.9162, 0.9162, 0.9415,\n                       1.0085, 0.8710, 1.1335, 0.9746, 0.9675, 1.1387, 0.8958, 0.8847, 0.9349,\n                       0.8942, 0.9399, 0.9043, 0.8966, 1.0088, 0.9554, 0.8685, 0.9915, 1.0712,\n                       0.8548, 0.9859, 0.8614, 1.0241, 0.7766, 0.8957, 0.9074, 0.9428, 0.8791,\n                       1.0017, 0.9339, 1.0243, 0.9131, 1.0210, 0.9894, 1.0053, 0.9250, 0.8407,\n                       0.8317, 0.8053, 0.9495, 0.8999, 0.8429, 1.0250, 0.8430, 0.9481, 0.9030,\n                       1.0417, 0.9927, 1.0173, 0.9734, 1.0410, 0.8797, 0.9257, 0.8218, 0.9799,\n                       1.1948, 0.8933, 0.9726, 0.8793, 0.9946, 0.8983, 0.9583, 0.9350, 0.8353,\n                       1.0346, 0.7592, 0.9004, 0.9571, 0.8478, 0.9319, 0.8913, 0.8330, 0.8405,\n                       0.9008, 1.0007, 0.8003, 0.9798, 0.9234, 0.9230, 1.0044, 1.0355, 1.0460,\n                       0.7976, 0.8895, 1.0352, 0.8793, 0.9148, 0.9760, 0.9901, 0.8613, 1.1454,\n                       0.9971, 0.9740, 1.0796, 0.8091, 0.8326, 0.9587, 0.9552, 0.8391, 1.1385,\n                       0.8241, 0.9849, 0.9553, 1.0485, 0.9972, 0.9159, 0.9593, 0.8325, 0.9556,\n                       0.8659, 0.9452, 0.8757, 0.9328, 0.8412, 1.0511, 0.9616, 0.9200, 0.8845,\n                       0.9646, 0.9869], device='cuda:0')),\n              ('block_2.0.batch_norm2.bias',\n               tensor([-0.0603,  0.0047,  0.0154, -0.0329, -0.0418, -0.0009,  0.0471,  0.0177,\n                       -0.0428,  0.0506, -0.0595, -0.0317, -0.0017,  0.0449,  0.0100,  0.0224,\n                       -0.0059,  0.0149, -0.0062, -0.0043, -0.0226,  0.0662, -0.0416, -0.0045,\n                       -0.0232,  0.0206,  0.0310, -0.0499, -0.0530, -0.0179, -0.0488, -0.0787,\n                        0.0040,  0.0085,  0.0333,  0.0092, -0.0158, -0.0543, -0.0516,  0.0589,\n                       -0.0759,  0.0269, -0.0217,  0.0672,  0.0519, -0.0577,  0.0997, -0.0751,\n                        0.0065, -0.0510,  0.0603,  0.0502, -0.0642, -0.0678,  0.0538,  0.0594,\n                        0.0186,  0.1011, -0.0121,  0.0279, -0.0275, -0.0246,  0.0300,  0.0518,\n                       -0.0348, -0.0262, -0.0184, -0.0788, -0.0483,  0.0588, -0.0270, -0.0592,\n                        0.0333, -0.0346, -0.0559,  0.0353, -0.0011, -0.0255, -0.0404, -0.0374,\n                       -0.0414,  0.0018,  0.0680, -0.0191, -0.0455,  0.0506, -0.0159,  0.0018,\n                        0.0112,  0.0334, -0.0570, -0.0209,  0.0214,  0.0159,  0.0946, -0.0122,\n                       -0.0572, -0.0123, -0.0860,  0.0012,  0.0027, -0.0122,  0.0036,  0.0042,\n                       -0.0304, -0.1124,  0.0824,  0.0736, -0.0714, -0.0070,  0.0446,  0.0103,\n                       -0.0090,  0.0008, -0.0114,  0.0705, -0.0788,  0.0355,  0.1033,  0.0155,\n                        0.0379, -0.0496,  0.0314, -0.0230,  0.0139,  0.0783,  0.0521,  0.0477],\n                      device='cuda:0')),\n              ('block_2.0.batch_norm2.running_mean',\n               tensor([-3.3904e-01,  6.1189e-02,  4.8734e-02,  3.3695e-01,  8.7740e-01,\n                       -9.8159e-02, -3.9193e-01, -2.1902e-01,  9.8380e-02,  1.2225e-01,\n                       -2.9690e-01, -3.6219e-02, -4.1253e-01,  1.4339e-01,  3.4861e-01,\n                        2.8157e-01, -2.6976e-01, -7.7383e-02,  1.7431e-01, -2.2360e-01,\n                       -2.6305e-01, -4.2251e-01, -1.3819e-02,  2.5534e-01, -1.0942e-01,\n                        6.8570e-02, -9.1618e-02,  1.2885e-01,  2.9380e-01,  4.6348e-01,\n                        1.0075e-01, -2.6571e-01, -3.6985e-01,  4.3816e-01,  1.6117e-01,\n                       -4.6888e-01, -1.9494e-01,  4.0533e-01, -1.4492e-01,  4.7995e-01,\n                        1.5384e-01, -2.1103e-01, -9.7605e-02, -6.8274e-02,  4.5565e-01,\n                        2.9133e-01,  2.5578e-01, -6.2953e-01,  1.9014e-01, -1.7897e-01,\n                        7.1695e-02,  7.3123e-01, -2.7568e-03, -5.0171e-01,  5.1690e-01,\n                        2.4197e-01, -3.2423e-01,  5.5220e-02, -7.6496e-02,  4.0043e-01,\n                       -5.9008e-01,  4.6801e-01, -1.5265e-01, -2.5675e-01, -7.3406e-01,\n                        1.3263e-01,  8.0129e-02,  9.0669e-02, -6.4264e-01, -1.9268e-01,\n                       -6.8278e-01, -2.3780e-01, -2.3173e-02,  1.9865e-01, -1.7630e-03,\n                       -4.0927e-01,  3.5695e-01,  2.3056e-02, -2.1880e-01, -5.3027e-02,\n                        3.0764e-01, -3.0497e-01, -2.2496e-01, -1.5418e-01, -8.2692e-01,\n                        2.9493e-02,  8.4970e-04,  4.6133e-01, -2.8189e-01, -1.5088e-01,\n                        6.7443e-02, -3.5243e-02,  9.8588e-02, -3.1858e-01, -1.7856e-01,\n                       -6.7151e-02, -2.7902e-01, -1.3510e-01, -1.7620e-01,  4.2609e-01,\n                        4.5295e-02,  2.2724e-01,  5.8924e-01, -1.8945e-01, -3.6432e-02,\n                       -3.0068e-01, -3.7190e-01,  9.0699e-02,  1.9313e-01,  1.0151e-01,\n                        2.5536e-01,  3.9918e-01, -6.8670e-02, -2.0068e-01,  1.7850e-01,\n                       -4.7582e-01,  2.4172e-01,  1.8204e-01,  6.1898e-02, -3.2767e-02,\n                       -4.0386e-01, -1.1460e-01,  5.2963e-01, -2.4000e-01, -3.0915e-01,\n                       -1.6977e-01, -3.0946e-01, -1.2937e-01], device='cuda:0')),\n              ('block_2.0.batch_norm2.running_var',\n               tensor([ 2.5735,  2.0186, 11.7633,  9.5003, 15.0936,  5.3176,  4.1562, 11.5413,\n                        5.5105,  2.9369,  3.5384,  6.9389,  9.1900,  9.7387,  5.3333,  3.7194,\n                        3.3902,  4.1930,  2.6669,  3.4083,  3.5778,  5.0390,  4.4433,  3.9538,\n                        9.1077,  3.6179,  5.0922,  9.8680,  6.9867,  3.5477,  1.6735, 17.4170,\n                        5.7357,  4.1278, 10.3786,  4.2198,  2.4247, 12.3643,  4.2285,  3.1573,\n                        3.4048,  5.9502,  2.7453,  6.5596, 12.4410,  4.2100,  5.6750,  5.6276,\n                        2.1372,  7.8815,  3.9547, 10.6647,  3.0553,  6.6693,  5.4402,  2.7503,\n                        3.9632,  4.4076,  6.7227, 10.6639,  9.2415,  6.2418,  4.4724,  3.1608,\n                        8.6909,  9.8742,  2.2364,  4.2945,  5.2057,  5.1891, 10.3388,  3.8410,\n                        2.4548,  7.9951,  4.4976,  8.3098,  7.1675,  3.6844,  3.3323,  5.3184,\n                        5.4564,  9.5836,  4.3446, 11.1636,  8.3656,  5.0909,  4.7024,  5.2953,\n                        4.2085,  3.6903,  2.4091,  3.2504,  2.6253,  7.2909, 13.6830,  7.2608,\n                        7.7653,  2.2246,  3.7573,  7.7534,  4.9757,  2.4458,  7.0228,  2.8177,\n                        3.6964, 13.9793,  9.3134,  1.9262,  2.7332,  7.1897,  6.0463,  3.1475,\n                        1.5988,  5.7547,  5.3481,  4.3177,  4.2161,  3.4909,  3.2906,  6.7133,\n                        4.0313, 12.4126,  4.2805,  3.3963,  4.0223,  2.5417,  3.9956,  4.4250],\n                      device='cuda:0')),\n              ('block_2.0.batch_norm2.num_batches_tracked',\n               tensor(10550, device='cuda:0')),\n              ('block_2.0.FFN.conv1.weight',\n               tensor([[[[-0.2342]],\n               \n                        [[-0.0589]],\n               \n                        [[ 0.0337]],\n               \n                        ...,\n               \n                        [[ 0.0229]],\n               \n                        [[-0.0762]],\n               \n                        [[ 0.1462]]],\n               \n               \n                       [[[-0.1223]],\n               \n                        [[-0.2030]],\n               \n                        [[ 0.0437]],\n               \n                        ...,\n               \n                        [[ 0.1239]],\n               \n                        [[-0.0131]],\n               \n                        [[-0.0278]]],\n               \n               \n                       [[[-0.0630]],\n               \n                        [[-0.0601]],\n               \n                        [[ 0.0590]],\n               \n                        ...,\n               \n                        [[ 0.0026]],\n               \n                        [[ 0.0014]],\n               \n                        [[ 0.1126]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.1699]],\n               \n                        [[ 0.1453]],\n               \n                        [[ 0.0331]],\n               \n                        ...,\n               \n                        [[-0.0390]],\n               \n                        [[-0.0043]],\n               \n                        [[ 0.1009]]],\n               \n               \n                       [[[ 0.0805]],\n               \n                        [[ 0.0041]],\n               \n                        [[ 0.0400]],\n               \n                        ...,\n               \n                        [[-0.0476]],\n               \n                        [[-0.0033]],\n               \n                        [[ 0.0816]]],\n               \n               \n                       [[[-0.1676]],\n               \n                        [[-0.0678]],\n               \n                        [[-0.1867]],\n               \n                        ...,\n               \n                        [[-0.0772]],\n               \n                        [[ 0.0838]],\n               \n                        [[-0.0194]]]], device='cuda:0')),\n              ('block_2.0.FFN.conv1.bias',\n               tensor([ 1.5433e-01,  6.6854e-03,  1.5921e-01,  4.1238e-02,  8.8184e-02,\n                       -7.4938e-02, -5.1085e-03, -1.0736e-03, -2.3546e-02,  7.5549e-02,\n                       -4.4148e-02, -5.0065e-02,  1.7904e-02, -2.3491e-02,  6.6230e-02,\n                       -8.7628e-02, -4.3293e-02,  8.6702e-02,  1.1596e-01, -1.5001e-02,\n                        8.2371e-02,  3.6277e-02, -2.6099e-02,  2.2346e-01, -1.8376e-01,\n                       -9.8641e-02, -5.3560e-03,  5.0535e-02, -5.2236e-02,  5.0580e-02,\n                        4.2105e-02,  1.1264e-01, -1.3472e-01, -4.8369e-02, -1.9517e-02,\n                        1.7891e-02,  7.8704e-02,  8.4524e-02, -2.1806e-01,  6.1125e-02,\n                       -2.1261e-01,  1.2117e-01, -6.6579e-02, -1.5593e-01,  2.0786e-02,\n                        5.4583e-02, -3.5179e-02,  6.8821e-02,  3.7738e-02,  1.2141e-01,\n                       -1.2956e-01,  1.5927e-01,  4.5665e-02, -7.7194e-02,  9.2312e-02,\n                        3.4786e-02,  2.0675e-02,  3.9766e-02, -4.1372e-02,  5.4501e-02,\n                        1.2924e-01, -2.5556e-02, -7.7753e-02,  1.2164e-01, -2.7602e-03,\n                        1.4954e-02, -5.5722e-02,  3.7413e-02, -7.7611e-02,  6.4566e-02,\n                       -3.6266e-02, -1.0892e-01, -1.6954e-02, -6.5447e-02,  1.2164e-01,\n                        3.3614e-02, -3.0072e-03,  1.0113e-01, -2.8473e-02,  9.8851e-02,\n                       -4.2839e-03, -4.7595e-03, -1.6416e-01, -8.0745e-02,  7.0520e-02,\n                       -6.2822e-02,  1.8418e-01, -9.9052e-02,  1.6969e-01, -8.9848e-02,\n                       -1.8251e-01, -1.0587e-01,  1.2753e-01,  6.4279e-02,  8.7293e-02,\n                        4.7469e-02,  6.3672e-02,  2.2058e-02,  6.1703e-02, -7.4849e-02,\n                       -1.7657e-02, -3.2843e-03, -4.0227e-03, -9.6950e-02, -2.3351e-01,\n                       -5.3043e-02, -1.0808e-03, -8.4233e-02,  6.7782e-02,  3.9045e-03,\n                        7.3870e-03,  1.6420e-01,  9.6097e-02,  7.2687e-02,  1.9877e-01,\n                       -4.8436e-02, -5.9567e-02, -9.5002e-03,  3.2788e-02,  1.1587e-01,\n                        3.7803e-02, -2.4138e-02,  1.5963e-01,  2.1725e-02,  6.0897e-02,\n                       -9.0800e-02,  2.2621e-01,  5.2548e-02, -5.2733e-02,  1.4412e-02,\n                        1.2184e-01,  1.8290e-01,  1.2602e-01, -1.2021e-02, -5.3992e-02,\n                       -1.2150e-01, -9.4783e-02, -9.9663e-02, -3.1072e-02, -1.2036e-01,\n                       -5.4176e-02,  5.0163e-02, -1.8327e-01,  8.9610e-02, -5.7862e-02,\n                       -3.1852e-02,  7.4192e-02,  2.2569e-01, -1.1980e-01,  2.5449e-02,\n                        6.3533e-02, -2.7256e-02, -9.9869e-02, -3.6859e-02,  1.0453e-01,\n                       -4.9269e-02,  8.0246e-02,  6.4044e-02,  1.5641e-01,  5.8763e-02,\n                        2.4803e-02,  1.0817e-01, -1.4375e-01,  1.9882e-02,  1.0435e-02,\n                        5.7134e-02, -2.4908e-01, -4.0782e-02,  5.5313e-02,  6.8069e-03,\n                        8.0369e-02, -1.7077e-01,  2.4982e-02, -1.5396e-01, -4.6961e-02,\n                       -1.2805e-01,  1.9481e-02, -7.1942e-02,  1.8615e-01,  7.5070e-02,\n                        2.7822e-02,  9.9653e-02,  1.5901e-01,  1.7534e-02,  1.4384e-01,\n                        6.3775e-02, -8.5969e-02, -7.6092e-02,  3.4200e-02, -1.1096e-01,\n                        1.3169e-02,  1.8287e-01,  1.9130e-01, -7.7464e-02,  5.8620e-03,\n                       -6.6096e-03, -1.1304e-02, -1.7127e-01,  4.2945e-03, -1.1855e-01,\n                        4.2181e-02, -1.7157e-01,  6.1715e-02, -9.5379e-02,  3.3545e-02,\n                        7.4264e-02,  1.7927e-02,  9.6986e-02, -1.4295e-01,  1.3407e-02,\n                        6.0923e-02,  2.5390e-02,  5.0222e-02,  1.6459e-01,  9.9192e-02,\n                       -1.5064e-01,  9.7769e-03,  3.1665e-02,  2.0533e-02,  1.0224e-02,\n                        1.2120e-01,  3.2653e-02,  9.0698e-02,  5.4759e-02,  1.8637e-01,\n                       -8.6674e-02, -2.6741e-02, -6.3776e-03,  6.6644e-03, -3.3079e-03,\n                        6.4742e-02,  4.8087e-02,  1.5006e-02, -6.0967e-02, -5.1694e-02,\n                       -1.6453e-02,  8.4861e-06, -8.0299e-02, -3.6109e-02,  9.2601e-03,\n                        6.3044e-02,  3.9146e-02,  8.8253e-02,  1.6672e-01,  5.5807e-02,\n                        4.2344e-02, -4.3444e-02,  7.5290e-02,  5.2304e-02,  1.1849e-02,\n                        1.0890e-01,  8.9182e-02,  7.0372e-02, -2.4206e-02,  7.4934e-02,\n                       -1.5644e-01, -1.0390e-01, -4.6203e-02, -1.3688e-01, -6.9963e-02,\n                       -2.8722e-01,  1.4350e-01,  1.5241e-01,  5.1310e-02, -1.2388e-01,\n                       -1.0626e-01, -1.9536e-02,  3.8063e-02, -6.0463e-02, -1.4440e-01,\n                        1.5487e-01, -4.4763e-02, -1.8102e-01, -1.4353e-01, -6.6937e-02,\n                       -1.1513e-01,  2.2056e-01, -4.0389e-02,  2.0316e-01, -1.3038e-02,\n                        7.2059e-02,  7.8459e-02,  2.3088e-03, -1.2290e-01, -6.8954e-02,\n                       -2.3029e-01, -1.7261e-02, -2.3969e-02, -2.0704e-01, -3.5749e-02,\n                       -1.2898e-01, -1.5472e-01, -1.0785e-01, -1.0075e-01,  1.2277e-02,\n                        7.6288e-02, -7.1553e-02,  2.0635e-02, -1.7935e-01,  1.5686e-01,\n                       -2.0377e-01, -2.2904e-01, -1.0641e-01, -7.1890e-02, -1.4315e-02,\n                        7.5650e-02, -2.8214e-02, -1.2299e-01,  2.4621e-02,  4.7192e-02,\n                        5.9783e-02, -1.3107e-01,  1.2961e-01,  8.5046e-02,  1.3503e-01,\n                        1.7303e-01, -4.6727e-02,  1.6855e-01,  6.3778e-02, -1.1340e-01,\n                       -2.5071e-02, -1.2850e-01,  8.6118e-02,  1.0716e-01, -1.5413e-02,\n                       -9.7265e-02, -3.1980e-02,  3.4459e-02, -7.0663e-02,  8.0608e-02,\n                        1.8003e-01, -7.0499e-02,  1.1199e-01, -2.6175e-01, -1.0304e-01,\n                       -1.8960e-01, -4.0764e-02, -1.0190e-01, -8.9858e-03,  1.5197e-01,\n                        1.3734e-01,  8.0910e-02, -4.0828e-02, -1.0505e-01, -9.0527e-02,\n                        3.8365e-03, -1.4826e-01,  1.2378e-01,  1.0622e-01,  9.5312e-02,\n                        9.6841e-02,  2.2646e-02,  3.2445e-02,  4.1791e-02, -1.0417e-01,\n                       -9.4115e-02,  3.5391e-03, -1.2536e-01, -7.7664e-02,  1.4133e-01,\n                       -2.2149e-02, -1.5075e-01,  2.6696e-02, -4.3205e-02, -1.5199e-01,\n                       -2.9306e-02, -1.2627e-01,  7.4327e-02, -1.0208e-01,  1.1259e-01,\n                        8.9537e-02, -2.8152e-02, -2.2602e-02, -2.8451e-02,  5.0147e-02,\n                       -6.3717e-02,  2.8523e-02, -1.7970e-01,  7.5560e-02, -8.1320e-02,\n                       -1.7692e-02, -4.9438e-02,  1.8260e-01, -2.4259e-02, -8.4918e-02,\n                       -1.0514e-01,  1.1564e-01, -6.0707e-02,  8.6204e-02,  8.0947e-02,\n                       -7.1484e-02, -1.2656e-01, -1.1553e-01, -1.2145e-01, -1.2988e-01,\n                        1.4061e-01,  7.3992e-02,  8.0912e-03, -2.0163e-01,  1.2134e-02,\n                       -5.5324e-02, -6.8987e-02,  5.3711e-02,  1.0180e-01, -3.2664e-02,\n                        5.6116e-03,  7.4955e-02,  6.5015e-02, -1.3034e-01, -5.6684e-02,\n                       -9.1340e-02,  2.6029e-01,  1.7734e-01,  8.9050e-02, -8.7720e-02,\n                        2.3579e-01,  1.2926e-01, -3.1075e-02,  2.5071e-03,  7.0203e-02,\n                        1.8010e-01,  7.5920e-02, -4.0313e-02,  2.3959e-01,  7.8040e-02,\n                        9.3904e-02, -5.3431e-02,  2.1264e-02,  1.2994e-01,  1.4624e-01,\n                        5.6386e-03,  7.0379e-02,  3.4295e-02,  7.4355e-02,  3.2992e-02,\n                        1.3182e-02, -7.5857e-02,  3.3330e-02,  6.0298e-02, -1.2849e-01,\n                       -9.7960e-02, -1.9787e-03, -1.4074e-01,  8.4542e-03,  2.3591e-02,\n                       -5.8825e-03, -2.6623e-01,  8.8819e-02, -1.5241e-02,  7.1007e-02,\n                        2.9027e-02,  4.0377e-03,  2.9457e-02,  4.3742e-03,  9.0124e-02,\n                       -3.5852e-02, -8.5374e-02,  2.0348e-01,  3.9733e-02, -3.0766e-02,\n                        4.9532e-02, -4.1991e-02, -2.6501e-02, -6.4859e-02, -5.8791e-03,\n                       -1.2103e-01,  7.3051e-02,  1.0541e-01, -1.9127e-01,  6.4433e-02,\n                        6.0646e-02,  2.9116e-01,  7.9514e-02,  6.7351e-02,  2.7236e-02,\n                        1.2700e-01, -5.9460e-02, -1.2995e-01,  3.0859e-02,  1.2086e-01,\n                        1.3194e-01,  9.2667e-02,  1.1269e-01,  9.2533e-03, -5.9692e-02,\n                       -1.4508e-01,  1.6928e-01,  1.0321e-01,  1.4314e-01, -2.6664e-02,\n                        4.0613e-02,  2.4203e-02, -2.8499e-02, -3.4099e-02,  5.3544e-02,\n                       -1.3354e-01,  8.8129e-02,  1.5351e-02, -1.2897e-02, -2.0485e-03,\n                        1.7578e-01,  9.3117e-03, -1.8182e-01, -2.1695e-01, -1.9179e-02,\n                       -1.5754e-01,  1.3348e-01, -3.5167e-01, -2.9302e-01, -1.5052e-01,\n                        1.0409e-01,  2.3302e-02], device='cuda:0')),\n              ('block_2.0.FFN.conv2.weight',\n               tensor([[[[-0.2420, -0.3795, -0.3490],\n                         [-0.1927, -0.2892, -0.0216],\n                         [ 0.0700,  0.1315,  0.0779]]],\n               \n               \n                       [[[ 0.1666,  0.1930,  0.0944],\n                         [ 0.1802, -0.2369,  0.0867],\n                         [-0.2070, -0.2469, -0.1453]]],\n               \n               \n                       [[[-0.1049,  0.0297, -0.2027],\n                         [-0.0106, -0.1868, -0.0824],\n                         [-0.0336, -0.2500,  0.1968]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.1407, -0.2153,  0.0419],\n                         [-0.0836, -0.0596,  0.0594],\n                         [-0.1365, -0.0489,  0.0804]]],\n               \n               \n                       [[[ 0.0409,  0.0623, -0.1192],\n                         [-0.2174, -0.0200,  0.0503],\n                         [-0.2164,  0.0415,  0.0861]]],\n               \n               \n                       [[[-0.2471,  0.1438, -0.1653],\n                         [-0.2752, -0.2697, -0.0469],\n                         [ 0.0909, -0.0698, -0.2716]]]], device='cuda:0')),\n              ('block_2.0.FFN.conv2.bias',\n               tensor([-5.6122e-03, -9.2110e-02,  1.2391e-01, -2.3689e-01,  1.8857e-01,\n                       -4.7162e-02, -2.6970e-01, -1.5767e-01, -2.7177e-01, -2.3484e-01,\n                       -1.7862e-01, -5.4291e-02, -1.1356e-01, -1.2480e-01, -2.1481e-01,\n                       -1.7450e-02, -2.2299e-01, -1.7411e-01, -4.7779e-02, -2.3545e-01,\n                       -1.6367e-01, -2.1159e-01, -6.2123e-02,  6.3033e-02,  2.0438e-02,\n                       -1.1628e-01, -2.1058e-01, -2.6493e-01, -1.7432e-01, -4.8427e-02,\n                       -4.6555e-02,  1.9981e-02,  4.5430e-02,  1.4194e-02, -1.7911e-01,\n                       -3.0379e-01,  5.0803e-02,  1.2483e-01,  9.7232e-02, -9.3166e-02,\n                        9.9543e-02, -1.0145e-01, -7.1551e-02,  9.1484e-02, -6.0253e-02,\n                       -2.4379e-01,  2.3280e-01, -9.3720e-02, -1.1355e-01, -1.4742e-01,\n                        3.4333e-02,  1.5924e-01, -1.5610e-01, -1.4743e-01,  1.8759e-01,\n                       -2.9489e-01, -1.0295e-01,  1.9509e-02, -1.9144e-01,  4.0918e-02,\n                        2.1014e-02,  1.8692e-02, -1.1302e-01,  5.8150e-02,  7.7920e-02,\n                       -2.7111e-01, -1.6151e-01, -3.4791e-01, -1.1878e-01, -2.7643e-01,\n                       -2.1444e-01,  2.0051e-01, -5.2859e-02,  3.9580e-02, -5.1454e-03,\n                       -2.8818e-01, -1.5809e-01, -4.0434e-02, -7.0166e-02, -1.5715e-01,\n                       -1.3008e-01, -2.0424e-01,  4.6874e-02, -5.8046e-02, -8.3022e-02,\n                       -9.9590e-02,  1.9413e-02,  8.3223e-02,  1.5631e-01, -8.9449e-03,\n                        9.5820e-02, -1.4529e-01,  2.6645e-02, -1.9605e-01, -2.9364e-01,\n                       -5.7633e-02, -1.8247e-01,  1.7985e-02, -3.4279e-02, -7.6703e-02,\n                       -1.4077e-01, -2.6443e-01, -2.5417e-01,  8.0700e-03,  1.2012e-01,\n                       -4.4310e-02, -1.7020e-01, -2.2757e-02, -4.6028e-02, -2.2997e-01,\n                       -2.7871e-01, -1.4329e-01,  9.4447e-02, -1.0354e-02,  1.2410e-01,\n                       -1.6862e-01, -2.3985e-01, -1.6968e-01, -4.8288e-02, -9.2559e-02,\n                       -9.6394e-02, -2.0910e-01,  1.0661e-01, -1.8548e-01, -3.9182e-02,\n                       -1.6022e-02,  1.0892e-01,  9.7205e-02, -2.0801e-01,  6.7986e-02,\n                       -2.6145e-02,  2.5221e-02,  4.2946e-03, -2.0444e-01, -1.4799e-01,\n                        5.5096e-02, -2.1932e-02,  2.1521e-01, -3.5202e-01, -5.6476e-02,\n                        1.3943e-01, -1.3714e-01,  1.6832e-01, -5.0076e-02,  4.1273e-02,\n                       -2.4986e-01,  2.6029e-03,  2.7063e-02, -1.1546e-01, -8.2473e-02,\n                        1.4115e-01, -2.2736e-01, -5.6963e-02, -3.3606e-01, -1.3190e-01,\n                       -1.9980e-01, -1.1410e-01,  4.5320e-02,  7.2006e-02,  5.0215e-02,\n                       -1.7638e-01, -1.0632e-01,  7.7186e-02, -8.6494e-02, -2.4367e-01,\n                        9.1104e-02,  2.8421e-02, -9.4431e-02,  1.4885e-01, -6.8846e-02,\n                       -4.7509e-03,  1.4107e-01, -9.1104e-02, -9.7191e-02,  3.4926e-02,\n                        1.1068e-01, -9.2559e-02, -8.4068e-02,  1.2403e-01,  1.0445e-02,\n                       -2.1640e-01,  5.2045e-02,  1.5866e-02, -1.0273e-01,  4.6141e-02,\n                       -2.7814e-01, -3.6989e-02, -1.3079e-01, -1.9441e-01, -2.0214e-02,\n                       -6.8349e-02, -4.6222e-03,  6.2566e-02, -2.9039e-01, -5.5622e-02,\n                       -1.4391e-01,  4.2281e-02,  4.3383e-02, -2.4659e-01, -2.1099e-01,\n                       -2.9030e-01,  9.7052e-02, -2.0805e-01, -3.9333e-01,  2.3380e-01,\n                       -2.5077e-01, -1.0488e-01, -6.5390e-02, -2.6283e-02, -1.0687e-01,\n                       -1.7399e-01, -1.1757e-01,  3.4233e-02, -1.2999e-01, -1.3991e-01,\n                        2.5960e-03, -1.8551e-01, -2.5642e-01, -1.4846e-01,  6.2453e-03,\n                        4.8166e-02, -1.2825e-01,  3.0237e-03,  4.2878e-02,  4.0302e-02,\n                       -8.4895e-02, -1.9738e-01, -2.2929e-01, -2.3400e-01, -1.7154e-01,\n                       -2.2299e-01, -2.5705e-01, -1.7889e-01, -2.4356e-01, -1.0792e-01,\n                       -7.6733e-02, -1.7420e-01, -4.1137e-02, -2.0709e-02, -1.7404e-01,\n                       -1.2534e-01, -8.6222e-02, -3.7361e-02,  2.9839e-02, -1.8232e-01,\n                       -2.9178e-01, -7.8160e-02,  1.6199e-01, -1.9897e-03, -9.7588e-02,\n                        2.0809e-01, -2.7695e-01, -3.7281e-02, -1.8816e-01,  1.3098e-01,\n                       -2.1098e-01,  9.0294e-02, -9.5830e-02, -1.0070e-01, -3.1600e-01,\n                        9.2283e-02,  2.2517e-01,  1.0963e-03, -9.0491e-02,  1.2436e-01,\n                        1.5220e-01, -2.0679e-01, -1.7444e-01, -5.8497e-02, -5.8470e-05,\n                        2.8014e-03, -1.4865e-01, -5.3618e-02,  1.4080e-01,  1.4455e-01,\n                       -1.2834e-01, -6.0644e-02, -2.9826e-01,  1.6150e-02, -1.2901e-01,\n                       -3.0923e-01,  1.9395e-01, -9.7891e-02, -1.2917e-01, -8.3314e-02,\n                        4.7356e-02, -3.3276e-01, -1.9417e-01,  1.4875e-01, -2.4523e-01,\n                        2.4820e-02,  2.5704e-02, -4.0847e-02,  1.2074e-01, -1.5057e-01,\n                        1.2826e-01, -4.5499e-02, -1.1631e-02,  6.2876e-02,  4.5413e-02,\n                       -9.0510e-03,  2.6781e-02,  1.1503e-01, -1.3233e-01, -3.1124e-01,\n                        2.9307e-02, -7.0748e-03, -7.9564e-02, -8.4191e-02,  6.5721e-02,\n                        5.1910e-02, -2.2714e-03,  6.8688e-02,  7.1277e-02,  7.0057e-02,\n                        3.1689e-02, -1.2236e-01,  3.8228e-02, -1.6734e-01,  3.3533e-02,\n                       -2.2650e-01,  1.9215e-01, -1.9672e-02, -6.3758e-02, -1.0810e-01,\n                       -1.1279e-01, -2.7206e-02, -6.7544e-02, -1.1265e-01,  1.4196e-02,\n                        7.5109e-02, -4.7204e-02,  1.6314e-01,  6.4188e-02,  8.5147e-02,\n                        8.5240e-02, -1.7409e-01,  6.5227e-02, -9.3746e-02,  6.2387e-02,\n                        9.1408e-02, -6.1497e-02, -2.7788e-01, -4.2073e-02, -2.8726e-02,\n                       -1.1532e-01,  1.3634e-01,  4.0565e-02,  5.9555e-02, -2.1176e-02,\n                        7.7630e-02,  1.1677e-02, -8.1004e-02, -2.8716e-03, -1.3689e-02,\n                       -6.6905e-02, -2.4261e-01, -1.7283e-01, -2.6681e-01,  5.0314e-02,\n                       -1.9929e-01,  8.4498e-02, -1.5456e-01, -1.7741e-01, -1.6992e-01,\n                       -2.8190e-01,  4.8501e-03, -2.6929e-01, -1.2637e-01,  2.3606e-02,\n                       -5.6516e-02, -1.3364e-01, -5.4987e-02, -1.5522e-01, -2.7607e-01,\n                       -5.9491e-02,  1.8795e-01,  1.5166e-01, -1.6315e-01, -1.7739e-02,\n                       -2.1768e-01, -2.0935e-01,  8.1031e-03, -2.0573e-01,  6.3605e-02,\n                        4.4528e-02,  1.7389e-01, -2.4166e-01, -4.4060e-02, -3.6582e-02,\n                       -2.9894e-02, -5.7381e-03, -4.3351e-02,  3.0384e-02,  7.6265e-02,\n                       -1.8249e-02, -2.4920e-02, -2.7761e-01,  1.3216e-01,  3.1562e-02,\n                       -2.1400e-01, -3.0407e-01, -2.1463e-01,  3.7831e-02, -3.2108e-01,\n                       -2.3767e-01, -2.0962e-01, -1.8101e-01,  6.7416e-02, -2.8287e-01,\n                       -1.6963e-01,  1.1819e-02,  1.7883e-01,  2.2813e-01, -1.8180e-01,\n                        2.0977e-02,  1.4772e-01,  8.4059e-02, -1.3616e-01, -2.0964e-01,\n                        4.7898e-02, -2.1201e-01,  2.2790e-02, -6.9609e-02, -1.8969e-01,\n                       -1.4404e-01, -2.1577e-01, -1.4100e-01,  3.7962e-02,  6.5077e-02,\n                       -1.6365e-01, -3.2655e-02, -1.3307e-01,  1.0412e-01, -1.4291e-01,\n                       -1.5183e-01,  1.7277e-01, -1.7320e-01, -1.7617e-01,  6.1614e-02,\n                       -1.7281e-01, -1.4228e-01, -1.0791e-01, -1.6000e-01, -9.8652e-02,\n                       -2.1527e-01,  7.0115e-02, -1.4887e-01, -8.3759e-02, -1.9099e-01,\n                       -1.8837e-01, -2.3431e-01, -1.9918e-01, -6.4772e-02,  6.4393e-02,\n                       -9.5041e-02,  3.5268e-02, -5.1330e-02, -2.3528e-01, -1.1619e-01,\n                       -2.4858e-01, -5.5108e-02, -1.2178e-01, -1.9675e-01, -3.1504e-01,\n                       -8.4689e-02, -4.3121e-02, -3.3363e-02, -5.9646e-03, -3.6917e-02,\n                       -3.3713e-01,  8.3149e-02, -1.7253e-01, -1.2305e-01, -1.8204e-01,\n                       -8.3467e-03, -2.8307e-01,  1.3049e-01, -1.9626e-01, -1.0022e-01,\n                        3.6732e-02, -4.3937e-02, -5.0077e-02, -2.7258e-01, -1.3619e-01,\n                        4.5886e-02, -2.3619e-01,  1.2699e-02, -6.6899e-02,  6.0250e-02,\n                       -1.5312e-01, -8.3181e-02,  3.0386e-02, -4.8976e-02, -2.7784e-01,\n                        1.5763e-01, -2.9130e-01, -2.2774e-01, -1.7995e-01, -2.0429e-01,\n                        1.4287e-02, -2.0586e-01,  6.4932e-02,  5.6011e-02, -1.6281e-01,\n                        1.1301e-01, -8.6018e-02, -3.0347e-01,  1.5406e-02, -1.0153e-01,\n                       -3.3168e-02,  1.7089e-03], device='cuda:0')),\n              ('block_2.0.FFN.conv3.weight',\n               tensor([[[[-0.1036]],\n               \n                        [[ 0.0696]],\n               \n                        [[ 0.1667]],\n               \n                        ...,\n               \n                        [[-0.0093]],\n               \n                        [[ 0.0587]],\n               \n                        [[-0.0740]]],\n               \n               \n                       [[[-0.0173]],\n               \n                        [[-0.0067]],\n               \n                        [[-0.0202]],\n               \n                        ...,\n               \n                        [[-0.0244]],\n               \n                        [[-0.0654]],\n               \n                        [[ 0.0014]]],\n               \n               \n                       [[[ 0.0428]],\n               \n                        [[ 0.0459]],\n               \n                        [[-0.0465]],\n               \n                        ...,\n               \n                        [[-0.0077]],\n               \n                        [[-0.0390]],\n               \n                        [[ 0.0881]]],\n               \n               \n                       ...,\n               \n               \n                       [[[ 0.0625]],\n               \n                        [[ 0.1097]],\n               \n                        [[ 0.0751]],\n               \n                        ...,\n               \n                        [[ 0.0113]],\n               \n                        [[-0.0293]],\n               \n                        [[ 0.0513]]],\n               \n               \n                       [[[-0.0450]],\n               \n                        [[ 0.0881]],\n               \n                        [[ 0.0476]],\n               \n                        ...,\n               \n                        [[-0.0066]],\n               \n                        [[-0.0570]],\n               \n                        [[-0.0456]]],\n               \n               \n                       [[[-0.0425]],\n               \n                        [[ 0.0087]],\n               \n                        [[-0.0566]],\n               \n                        ...,\n               \n                        [[ 0.0192]],\n               \n                        [[-0.0306]],\n               \n                        [[-0.0069]]]], device='cuda:0')),\n              ('block_2.0.FFN.conv3.bias',\n               tensor([-9.0498e-03, -1.3636e-02, -2.5295e-02, -2.8451e-02,  2.7303e-03,\n                        2.4046e-02, -1.9074e-02,  1.2091e-02,  3.4044e-02, -4.3100e-02,\n                        4.9419e-02, -6.7548e-03,  1.6553e-02, -1.2942e-02, -5.7629e-02,\n                        1.6582e-02,  2.1189e-02, -1.7087e-02, -1.5049e-02,  3.4736e-03,\n                        3.6865e-02, -8.1119e-03, -5.5919e-03,  4.8145e-02,  3.6952e-02,\n                       -6.8072e-02,  2.0027e-02,  4.4353e-03, -8.3566e-03,  5.1543e-03,\n                       -1.5616e-02, -1.6441e-02, -4.8513e-02,  1.2724e-02,  6.9365e-02,\n                        5.6472e-03, -8.5004e-02,  2.8203e-02,  2.6106e-02,  1.4386e-02,\n                       -1.2852e-02, -2.5345e-02,  3.6537e-02,  8.5636e-03, -2.8579e-03,\n                       -2.6625e-02,  5.7641e-02,  1.5724e-02, -2.2588e-02, -1.1072e-02,\n                       -1.9992e-02, -5.7044e-03, -2.2481e-03, -3.5643e-02,  1.1567e-05,\n                        3.3586e-02, -2.2854e-02, -5.3036e-02,  6.1003e-02, -3.6544e-03,\n                       -1.6555e-03,  3.1098e-02,  1.1905e-02, -3.9936e-02, -3.5114e-02,\n                       -4.5899e-02, -5.4283e-02, -5.0647e-02,  3.4840e-02, -3.3695e-02,\n                        3.2517e-02,  1.1377e-02,  2.6182e-02,  1.9974e-02,  1.6515e-02,\n                        2.0281e-02,  3.4225e-02, -2.9336e-02,  3.4836e-02, -3.8488e-02,\n                        3.9903e-02, -3.4081e-02, -5.2754e-02, -3.1455e-02,  6.3385e-03,\n                       -5.1248e-03, -4.9382e-03, -8.1974e-03,  1.8088e-02,  2.4436e-02,\n                       -1.0915e-02,  4.3217e-02, -4.6793e-03,  5.4481e-02,  1.6467e-02,\n                        4.5036e-03, -2.2358e-02,  2.6957e-02,  2.6147e-02,  1.1202e-03,\n                       -2.3105e-03,  2.0185e-02, -4.2171e-02,  1.1080e-02, -6.3954e-03,\n                       -1.9302e-02,  1.2951e-02, -2.3814e-02,  4.4702e-02, -2.9590e-02,\n                       -1.4994e-03,  8.0126e-03, -2.0541e-02,  2.2051e-02, -2.1280e-02,\n                       -2.2500e-02, -4.7881e-02, -3.5988e-02, -4.4059e-02, -3.5849e-03,\n                        4.4907e-02, -4.0752e-04, -2.1244e-02,  2.4623e-02, -4.2171e-02,\n                        1.2312e-02,  1.0978e-03,  7.3692e-03], device='cuda:0'))]),\n 'optimizer_state_dict': {'state': {0: {'step': tensor(10550.),\n    'exp_avg': tensor([[ 1.8425e-04, -1.5420e-06,  4.9423e-05,  ..., -9.8963e-05,\n              5.6587e-05,  8.1240e-05],\n            [ 4.0447e-04, -1.7500e-04, -9.5977e-04,  ..., -1.1893e-03,\n             -1.0350e-03,  1.1753e-03],\n            [-9.6686e-05, -7.5444e-05, -5.6779e-05,  ...,  1.2620e-04,\n             -1.3456e-05, -9.5833e-05],\n            ...,\n            [ 1.3345e-04,  2.4935e-05,  6.3635e-05,  ..., -2.7571e-05,\n             -7.9830e-05, -1.0536e-04],\n            [-4.8961e-04,  1.6781e-04,  2.7406e-04,  ..., -3.6480e-04,\n              3.4048e-04, -6.0651e-04],\n            [-9.3877e-04, -1.5976e-04, -4.1271e-04,  ...,  4.1114e-04,\n              3.5358e-04,  9.7858e-04]], device='cuda:0'),\n    'exp_avg_sq': tensor([[1.5579e-06, 6.3023e-06, 4.1279e-06,  ..., 3.7175e-06, 6.7267e-06,\n             1.1012e-05],\n            [1.5421e-06, 2.2444e-06, 1.8845e-06,  ..., 4.9997e-06, 3.0563e-06,\n             4.6160e-06],\n            [7.7462e-06, 1.3217e-05, 4.1917e-06,  ..., 1.1073e-05, 6.1361e-06,\n             1.5402e-05],\n            ...,\n            [1.9278e-06, 3.5839e-06, 2.2834e-06,  ..., 7.6481e-06, 2.2911e-06,\n             1.0021e-05],\n            [1.8240e-05, 1.1855e-05, 1.1555e-05,  ..., 1.0748e-05, 5.6017e-06,\n             3.6767e-05],\n            [9.0351e-06, 4.5854e-06, 5.3508e-06,  ..., 1.1568e-05, 5.5548e-06,\n             1.0766e-05]], device='cuda:0')},\n   1: {'step': tensor(10550.),\n    'exp_avg': tensor([-2.5412e-05, -2.7156e-04,  1.4271e-04, -1.6648e-04, -4.9163e-04,\n             1.6613e-04, -4.7801e-04, -1.5198e-05,  5.5197e-04,  5.8749e-04],\n           device='cuda:0'),\n    'exp_avg_sq': tensor([2.0357e-06, 3.0939e-06, 3.4814e-06, 3.4505e-06, 4.3449e-06, 2.8856e-06,\n            2.6381e-06, 3.8222e-06, 5.1401e-06, 5.4576e-06], device='cuda:0')},\n   2: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[ 1.3077e-04, -2.8583e-06, -1.6263e-05],\n              [-1.5468e-04, -2.0447e-04,  1.3542e-05],\n              [-6.2904e-04, -1.8056e-04,  5.2751e-05]]],\n    \n    \n            [[[-1.2264e-03, -3.4803e-04,  1.7042e-04],\n              [-7.6377e-04, -2.3152e-04, -5.6041e-04],\n              [ 5.1621e-04,  1.0059e-04, -1.0417e-03]]],\n    \n    \n            [[[-8.1200e-05, -5.6977e-04,  2.5906e-04],\n              [-3.0675e-04, -1.3882e-03, -8.4347e-04],\n              [-1.6530e-04, -1.0760e-03, -7.1563e-04]]],\n    \n    \n            [[[-7.1755e-04, -2.0877e-03, -4.5816e-03],\n              [-8.3164e-05, -2.0926e-03, -2.9805e-03],\n              [ 1.2311e-03,  1.7751e-04, -1.4665e-03]]],\n    \n    \n            [[[ 1.3034e-04,  2.3925e-04,  8.2082e-05],\n              [ 2.2012e-04,  2.8451e-04,  9.0491e-05],\n              [ 1.9789e-04, -1.1505e-04, -4.2003e-05]]],\n    \n    \n            [[[-4.0071e-04, -8.0643e-04, -1.0152e-03],\n              [-4.5522e-04, -4.0111e-04, -2.4018e-04],\n              [-1.0939e-03, -1.6137e-04,  2.8446e-04]]],\n    \n    \n            [[[-3.8919e-03, -3.6326e-03, -3.2516e-03],\n              [-3.5347e-03, -1.8801e-03, -3.2874e-03],\n              [-3.2332e-03, -2.7596e-03, -2.8313e-03]]],\n    \n    \n            [[[ 8.1737e-04,  2.2798e-04,  9.5100e-04],\n              [ 1.2486e-04, -1.0277e-03,  8.1199e-04],\n              [ 1.2394e-03,  1.0299e-03,  1.6872e-03]]],\n    \n    \n            [[[ 1.8889e-04,  2.4442e-04,  4.4446e-04],\n              [-8.1368e-05,  1.6027e-04,  3.7161e-04],\n              [-4.6453e-04, -8.0906e-05,  2.3846e-04]]],\n    \n    \n            [[[-9.6748e-04, -1.4900e-03, -1.3870e-03],\n              [-1.0314e-04, -4.9848e-04, -9.4433e-04],\n              [ 5.8308e-05, -5.8292e-04, -1.0194e-03]]],\n    \n    \n            [[[ 3.3467e-05, -4.1559e-04, -8.5396e-04],\n              [ 4.4523e-04, -3.3718e-05, -1.7368e-04],\n              [ 6.6856e-04,  4.6247e-04, -5.8335e-05]]],\n    \n    \n            [[[ 2.6513e-04,  4.6581e-04,  5.3220e-04],\n              [-1.1988e-04,  4.0140e-04,  1.3714e-04],\n              [ 1.1745e-04,  8.6829e-05, -1.1598e-04]]],\n    \n    \n            [[[-1.0631e-03, -2.2449e-03, -2.4565e-03],\n              [-1.1284e-03, -1.9582e-03, -1.5632e-03],\n              [-2.8410e-03, -2.2278e-03, -1.7366e-03]]],\n    \n    \n            [[[ 1.4715e-04,  6.2895e-04,  8.6668e-04],\n              [-3.9677e-04,  2.2531e-04,  7.6867e-04],\n              [-7.9624e-04,  1.7971e-04,  5.6405e-04]]],\n    \n    \n            [[[ 4.7865e-05,  1.8381e-03,  1.5151e-03],\n              [-9.2247e-04,  1.0500e-03,  3.8687e-04],\n              [-2.7550e-04,  3.1764e-04, -1.8954e-04]]],\n    \n    \n            [[[-1.3677e-03, -1.3810e-03, -8.3823e-04],\n              [-3.3044e-04, -1.1043e-03, -6.7847e-04],\n              [ 7.0906e-04, -6.6218e-04, -7.8549e-04]]],\n    \n    \n            [[[-2.4447e-04,  3.6478e-04,  1.5916e-04],\n              [ 1.1667e-03,  7.9825e-04,  1.2814e-03],\n              [ 1.6894e-03,  1.2456e-03,  9.4168e-04]]],\n    \n    \n            [[[-1.9363e-04,  1.3223e-04,  4.1713e-04],\n              [-5.2064e-05,  1.0458e-04,  6.3358e-06],\n              [ 3.3326e-04,  1.0399e-05, -7.9117e-05]]],\n    \n    \n            [[[ 1.2023e-03,  1.4475e-03,  1.0708e-03],\n              [ 1.2932e-03,  1.6451e-03,  1.7346e-03],\n              [ 5.0882e-04,  1.1554e-03,  1.0434e-03]]],\n    \n    \n            [[[ 2.2516e-04, -5.5171e-05,  2.0020e-04],\n              [-1.2182e-03, -1.5560e-04, -4.1211e-04],\n              [-1.9440e-03, -3.3736e-04,  9.2082e-05]]],\n    \n    \n            [[[ 5.7236e-04,  2.5697e-04,  6.1194e-05],\n              [-2.7473e-05,  3.3357e-04,  2.8707e-04],\n              [-9.9637e-04,  2.1392e-04,  7.6147e-04]]],\n    \n    \n            [[[-1.9161e-04, -6.2966e-05, -1.4368e-04],\n              [-9.4678e-05,  3.8019e-05, -7.9083e-05],\n              [ 1.5617e-04,  9.4564e-05,  2.2079e-06]]],\n    \n    \n            [[[ 5.8762e-04,  6.6182e-04,  4.4489e-04],\n              [ 6.1097e-04,  9.6035e-04,  4.9456e-04],\n              [ 5.2077e-04,  5.1309e-04,  3.0134e-04]]],\n    \n    \n            [[[ 1.5410e-03,  2.1843e-03,  1.6827e-03],\n              [ 1.2273e-03,  1.6486e-03,  1.1304e-03],\n              [ 2.0837e-03,  1.6141e-03,  7.7965e-04]]],\n    \n    \n            [[[ 9.4989e-04,  2.8615e-04,  5.6355e-04],\n              [ 2.0994e-04,  4.3551e-04,  6.6024e-04],\n              [ 7.0401e-05,  6.8289e-04,  1.1005e-03]]],\n    \n    \n            [[[-5.1344e-04, -5.2737e-04, -5.6165e-04],\n              [ 2.3746e-04, -2.2689e-04, -2.4943e-04],\n              [ 1.1315e-03,  1.7668e-04, -4.7871e-04]]],\n    \n    \n            [[[-6.6188e-04,  1.7227e-04, -3.2836e-04],\n              [ 3.6477e-05,  6.5013e-04,  5.3710e-04],\n              [-8.2492e-05, -1.4733e-04, -8.6107e-06]]],\n    \n    \n            [[[-1.6389e-03, -1.5468e-03, -1.2465e-03],\n              [-6.4784e-04, -6.4620e-04, -3.4415e-05],\n              [-1.0319e-03, -5.9123e-04, -2.7047e-05]]],\n    \n    \n            [[[ 7.1555e-07, -6.4472e-04,  4.0771e-04],\n              [-1.2329e-03, -1.4710e-03, -5.7865e-04],\n              [-1.8157e-03, -1.1809e-03, -1.3866e-05]]],\n    \n    \n            [[[-2.8170e-04,  1.2215e-04,  2.6249e-04],\n              [ 2.8580e-04, -4.1238e-04,  1.0840e-03],\n              [ 1.8230e-03,  9.9646e-04,  1.1737e-03]]],\n    \n    \n            [[[ 1.0098e-03,  1.3078e-03,  1.9132e-04],\n              [ 6.7008e-04,  1.2851e-03,  1.5867e-03],\n              [ 1.1135e-03,  1.2063e-03,  1.4767e-03]]],\n    \n    \n            [[[ 3.3167e-04,  2.6097e-05,  8.4243e-05],\n              [-4.0561e-04, -2.0645e-04,  3.2339e-04],\n              [-7.8262e-04, -1.5495e-04,  5.1977e-04]]],\n    \n    \n            [[[-5.2374e-04,  3.9247e-04, -6.3943e-06],\n              [-1.0131e-04,  5.1945e-04,  2.2732e-04],\n              [ 7.9207e-04,  7.3812e-04,  8.0150e-05]]],\n    \n    \n            [[[-1.4299e-03, -1.2465e-03, -1.0790e-03],\n              [ 3.0835e-04, -3.6908e-04, -1.8871e-04],\n              [ 9.7674e-04,  3.6671e-04, -1.7722e-04]]],\n    \n    \n            [[[ 3.0890e-04,  3.7484e-04,  5.5876e-04],\n              [ 5.1005e-05,  5.3668e-04,  8.5821e-04],\n              [-1.8112e-04,  5.5221e-04,  8.4934e-04]]],\n    \n    \n            [[[ 8.9705e-04,  2.6514e-04, -1.9755e-04],\n              [ 6.5804e-04,  3.3570e-04,  1.7511e-04],\n              [-2.0587e-04,  3.2285e-04,  3.9626e-04]]],\n    \n    \n            [[[-2.9053e-03, -4.0710e-03, -3.9639e-03],\n              [-2.0464e-03, -4.5397e-03, -3.4038e-03],\n              [-3.2307e-03, -3.9179e-03, -2.6115e-03]]],\n    \n    \n            [[[ 1.6929e-03,  1.2274e-03,  1.3130e-03],\n              [-1.4227e-04,  1.0364e-03,  2.1019e-03],\n              [-6.1233e-04,  1.2430e-03,  2.2089e-03]]],\n    \n    \n            [[[-1.0905e-03, -1.0384e-03, -7.8155e-04],\n              [-6.6953e-04, -8.4064e-04, -2.0281e-04],\n              [-7.1031e-04, -5.4678e-04, -4.7482e-05]]],\n    \n    \n            [[[ 2.1564e-04,  5.5079e-04,  7.3738e-04],\n              [-3.6844e-05,  1.9556e-04,  3.4202e-04],\n              [ 1.1714e-04,  1.3080e-05,  7.1058e-05]]],\n    \n    \n            [[[ 9.9003e-04,  1.1030e-03,  5.0552e-05],\n              [ 5.2221e-04,  9.9369e-04,  9.4148e-04],\n              [ 1.5537e-06,  5.3884e-04,  4.9185e-04]]],\n    \n    \n            [[[ 1.8479e-04,  3.0807e-04,  4.5184e-04],\n              [-3.8530e-04,  5.8858e-05,  3.3186e-04],\n              [-8.5623e-04, -1.0566e-04,  2.7023e-04]]],\n    \n    \n            [[[ 4.5042e-06, -4.8288e-05,  9.3847e-05],\n              [ 3.3953e-05, -2.9409e-05,  2.0334e-05],\n              [ 1.0227e-05, -2.6663e-05,  3.4356e-05]]],\n    \n    \n            [[[-2.9593e-04,  4.2959e-05,  5.4485e-04],\n              [-5.0858e-04, -4.1773e-04,  2.6899e-04],\n              [ 1.9531e-04,  2.1433e-05,  2.3551e-04]]],\n    \n    \n            [[[ 1.2990e-03,  2.5270e-03,  2.3025e-03],\n              [ 1.9038e-03,  3.4090e-03,  1.8386e-03],\n              [ 2.3228e-03,  2.2308e-03,  7.3293e-04]]],\n    \n    \n            [[[-5.0284e-04, -1.1075e-03, -1.3782e-03],\n              [-1.1898e-04, -8.3437e-04, -7.9883e-04],\n              [ 8.0009e-05, -9.0212e-05, -9.0287e-05]]],\n    \n    \n            [[[ 2.8683e-03,  4.2418e-03,  4.0202e-03],\n              [ 1.3143e-03,  3.3473e-03,  3.2689e-03],\n              [ 7.6475e-04,  2.7060e-03,  2.8381e-03]]],\n    \n    \n            [[[-1.4973e-04, -1.0204e-03, -1.4867e-03],\n              [ 4.6465e-04, -1.0894e-04, -1.2779e-04],\n              [-2.6885e-04,  1.3857e-04,  6.7470e-05]]],\n    \n    \n            [[[ 1.2715e-04, -1.9495e-05,  1.0031e-04],\n              [ 1.4886e-04, -1.7867e-04,  1.0651e-04],\n              [-1.4604e-04, -6.0958e-06,  1.4248e-04]]],\n    \n    \n            [[[ 4.2245e-04,  1.3036e-04, -5.7188e-06],\n              [ 1.3084e-04,  1.1570e-04,  8.8222e-05],\n              [-3.5061e-04,  7.6428e-06,  1.7091e-04]]],\n    \n    \n            [[[ 1.5240e-04,  3.8050e-04,  4.8791e-04],\n              [-2.4160e-04,  1.0617e-03,  2.9331e-04],\n              [ 1.8024e-04,  8.5829e-04,  3.8296e-04]]],\n    \n    \n            [[[-4.5914e-04, -8.4834e-04, -6.4201e-04],\n              [-3.3134e-04, -8.6293e-04, -3.2523e-04],\n              [ 1.5957e-04, -1.4638e-04,  1.8205e-04]]],\n    \n    \n            [[[-4.4924e-04, -7.7768e-04, -6.5415e-04],\n              [-4.6677e-04, -9.3013e-04, -9.8269e-04],\n              [ 5.7787e-04, -5.4051e-04, -7.4923e-04]]],\n    \n    \n            [[[-9.8752e-05, -4.4202e-04, -2.7211e-04],\n              [-2.9791e-04, -1.0535e-04,  5.6073e-05],\n              [-7.1334e-04, -2.2877e-04,  9.8187e-05]]],\n    \n    \n            [[[-1.3971e-03, -6.8428e-04, -1.1672e-03],\n              [-2.9182e-05,  2.9510e-05, -5.4696e-04],\n              [ 3.4585e-04, -1.3095e-04, -8.2233e-04]]],\n    \n    \n            [[[-1.2759e-03, -1.4739e-03, -1.6642e-03],\n              [ 8.0679e-04, -5.5880e-04, -1.4299e-03],\n              [ 1.8045e-03, -2.4131e-04, -1.3242e-03]]],\n    \n    \n            [[[ 3.6661e-04, -1.6072e-04, -5.8701e-05],\n              [ 3.2772e-04, -2.7790e-04, -1.4280e-04],\n              [-5.7138e-05, -2.6359e-04, -1.9150e-04]]],\n    \n    \n            [[[-5.0298e-04, -2.4116e-03, -2.1894e-03],\n              [-2.1769e-03, -2.2049e-03, -3.2507e-03],\n              [-2.0469e-03, -1.4546e-03, -1.9388e-03]]],\n    \n    \n            [[[ 2.4662e-04, -1.1509e-04,  1.1437e-04],\n              [-2.4318e-04, -4.4486e-05,  3.1697e-04],\n              [-1.0243e-03,  1.3316e-06,  5.6032e-04]]],\n    \n    \n            [[[ 7.7380e-04,  1.4239e-03,  1.3436e-03],\n              [ 2.3165e-04,  4.2760e-04,  7.4084e-05],\n              [-1.6208e-04,  5.5105e-05, -2.1461e-04]]],\n    \n    \n            [[[-1.0002e-03, -9.1867e-04,  6.6888e-05],\n              [-3.9167e-04, -8.3147e-04, -3.3474e-04],\n              [-1.2304e-03, -1.1832e-03, -4.4093e-04]]],\n    \n    \n            [[[-4.4301e-04, -1.2136e-04, -1.7908e-03],\n              [ 1.2679e-03, -1.7376e-04, -8.5474e-04],\n              [ 1.6867e-03,  1.8794e-05, -1.1786e-03]]],\n    \n    \n            [[[-1.1366e-03, -1.9671e-03, -1.7886e-03],\n              [-9.8003e-04, -1.8431e-03, -2.1972e-03],\n              [-4.7385e-04, -1.7918e-03, -1.7888e-03]]],\n    \n    \n            [[[-7.4791e-04, -5.6816e-04, -4.9078e-04],\n              [-5.6762e-05, -2.6184e-04, -4.4322e-04],\n              [ 6.7768e-04,  1.3136e-04, -4.2195e-04]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[1.2080e-06, 5.3459e-07, 6.5999e-07],\n              [1.3666e-06, 5.1523e-07, 2.6403e-07],\n              [1.5029e-06, 7.4879e-07, 6.9272e-07]]],\n    \n    \n            [[[1.3010e-05, 1.2963e-05, 1.5418e-05],\n              [1.0114e-05, 8.1749e-06, 1.4497e-05],\n              [1.6566e-05, 1.5308e-05, 1.8683e-05]]],\n    \n    \n            [[[2.7681e-05, 2.9315e-05, 2.6737e-05],\n              [2.1399e-05, 2.2701e-05, 2.2606e-05],\n              [1.4782e-05, 2.3329e-05, 2.5270e-05]]],\n    \n    \n            [[[6.1584e-05, 4.8639e-05, 7.2740e-05],\n              [5.8397e-05, 4.9177e-05, 6.1374e-05],\n              [5.6269e-05, 4.4506e-05, 5.9349e-05]]],\n    \n    \n            [[[1.2949e-05, 1.0844e-05, 9.1288e-06],\n              [9.0581e-06, 8.5544e-06, 8.7653e-06],\n              [6.2713e-06, 6.8806e-06, 1.0330e-05]]],\n    \n    \n            [[[1.2786e-05, 1.3913e-05, 1.6659e-05],\n              [8.6291e-06, 1.0161e-05, 1.5792e-05],\n              [5.5149e-06, 5.2148e-06, 1.1375e-05]]],\n    \n    \n            [[[8.6922e-05, 7.0825e-05, 8.1788e-05],\n              [1.0404e-04, 8.8733e-05, 8.9916e-05],\n              [9.5689e-05, 9.2073e-05, 1.0174e-04]]],\n    \n    \n            [[[2.8459e-05, 2.3250e-05, 2.7034e-05],\n              [2.8546e-05, 2.7071e-05, 2.3825e-05],\n              [3.3199e-05, 3.1849e-05, 2.7895e-05]]],\n    \n    \n            [[[2.3697e-06, 1.6160e-06, 1.2067e-06],\n              [9.5731e-07, 4.1092e-07, 4.4766e-07],\n              [6.8053e-07, 3.6064e-07, 7.9833e-07]]],\n    \n    \n            [[[1.6914e-05, 2.0316e-05, 2.3765e-05],\n              [1.8325e-05, 1.9621e-05, 2.3410e-05],\n              [2.0319e-05, 2.4194e-05, 2.5510e-05]]],\n    \n    \n            [[[1.2589e-05, 1.1282e-05, 9.4053e-06],\n              [1.0747e-05, 8.1859e-06, 6.6689e-06],\n              [8.9517e-06, 5.2448e-06, 4.8067e-06]]],\n    \n    \n            [[[9.0816e-06, 1.0823e-05, 1.3338e-05],\n              [1.0653e-05, 8.7763e-06, 6.7010e-06],\n              [8.9043e-06, 5.9047e-06, 2.7624e-06]]],\n    \n    \n            [[[3.6481e-05, 4.3286e-05, 6.1328e-05],\n              [4.7011e-05, 4.6422e-05, 5.3766e-05],\n              [4.7560e-05, 3.8403e-05, 5.6853e-05]]],\n    \n    \n            [[[9.1667e-07, 1.7535e-06, 2.5902e-06],\n              [1.7843e-06, 2.3819e-06, 2.9545e-06],\n              [3.0319e-06, 3.3356e-06, 4.0393e-06]]],\n    \n    \n            [[[3.4319e-05, 3.8773e-05, 4.1374e-05],\n              [3.3402e-05, 3.2911e-05, 3.3022e-05],\n              [2.8107e-05, 3.5777e-05, 4.5298e-05]]],\n    \n    \n            [[[6.7977e-06, 5.0254e-06, 8.6539e-06],\n              [4.0326e-06, 4.1818e-06, 9.6034e-06],\n              [6.4339e-06, 7.6006e-06, 1.1191e-05]]],\n    \n    \n            [[[2.8188e-05, 2.6029e-05, 1.8881e-05],\n              [2.2709e-05, 2.1392e-05, 1.8495e-05],\n              [2.6754e-05, 2.2369e-05, 2.4636e-05]]],\n    \n    \n            [[[1.0316e-06, 8.4429e-07, 3.2454e-06],\n              [8.4361e-07, 7.8141e-07, 2.6899e-06],\n              [1.7900e-06, 2.0529e-06, 4.1328e-06]]],\n    \n    \n            [[[1.2609e-05, 1.4293e-05, 1.2669e-05],\n              [7.9914e-06, 1.2054e-05, 9.7951e-06],\n              [9.8389e-06, 1.1779e-05, 9.2513e-06]]],\n    \n    \n            [[[1.4545e-05, 9.9785e-06, 6.3840e-06],\n              [1.1733e-05, 6.0015e-06, 5.2170e-06],\n              [1.1570e-05, 6.2288e-06, 7.9978e-06]]],\n    \n    \n            [[[7.1060e-06, 4.2733e-06, 4.0899e-06],\n              [4.8685e-06, 1.4760e-06, 2.3762e-06],\n              [4.0286e-06, 8.6976e-07, 3.7170e-06]]],\n    \n    \n            [[[6.0971e-07, 5.6847e-07, 8.7977e-07],\n              [7.1016e-08, 3.0584e-07, 7.3163e-07],\n              [3.4829e-07, 5.8450e-07, 1.0284e-06]]],\n    \n    \n            [[[1.0683e-05, 1.0522e-05, 8.8896e-06],\n              [1.2915e-05, 1.1015e-05, 1.0168e-05],\n              [1.0778e-05, 1.0831e-05, 1.2376e-05]]],\n    \n    \n            [[[1.2572e-05, 1.0889e-05, 1.1757e-05],\n              [9.2397e-06, 6.5737e-06, 6.8107e-06],\n              [9.1920e-06, 5.4769e-06, 4.8365e-06]]],\n    \n    \n            [[[9.5732e-06, 6.5261e-06, 7.5346e-06],\n              [1.1659e-05, 9.7520e-06, 1.1575e-05],\n              [1.1628e-05, 8.9946e-06, 1.1087e-05]]],\n    \n    \n            [[[1.0106e-05, 8.2934e-06, 6.2417e-06],\n              [8.6664e-06, 6.2698e-06, 3.7552e-06],\n              [9.1231e-06, 5.0985e-06, 3.6804e-06]]],\n    \n    \n            [[[1.5757e-05, 1.5845e-05, 1.2159e-05],\n              [1.3151e-05, 1.3718e-05, 1.1643e-05],\n              [1.2498e-05, 1.5519e-05, 1.4694e-05]]],\n    \n    \n            [[[1.6418e-05, 1.8806e-05, 1.7829e-05],\n              [9.5176e-06, 1.3605e-05, 1.7061e-05],\n              [6.0863e-06, 1.0400e-05, 1.4601e-05]]],\n    \n    \n            [[[3.0982e-05, 2.8140e-05, 1.9066e-05],\n              [2.7880e-05, 2.0468e-05, 1.9390e-05],\n              [3.2955e-05, 2.7729e-05, 3.4288e-05]]],\n    \n    \n            [[[3.7793e-05, 4.3907e-05, 5.0370e-05],\n              [3.9902e-05, 3.4955e-05, 3.8925e-05],\n              [4.3176e-05, 3.1708e-05, 3.7415e-05]]],\n    \n    \n            [[[4.3827e-05, 4.7282e-05, 4.5318e-05],\n              [3.4746e-05, 4.1401e-05, 3.9611e-05],\n              [4.5970e-05, 4.9041e-05, 5.2006e-05]]],\n    \n    \n            [[[2.2109e-06, 7.7661e-07, 1.3245e-06],\n              [2.4208e-06, 1.8521e-06, 2.5101e-06],\n              [3.1686e-06, 3.2246e-06, 4.1539e-06]]],\n    \n    \n            [[[9.9414e-06, 9.4353e-06, 6.9086e-06],\n              [8.1053e-06, 5.4544e-06, 3.4009e-06],\n              [8.9551e-06, 7.9988e-06, 6.1439e-06]]],\n    \n    \n            [[[2.8271e-05, 3.1909e-05, 2.5966e-05],\n              [2.6460e-05, 2.6100e-05, 2.6701e-05],\n              [2.9834e-05, 2.8891e-05, 2.9629e-05]]],\n    \n    \n            [[[9.7879e-06, 8.0796e-06, 8.9670e-06],\n              [9.0488e-06, 7.2041e-06, 7.4235e-06],\n              [7.7642e-06, 6.2100e-06, 8.5548e-06]]],\n    \n    \n            [[[7.0606e-06, 6.9291e-06, 9.5921e-06],\n              [7.7522e-06, 5.2560e-06, 4.8564e-06],\n              [7.3031e-06, 8.3845e-06, 1.0592e-05]]],\n    \n    \n            [[[6.8649e-05, 6.9263e-05, 7.8430e-05],\n              [5.5978e-05, 5.0176e-05, 6.4109e-05],\n              [6.4623e-05, 5.9115e-05, 8.3867e-05]]],\n    \n    \n            [[[4.8214e-05, 3.1335e-05, 4.3925e-05],\n              [4.1473e-05, 3.9161e-05, 3.9750e-05],\n              [4.2594e-05, 3.6880e-05, 4.6121e-05]]],\n    \n    \n            [[[1.1565e-05, 1.0521e-05, 1.2794e-05],\n              [1.0755e-05, 8.6103e-06, 9.2727e-06],\n              [1.0282e-05, 5.5940e-06, 7.9144e-06]]],\n    \n    \n            [[[3.3840e-06, 2.3403e-06, 3.2572e-06],\n              [2.5260e-06, 9.0709e-07, 1.0051e-06],\n              [3.0671e-06, 1.0339e-06, 8.3978e-07]]],\n    \n    \n            [[[3.4809e-05, 3.6805e-05, 3.6922e-05],\n              [2.4042e-05, 3.0723e-05, 3.5568e-05],\n              [2.3902e-05, 3.4658e-05, 3.9294e-05]]],\n    \n    \n            [[[3.4454e-07, 2.0717e-07, 5.4907e-07],\n              [3.8572e-07, 3.4176e-07, 7.4494e-07],\n              [7.9668e-07, 7.2205e-07, 1.1745e-06]]],\n    \n    \n            [[[1.0214e-06, 4.1303e-07, 4.5484e-07],\n              [1.0011e-06, 2.7895e-07, 2.5879e-07],\n              [1.0656e-06, 6.8330e-07, 6.8112e-07]]],\n    \n    \n            [[[8.9017e-06, 6.1159e-06, 5.4314e-06],\n              [6.2874e-06, 2.7259e-06, 4.2602e-06],\n              [3.9483e-06, 2.6128e-06, 5.5523e-06]]],\n    \n    \n            [[[6.3715e-05, 5.5385e-05, 4.8167e-05],\n              [5.3267e-05, 4.7990e-05, 4.8345e-05],\n              [3.0931e-05, 4.3693e-05, 5.2978e-05]]],\n    \n    \n            [[[6.7812e-06, 6.7826e-06, 6.9202e-06],\n              [5.6085e-06, 5.0184e-06, 3.9010e-06],\n              [7.1213e-06, 3.6110e-06, 2.2896e-06]]],\n    \n    \n            [[[3.6147e-05, 3.3592e-05, 3.1802e-05],\n              [3.3257e-05, 3.4921e-05, 3.5717e-05],\n              [2.9226e-05, 2.2676e-05, 3.1393e-05]]],\n    \n    \n            [[[3.0312e-05, 2.1416e-05, 2.4081e-05],\n              [2.4810e-05, 1.4734e-05, 1.8929e-05],\n              [2.4223e-05, 1.4975e-05, 2.2356e-05]]],\n    \n    \n            [[[8.9282e-07, 7.3421e-07, 1.1285e-06],\n              [4.7173e-07, 2.3815e-07, 2.8210e-07],\n              [5.9858e-07, 4.1949e-07, 4.7013e-07]]],\n    \n    \n            [[[7.1787e-07, 5.0108e-07, 7.9890e-07],\n              [2.2613e-07, 2.5822e-07, 5.6668e-07],\n              [2.5681e-07, 3.0268e-07, 7.6528e-07]]],\n    \n    \n            [[[9.9389e-06, 1.2448e-05, 1.1284e-05],\n              [1.1595e-05, 1.0768e-05, 6.1256e-06],\n              [1.2746e-05, 1.0310e-05, 5.4568e-06]]],\n    \n    \n            [[[2.9270e-06, 2.3729e-06, 2.1641e-06],\n              [2.7631e-06, 2.0798e-06, 1.0676e-06],\n              [2.6452e-06, 1.5855e-06, 5.8442e-07]]],\n    \n    \n            [[[4.9955e-06, 4.9263e-06, 7.4335e-06],\n              [1.2605e-06, 2.1843e-06, 5.7414e-06],\n              [1.1129e-06, 2.4911e-06, 6.8569e-06]]],\n    \n    \n            [[[4.7889e-06, 4.4115e-06, 4.5643e-06],\n              [2.1458e-06, 1.4067e-06, 1.9269e-06],\n              [1.4395e-06, 1.0133e-06, 2.1617e-06]]],\n    \n    \n            [[[3.6312e-05, 3.0896e-05, 2.2699e-05],\n              [3.9597e-05, 3.0295e-05, 2.7017e-05],\n              [3.6842e-05, 2.7099e-05, 2.7631e-05]]],\n    \n    \n            [[[1.3434e-05, 1.0003e-05, 1.6586e-05],\n              [1.6330e-05, 1.5522e-05, 1.6127e-05],\n              [2.0479e-05, 1.4905e-05, 1.4698e-05]]],\n    \n    \n            [[[5.4762e-06, 4.2221e-06, 2.8434e-06],\n              [4.2429e-06, 2.3795e-06, 3.4763e-07],\n              [4.8609e-06, 3.5162e-06, 3.4575e-06]]],\n    \n    \n            [[[6.0789e-05, 5.5811e-05, 6.0432e-05],\n              [5.4539e-05, 5.1690e-05, 5.9649e-05],\n              [5.3488e-05, 5.0796e-05, 6.5064e-05]]],\n    \n    \n            [[[1.8390e-06, 1.9461e-06, 4.8319e-06],\n              [2.1025e-06, 2.2039e-06, 3.9687e-06],\n              [2.0523e-06, 1.5310e-06, 3.5792e-06]]],\n    \n    \n            [[[3.0926e-05, 2.7568e-05, 2.6369e-05],\n              [2.7281e-05, 2.0134e-05, 2.3977e-05],\n              [3.0265e-05, 2.7445e-05, 2.4160e-05]]],\n    \n    \n            [[[2.7189e-05, 2.0211e-05, 2.8314e-05],\n              [2.5392e-05, 2.3151e-05, 2.4482e-05],\n              [2.4672e-05, 2.2575e-05, 2.5949e-05]]],\n    \n    \n            [[[5.5062e-05, 5.7771e-05, 6.1329e-05],\n              [5.3224e-05, 4.7717e-05, 5.9772e-05],\n              [5.6872e-05, 4.9043e-05, 5.3883e-05]]],\n    \n    \n            [[[4.2738e-06, 6.5919e-06, 1.2187e-05],\n              [7.3811e-06, 9.1691e-06, 1.1754e-05],\n              [1.0169e-05, 1.0128e-05, 1.1674e-05]]],\n    \n    \n            [[[9.4854e-07, 5.3875e-07, 9.7437e-07],\n              [5.4779e-07, 4.5838e-07, 1.2438e-06],\n              [6.8728e-07, 8.8504e-07, 1.9926e-06]]]], device='cuda:0')},\n   3: {'step': tensor(10550.),\n    'exp_avg': tensor([-3.8173e-04, -6.3993e-04, -3.5798e-05,  7.2340e-04, -1.8905e-04,\n             2.3586e-04,  1.2507e-04,  2.4643e-04,  1.6418e-04,  6.1056e-04,\n            -5.3957e-05, -1.5076e-04,  1.4579e-05, -3.2783e-04, -7.1536e-05,\n            -5.7338e-05,  1.6380e-04, -5.3405e-04,  1.1768e-03,  1.3384e-04,\n             6.0764e-04, -2.9328e-04, -3.5810e-04, -4.1744e-04, -6.8162e-04,\n             3.1358e-04,  2.3601e-05,  1.1834e-04, -7.0750e-04, -8.4985e-05,\n            -1.2648e-04, -5.5812e-04, -7.8314e-05, -6.8143e-04, -4.0045e-04,\n            -1.1644e-04, -1.2317e-03, -7.4706e-04, -4.8365e-05,  7.0043e-04,\n            -1.6990e-04, -1.4859e-04,  5.8693e-04,  9.8156e-05,  1.6625e-03,\n             5.4328e-04, -4.1307e-04,  4.8305e-04,  5.9200e-05,  1.7755e-04,\n            -8.4704e-06,  3.1868e-04,  7.2374e-04,  2.6487e-05, -2.5185e-04,\n            -4.5885e-04,  5.5615e-04,  2.3071e-04,  7.6146e-05, -4.3624e-04,\n            -5.1530e-04, -3.3726e-04, -6.6683e-04, -3.9212e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([1.9316e-06, 2.3296e-06, 3.5671e-06, 3.6702e-06, 2.4570e-06, 2.5559e-06,\n            4.9919e-06, 2.8692e-06, 1.5825e-06, 2.7474e-06, 3.2613e-06, 2.4463e-06,\n            2.7086e-06, 2.4974e-06, 3.6550e-06, 2.7734e-06, 2.8861e-06, 2.0750e-06,\n            4.7088e-06, 1.9957e-06, 1.7017e-06, 1.2461e-06, 2.3876e-06, 3.2152e-06,\n            2.8255e-06, 2.1082e-06, 2.5369e-06, 2.3685e-06, 2.7703e-06, 3.6291e-06,\n            3.1818e-06, 2.0637e-06, 1.8720e-06, 3.0078e-06, 2.7264e-06, 1.6448e-06,\n            6.5316e-06, 3.8760e-06, 2.7750e-06, 1.6017e-06, 3.5759e-06, 1.8470e-06,\n            1.0787e-06, 1.9583e-06, 3.4369e-06, 1.9784e-06, 3.2131e-06, 3.0214e-06,\n            1.3604e-06, 1.7719e-06, 2.7454e-06, 1.8784e-06, 2.2339e-06, 2.2669e-06,\n            2.9901e-06, 2.4571e-06, 2.1034e-06, 3.8070e-06, 1.6792e-06, 3.1775e-06,\n            3.2998e-06, 3.4129e-06, 3.0981e-06, 1.9858e-06], device='cuda:0')},\n   4: {'step': tensor(10550.),\n    'exp_avg': tensor([-9.0761e-04,  2.8959e-04, -2.7903e-04, -5.0885e-05,  3.5172e-04,\n            -9.7215e-05,  3.7664e-04, -5.5549e-04, -2.7247e-04, -3.3844e-05,\n            -2.3711e-04,  9.9994e-05, -1.5616e-04, -5.6193e-04,  1.8757e-04,\n             6.9976e-04,  1.7390e-04,  3.1687e-04, -7.2088e-04, -2.8590e-04,\n            -3.8526e-04,  6.6393e-06,  4.7760e-04, -3.2490e-05, -4.0518e-04,\n            -6.4480e-06,  4.8446e-04,  2.0034e-04, -4.5534e-04, -5.3842e-05,\n             1.7241e-04, -5.2458e-04,  3.8584e-04, -2.5983e-04, -3.5534e-04,\n            -4.2182e-04, -4.1338e-04, -3.2127e-05, -8.0141e-05,  1.0005e-04,\n             5.4424e-05, -5.9296e-04,  1.5453e-04, -8.3997e-05,  3.1466e-04,\n             8.4268e-05, -3.9545e-04, -2.3251e-04, -5.5843e-04, -6.1515e-04,\n             3.9924e-04, -1.7453e-04,  7.9763e-05, -1.1471e-04,  3.6179e-04,\n             5.1572e-04, -2.3022e-04, -1.6394e-04, -2.2303e-04,  1.0397e-04,\n             3.3244e-04,  3.0345e-04,  2.8287e-04,  2.9482e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([2.2289e-06, 1.9606e-06, 1.9120e-06, 1.6636e-06, 1.4185e-06, 2.7386e-06,\n            3.0399e-06, 2.2831e-06, 1.7211e-06, 1.7446e-06, 1.3724e-06, 2.1448e-06,\n            2.2935e-06, 1.9851e-06, 2.0287e-06, 2.4133e-06, 1.5995e-06, 2.2928e-06,\n            2.6818e-06, 2.0041e-06, 2.2385e-06, 1.7130e-06, 1.3845e-06, 2.4763e-06,\n            2.7904e-06, 1.8666e-06, 1.3505e-06, 1.9675e-06, 1.9477e-06, 2.4642e-06,\n            1.4969e-06, 2.6183e-06, 1.1778e-06, 1.5859e-06, 1.9664e-06, 1.7536e-06,\n            2.8843e-06, 1.8397e-06, 1.8922e-06, 1.2016e-06, 2.2445e-06, 1.8439e-06,\n            1.1448e-06, 2.0561e-06, 1.8916e-06, 2.0509e-06, 1.9401e-06, 1.8169e-06,\n            2.0827e-06, 2.4466e-06, 2.1836e-06, 1.5416e-06, 3.0338e-06, 1.6908e-06,\n            2.0963e-06, 1.7238e-06, 1.9811e-06, 1.7498e-06, 1.5951e-06, 1.4228e-06,\n            1.7689e-06, 2.2294e-06, 2.4056e-06, 2.2818e-06], device='cuda:0')},\n   5: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-2.5873e-04]],\n    \n             [[ 8.5031e-04]],\n    \n             [[ 2.4087e-04]],\n    \n             ...,\n    \n             [[ 5.5036e-05]],\n    \n             [[ 5.9206e-04]],\n    \n             [[ 5.6932e-04]]],\n    \n    \n            [[[-5.0446e-05]],\n    \n             [[-9.8049e-05]],\n    \n             [[-4.0325e-04]],\n    \n             ...,\n    \n             [[ 1.5462e-04]],\n    \n             [[-1.4562e-04]],\n    \n             [[-2.9148e-05]]],\n    \n    \n            [[[-1.5293e-04]],\n    \n             [[ 4.3952e-05]],\n    \n             [[ 3.2455e-04]],\n    \n             ...,\n    \n             [[-3.6010e-04]],\n    \n             [[ 4.0695e-04]],\n    \n             [[ 3.6617e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[-1.8820e-04]],\n    \n             [[ 5.7595e-04]],\n    \n             [[ 2.9315e-04]],\n    \n             ...,\n    \n             [[ 2.6989e-04]],\n    \n             [[-4.2494e-04]],\n    \n             [[ 5.0100e-05]]],\n    \n    \n            [[[ 4.5341e-05]],\n    \n             [[-6.8937e-04]],\n    \n             [[-2.2826e-04]],\n    \n             ...,\n    \n             [[-2.1636e-04]],\n    \n             [[-1.0345e-04]],\n    \n             [[-1.2160e-04]]],\n    \n    \n            [[[-3.3974e-05]],\n    \n             [[-2.8837e-05]],\n    \n             [[-1.8564e-04]],\n    \n             ...,\n    \n             [[-9.1739e-05]],\n    \n             [[ 1.8298e-05]],\n    \n             [[ 2.9869e-04]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[2.2531e-06]],\n    \n             [[3.4224e-06]],\n    \n             [[2.3139e-06]],\n    \n             ...,\n    \n             [[3.8634e-06]],\n    \n             [[2.8583e-06]],\n    \n             [[2.2110e-06]]],\n    \n    \n            [[[1.3531e-06]],\n    \n             [[2.3888e-06]],\n    \n             [[2.4963e-06]],\n    \n             ...,\n    \n             [[2.2411e-06]],\n    \n             [[1.5921e-06]],\n    \n             [[1.3754e-06]]],\n    \n    \n            [[[9.5619e-07]],\n    \n             [[2.0632e-06]],\n    \n             [[3.5130e-06]],\n    \n             ...,\n    \n             [[3.2722e-06]],\n    \n             [[2.8760e-06]],\n    \n             [[1.8761e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[4.1925e-06]],\n    \n             [[3.9297e-06]],\n    \n             [[4.9423e-06]],\n    \n             ...,\n    \n             [[2.8897e-06]],\n    \n             [[4.9628e-06]],\n    \n             [[4.6027e-06]]],\n    \n    \n            [[[1.0022e-06]],\n    \n             [[3.5439e-06]],\n    \n             [[2.8995e-06]],\n    \n             ...,\n    \n             [[4.1508e-06]],\n    \n             [[3.9463e-06]],\n    \n             [[2.1231e-06]]],\n    \n    \n            [[[2.5248e-06]],\n    \n             [[4.1438e-06]],\n    \n             [[5.1533e-06]],\n    \n             ...,\n    \n             [[6.0185e-06]],\n    \n             [[1.8898e-06]],\n    \n             [[3.0391e-06]]]], device='cuda:0')},\n   6: {'step': tensor(10550.),\n    'exp_avg': tensor([-5.9071e-04,  2.9752e-06, -6.9262e-04,  3.1660e-04, -2.5472e-04,\n            -1.2741e-04,  4.1043e-04,  2.1595e-04,  1.3246e-04, -2.2437e-04,\n             1.4962e-04,  7.5961e-04, -1.9520e-05, -2.7038e-04,  5.1592e-04,\n            -2.6261e-04, -4.2963e-04, -3.0889e-04, -8.8109e-04, -4.3867e-04,\n            -2.2923e-04,  3.5628e-04, -5.5034e-04,  1.3901e-03,  2.6650e-04,\n            -1.4805e-04,  2.0579e-04,  2.5524e-04,  9.3270e-05, -1.2054e-04,\n             6.3315e-05,  9.2658e-05, -7.8628e-05, -2.8790e-04,  3.5436e-04,\n             9.6195e-05, -1.8534e-04, -4.4406e-04,  4.2178e-05, -1.3980e-04,\n             1.9450e-05, -1.3496e-05,  5.7237e-04,  1.5700e-04,  2.7706e-05,\n             1.0356e-04, -2.0009e-04,  1.7023e-04, -2.6108e-04, -1.5654e-03,\n            -2.5969e-04, -1.1708e-04, -5.3756e-05, -2.7060e-04, -1.1793e-04,\n            -4.4663e-04, -1.6703e-04, -4.9660e-05,  8.0263e-05,  2.0250e-05,\n            -8.2695e-05, -2.1433e-05, -4.1520e-05, -1.0947e-03], device='cuda:0'),\n    'exp_avg_sq': tensor([9.6422e-07, 3.1604e-07, 1.8279e-06, 5.1557e-07, 1.5019e-06, 9.7759e-07,\n            2.7079e-06, 2.3781e-06, 3.0889e-07, 2.0476e-06, 2.6780e-06, 4.9694e-06,\n            1.0738e-06, 5.3159e-06, 9.4547e-07, 1.1950e-06, 1.0524e-06, 6.3424e-06,\n            4.7427e-06, 1.7625e-06, 3.3061e-06, 2.8645e-06, 4.6195e-06, 4.0542e-06,\n            6.9300e-07, 8.3359e-07, 1.0610e-06, 5.6163e-06, 5.3496e-06, 2.3703e-06,\n            3.2422e-07, 1.7339e-06, 1.8248e-06, 6.2749e-06, 1.5226e-06, 4.4201e-07,\n            7.9021e-06, 8.0618e-06, 3.5602e-07, 9.8978e-07, 6.0487e-07, 1.9195e-06,\n            3.8978e-06, 7.6098e-06, 8.2523e-07, 3.6047e-07, 4.6987e-07, 9.2776e-06,\n            3.1347e-06, 7.4233e-06, 2.5985e-06, 9.3702e-07, 1.3174e-06, 4.1630e-06,\n            5.3176e-07, 5.0003e-06, 2.3502e-06, 2.8305e-07, 4.5198e-07, 6.6806e-07,\n            7.4233e-07, 1.2186e-06, 1.9059e-06, 4.4257e-06], device='cuda:0')},\n   7: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-6.3056e-05,  1.2944e-04,  5.5833e-04,  7.5172e-04, -2.0318e-04],\n              [ 1.6692e-04, -3.9671e-04,  2.3174e-05,  9.7124e-04,  3.2089e-04],\n              [ 8.3354e-05, -8.9729e-05, -4.7685e-04,  1.0295e-03,  6.5035e-04],\n              [-6.9490e-04, -3.1419e-04, -5.9654e-04,  1.7446e-04,  3.7905e-04],\n              [-3.3191e-04, -6.4009e-04, -1.9370e-04,  7.5377e-05, -1.7070e-04]]],\n    \n    \n            [[[-1.0913e-03, -5.8596e-04, -8.8179e-04, -1.4984e-03, -1.4138e-03],\n              [-3.8078e-04, -1.5571e-03, -1.4551e-04, -1.0495e-03, -1.1935e-03],\n              [-4.0552e-04, -1.1783e-03, -5.5244e-04, -8.0134e-04, -8.1266e-04],\n              [ 1.1457e-03, -1.0362e-03, -1.3603e-03,  6.3759e-04, -1.4062e-05],\n              [ 1.9011e-04,  1.6615e-04, -5.0981e-04, -3.3996e-04,  2.1136e-04]]],\n    \n    \n            [[[ 1.1565e-05,  2.5651e-04,  6.0423e-05, -6.7573e-05,  2.0126e-04],\n              [-6.5066e-07, -2.1418e-04, -8.7135e-05,  1.7741e-04,  3.0867e-04],\n              [ 2.8376e-04,  5.2611e-05,  1.7770e-04,  1.0732e-04,  2.6372e-04],\n              [ 3.0079e-04, -7.3188e-05, -2.8014e-04,  2.3298e-04,  1.8114e-04],\n              [ 2.2262e-04,  2.2320e-06,  1.7050e-04,  1.9541e-04,  3.7029e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[-6.0986e-04, -7.6170e-04, -7.9371e-04,  7.0235e-05, -1.2741e-05],\n              [-8.2303e-04, -7.9019e-04, -4.9053e-04, -3.0140e-04, -2.3883e-04],\n              [-4.7077e-04,  3.5933e-04,  5.0276e-04, -2.0229e-04, -8.0029e-04],\n              [ 1.7148e-04,  7.2021e-04,  4.0283e-04,  9.9108e-04, -5.5931e-04],\n              [ 5.7457e-04,  5.0456e-05,  5.1422e-05,  7.1585e-04, -4.0473e-04]]],\n    \n    \n            [[[ 6.9261e-04,  1.2429e-03,  1.3650e-03, -3.4273e-04,  8.3953e-05],\n              [ 5.5320e-04,  1.2419e-03,  1.3543e-03,  2.1871e-04, -2.7575e-04],\n              [ 2.2839e-04,  9.5771e-04,  5.9446e-04,  9.9689e-04,  1.1182e-04],\n              [-2.7685e-04,  9.1174e-04,  1.5891e-03, -1.2616e-05,  1.8287e-03],\n              [-5.8684e-04,  1.5395e-04,  1.5438e-03, -6.8519e-04,  1.6832e-03]]],\n    \n    \n            [[[-8.2541e-04, -2.7554e-04, -7.6713e-04, -2.3433e-04, -1.8981e-04],\n              [-5.6494e-04, -8.1526e-04, -8.4100e-04, -8.9563e-04, -8.8957e-04],\n              [ 5.7781e-04, -7.1505e-04, -1.5385e-03, -7.0392e-04, -3.1737e-04],\n              [-2.7677e-05,  9.7937e-05, -6.6970e-04, -8.5533e-04, -5.5820e-05],\n              [-5.3282e-04,  3.2520e-05, -5.1604e-04, -7.3091e-04, -8.4918e-04]]]],\n           device='cuda:0'),\n    'exp_avg_sq': tensor([[[[3.9567e-06, 4.5647e-06, 4.1938e-06, 3.9306e-06, 4.4267e-06],\n              [4.0455e-06, 4.1777e-06, 4.5860e-06, 4.7505e-06, 4.2107e-06],\n              [4.6013e-06, 5.1662e-06, 5.0320e-06, 5.6852e-06, 4.9194e-06],\n              [5.1993e-06, 6.4602e-06, 5.7243e-06, 5.5439e-06, 4.9825e-06],\n              [4.7022e-06, 4.5132e-06, 4.3682e-06, 4.7903e-06, 5.2374e-06]]],\n    \n    \n            [[[1.2229e-05, 1.5171e-05, 1.5546e-05, 1.3197e-05, 1.1608e-05],\n              [1.5007e-05, 1.6079e-05, 1.4457e-05, 1.2060e-05, 1.0867e-05],\n              [1.7637e-05, 1.4004e-05, 1.4921e-05, 1.2609e-05, 1.2188e-05],\n              [1.2332e-05, 1.2754e-05, 1.6179e-05, 1.3755e-05, 1.2404e-05],\n              [1.1654e-05, 1.3002e-05, 1.0974e-05, 1.2534e-05, 1.1422e-05]]],\n    \n    \n            [[[2.4847e-06, 3.0635e-06, 2.8818e-06, 2.6689e-06, 2.7818e-06],\n              [3.0326e-06, 2.9395e-06, 3.2981e-06, 3.5259e-06, 2.8856e-06],\n              [3.5606e-06, 3.2894e-06, 3.9806e-06, 3.6902e-06, 3.1121e-06],\n              [3.5759e-06, 3.7778e-06, 4.3131e-06, 3.4501e-06, 3.4202e-06],\n              [3.7829e-06, 3.6573e-06, 4.2345e-06, 4.1091e-06, 3.6235e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[6.4336e-06, 8.2037e-06, 1.1303e-05, 1.1748e-05, 9.6572e-06],\n              [7.6125e-06, 8.0200e-06, 1.0873e-05, 1.5410e-05, 9.7847e-06],\n              [6.5590e-06, 7.6635e-06, 8.9344e-06, 1.0066e-05, 1.1385e-05],\n              [8.2440e-06, 8.9520e-06, 8.7200e-06, 8.5297e-06, 8.4214e-06],\n              [6.3386e-06, 6.5823e-06, 8.3276e-06, 7.8333e-06, 5.8048e-06]]],\n    \n    \n            [[[6.5229e-06, 8.7620e-06, 9.4407e-06, 1.1829e-05, 1.0945e-05],\n              [6.7383e-06, 9.2289e-06, 1.1170e-05, 1.0935e-05, 1.0634e-05],\n              [6.9510e-06, 8.8638e-06, 1.4401e-05, 1.0854e-05, 1.0770e-05],\n              [6.3179e-06, 9.1873e-06, 1.1060e-05, 1.0653e-05, 8.4412e-06],\n              [5.2490e-06, 6.6495e-06, 8.7604e-06, 1.0720e-05, 8.1595e-06]]],\n    \n    \n            [[[2.2419e-06, 2.4850e-06, 2.4963e-06, 3.0232e-06, 3.6094e-06],\n              [2.0981e-06, 2.8491e-06, 3.3450e-06, 4.0793e-06, 3.7822e-06],\n              [2.2815e-06, 2.6596e-06, 3.8642e-06, 3.3544e-06, 2.8797e-06],\n              [2.2608e-06, 2.4802e-06, 2.6697e-06, 2.7696e-06, 2.7673e-06],\n              [1.8063e-06, 2.5352e-06, 1.9722e-06, 3.3259e-06, 2.4463e-06]]]],\n           device='cuda:0')},\n   8: {'step': tensor(10550.),\n    'exp_avg': tensor([ 1.1973e-04, -4.2063e-04, -6.3642e-05, -2.5904e-04, -1.4908e-04,\n            -4.8759e-04,  2.8335e-04, -2.9631e-04, -2.9345e-05, -1.2602e-04,\n            -8.9125e-05,  1.4196e-04,  2.1204e-04,  1.3470e-04, -1.5291e-04,\n             9.1350e-05, -2.3829e-04,  3.3008e-04, -2.4201e-04,  3.0612e-04,\n             6.6480e-04, -9.5974e-04, -1.1523e-04, -2.5713e-04, -6.9845e-04,\n            -3.3078e-04,  3.8199e-04, -4.7501e-05,  6.4566e-04,  6.9580e-04,\n            -2.3776e-04, -5.7092e-04, -1.0520e-03, -1.5700e-04,  1.4639e-04,\n             2.7706e-04, -6.7664e-04, -2.1522e-04, -4.3758e-04, -2.3467e-04,\n             2.2628e-04,  3.7012e-04, -2.2066e-04, -3.6066e-04, -8.5171e-05,\n            -2.9515e-04, -1.9620e-04,  1.9853e-05, -1.8729e-04, -4.5961e-04,\n             4.4591e-04,  2.2096e-04,  4.7327e-04, -3.4635e-04, -4.9429e-04,\n            -6.5014e-04,  6.3065e-04, -7.6832e-05,  1.0595e-06,  5.0939e-04,\n            -1.6660e-05, -8.0915e-04,  2.7951e-04, -8.2746e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([1.5974e-06, 2.2519e-06, 1.6887e-06, 1.0406e-06, 6.7296e-06, 1.9840e-06,\n            3.5957e-06, 9.5513e-07, 8.2329e-07, 4.3536e-06, 6.9117e-06, 2.1147e-06,\n            2.7837e-06, 4.5417e-06, 1.5496e-06, 1.8406e-06, 2.0727e-06, 3.0738e-06,\n            4.1026e-06, 1.2797e-06, 4.9374e-06, 7.5260e-06, 5.2252e-06, 2.3176e-06,\n            3.5586e-06, 1.8997e-06, 3.7320e-06, 8.4493e-06, 1.2329e-05, 3.9509e-06,\n            9.7679e-07, 2.8015e-06, 3.3270e-06, 5.6449e-06, 1.4964e-06, 8.8052e-07,\n            2.6607e-05, 4.5829e-06, 4.1088e-06, 2.5606e-06, 1.4523e-06, 6.8746e-06,\n            2.9591e-06, 1.6110e-06, 1.0667e-06, 2.0636e-06, 1.0928e-06, 1.6547e-06,\n            1.6420e-06, 8.8184e-06, 4.7494e-06, 2.5953e-06, 2.1517e-06, 7.1554e-06,\n            2.7203e-06, 3.3846e-06, 3.1990e-06, 5.5505e-07, 1.3760e-06, 1.0344e-06,\n            2.7656e-06, 5.4384e-06, 2.5701e-06, 4.1445e-06], device='cuda:0')},\n   9: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-3.2662e-04,  5.6748e-06,  2.2235e-06,  1.3938e-04, -2.3780e-05],\n              [-1.5922e-04, -2.1713e-04,  5.2536e-04, -5.7976e-05, -4.5049e-05],\n              [ 1.5915e-04, -4.1534e-04, -5.8835e-04,  3.7667e-04,  1.4436e-04],\n              [ 2.0705e-04, -1.8376e-04,  2.3156e-04, -2.6721e-04,  7.2019e-05],\n              [ 6.5388e-05,  7.1563e-05,  4.6476e-04, -1.3099e-05,  4.9080e-04]]],\n    \n    \n            [[[-9.9046e-04,  1.5451e-03, -2.3067e-05, -5.5660e-04,  3.0126e-04],\n              [-3.1884e-04, -1.9104e-04,  5.8766e-05,  4.4357e-04, -3.1540e-04],\n              [ 3.2627e-04, -1.6281e-03,  5.4434e-04, -9.4622e-04,  8.1001e-05],\n              [-3.2735e-04,  1.4522e-03, -6.2791e-04,  5.4944e-04,  2.9078e-04],\n              [-4.6179e-04, -2.8124e-04,  6.9848e-04, -5.5570e-04, -1.5375e-04]]],\n    \n    \n            [[[ 6.9631e-06,  2.8207e-04,  2.2382e-04, -1.0118e-04, -1.2492e-06],\n              [-9.1315e-05,  7.4317e-05, -2.8654e-05,  6.8960e-05, -6.2162e-05],\n              [-1.8505e-04,  3.1927e-04,  1.0703e-04, -3.8765e-05,  1.6803e-04],\n              [ 2.2051e-05,  6.1079e-05,  3.0858e-05,  1.7514e-04, -1.0773e-04],\n              [ 2.9455e-05, -6.5564e-05, -1.9668e-04, -1.0090e-04,  1.2413e-05]]],\n    \n    \n            ...,\n    \n    \n            [[[ 1.0265e-03, -2.7076e-04,  6.4439e-04, -9.8939e-05, -1.5717e-04],\n              [ 8.5124e-05, -8.0395e-04,  1.3702e-03,  1.9616e-04,  3.7513e-05],\n              [-1.5630e-04, -2.0733e-03, -1.3580e-04,  9.0811e-04,  8.2628e-04],\n              [-4.5976e-05, -5.8444e-04, -8.8009e-04,  7.0630e-05,  3.3858e-04],\n              [ 1.5973e-04, -1.2738e-04, -1.0477e-03, -4.3723e-04,  9.8093e-05]]],\n    \n    \n            [[[ 3.3999e-04,  7.1236e-04,  1.3029e-03, -1.4427e-04,  1.4534e-04],\n              [ 1.2098e-04, -8.5859e-04, -1.2264e-03, -7.2680e-05,  1.9876e-04],\n              [-2.0898e-04, -8.0430e-04, -2.7831e-03, -1.1014e-03, -7.4567e-05],\n              [ 2.4503e-04,  6.9937e-04, -8.0096e-06,  3.9055e-05,  9.8243e-06],\n              [-9.9295e-05,  5.2899e-05, -3.6263e-04,  1.2331e-04,  3.9372e-04]]],\n    \n    \n            [[[-9.5735e-05, -5.3601e-05, -2.7964e-04,  8.4216e-05,  3.2971e-04],\n              [-1.3911e-04,  1.3049e-04, -1.8085e-04, -5.2829e-05,  6.9032e-04],\n              [-2.0948e-04,  1.8724e-05,  1.1978e-03,  9.7640e-05,  1.1471e-03],\n              [-8.5205e-05, -6.1637e-04,  4.8939e-04, -2.1952e-04,  7.7865e-04],\n              [ 3.5468e-04, -5.1698e-05,  4.3199e-04,  1.4228e-04,  4.2546e-04]]]],\n           device='cuda:0'),\n    'exp_avg_sq': tensor([[[[5.1213e-07, 1.8738e-06, 3.3253e-06, 2.2089e-06, 5.0474e-07],\n              [1.1929e-06, 4.0810e-06, 5.4883e-06, 3.1462e-06, 9.2359e-07],\n              [1.9647e-06, 7.1900e-06, 9.3150e-06, 5.5424e-06, 1.2373e-06],\n              [2.5213e-06, 7.6314e-06, 9.0589e-06, 4.4061e-06, 1.2135e-06],\n              [1.7221e-06, 3.4808e-06, 4.6604e-06, 2.5743e-06, 6.6342e-07]]],\n    \n    \n            [[[5.1285e-06, 1.2889e-05, 1.8468e-05, 1.4551e-05, 3.9717e-06],\n              [5.9126e-06, 1.8827e-05, 2.6564e-05, 2.2034e-05, 8.2242e-06],\n              [8.6580e-06, 2.3159e-05, 3.3783e-05, 2.6685e-05, 7.6211e-06],\n              [7.7164e-06, 1.6450e-05, 2.4918e-05, 2.1456e-05, 7.2348e-06],\n              [4.4412e-06, 9.8837e-06, 1.4818e-05, 1.2086e-05, 3.1508e-06]]],\n    \n    \n            [[[2.5125e-07, 1.2988e-06, 2.5281e-06, 1.9607e-06, 3.9364e-07],\n              [7.0972e-07, 2.3071e-06, 5.0365e-06, 4.9383e-06, 1.1875e-06],\n              [1.1580e-06, 3.6139e-06, 1.0026e-05, 5.9495e-06, 1.8157e-06],\n              [7.8400e-07, 3.3051e-06, 7.7791e-06, 6.2677e-06, 1.9740e-06],\n              [3.1825e-07, 1.8887e-06, 3.6760e-06, 3.0304e-06, 6.0073e-07]]],\n    \n    \n            ...,\n    \n    \n            [[[2.7324e-06, 7.0716e-06, 9.1791e-06, 6.2898e-06, 2.0377e-06],\n              [4.7839e-06, 1.3974e-05, 2.2312e-05, 1.2078e-05, 4.0558e-06],\n              [7.2779e-06, 1.7051e-05, 2.1386e-05, 1.5052e-05, 3.9524e-06],\n              [6.1238e-06, 1.3609e-05, 1.4660e-05, 1.1443e-05, 2.2135e-06],\n              [1.2853e-06, 4.3605e-06, 5.7002e-06, 3.4500e-06, 1.0978e-06]]],\n    \n    \n            [[[1.4209e-06, 6.2828e-06, 1.1001e-05, 1.0885e-05, 3.4366e-06],\n              [2.6984e-06, 1.1320e-05, 1.8246e-05, 1.6879e-05, 5.9841e-06],\n              [4.2235e-06, 1.1680e-05, 2.4342e-05, 2.0938e-05, 5.7414e-06],\n              [3.8594e-06, 1.0489e-05, 2.2073e-05, 1.4317e-05, 5.3149e-06],\n              [2.2260e-06, 5.7223e-06, 8.4119e-06, 5.8075e-06, 1.8583e-06]]],\n    \n    \n            [[[1.1494e-06, 2.7479e-06, 3.7823e-06, 4.6500e-06, 1.6135e-06],\n              [1.8076e-06, 4.5046e-06, 6.3586e-06, 5.1493e-06, 2.1747e-06],\n              [2.2485e-06, 6.2931e-06, 6.3909e-06, 7.4024e-06, 2.4550e-06],\n              [1.9060e-06, 4.3493e-06, 5.8990e-06, 4.1223e-06, 1.7797e-06],\n              [1.4801e-06, 3.3458e-06, 3.7549e-06, 2.3180e-06, 9.0046e-07]]]],\n           device='cuda:0')},\n   10: {'step': tensor(10550.),\n    'exp_avg': tensor([-1.4713e-04,  7.9550e-04,  8.8207e-04, -6.5724e-04, -7.0581e-04,\n            -3.6395e-04, -8.9670e-04, -9.3472e-04, -5.6433e-04,  6.6902e-04,\n            -8.2562e-04,  2.2576e-04, -1.6015e-03,  6.0190e-04,  1.6767e-03,\n            -7.8682e-04,  4.3893e-05,  1.9901e-04,  3.3111e-04,  6.8471e-04,\n            -1.2463e-03,  1.2291e-03, -7.7954e-04, -1.9466e-03, -1.1389e-04,\n            -5.0345e-04,  3.3426e-04,  3.6477e-04, -8.7120e-04,  1.3873e-03,\n            -8.1154e-04,  8.9978e-04, -1.0485e-03, -2.9532e-04,  4.8861e-04,\n             1.8637e-03,  3.0354e-05,  1.5921e-03,  5.6561e-04, -1.4787e-04,\n             9.7863e-04, -2.9535e-04, -5.8894e-04, -2.4560e-04,  5.8399e-04,\n            -9.2711e-04, -1.3306e-04,  1.2323e-03,  4.2823e-05,  6.4543e-04,\n             7.0417e-04,  3.2165e-04, -5.7297e-05,  9.9419e-04,  1.4707e-03,\n             4.8932e-04,  6.7373e-04,  2.0618e-04, -1.3324e-03, -7.9521e-04,\n            -7.0191e-05, -5.3214e-04,  2.9309e-04,  4.3335e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([9.6349e-06, 1.0042e-05, 9.6347e-06, 1.4529e-05, 1.6577e-05, 1.0831e-05,\n            1.4778e-05, 8.7191e-06, 8.9036e-06, 1.2991e-05, 1.5426e-05, 2.2030e-05,\n            1.9560e-05, 1.1233e-05, 1.5721e-05, 1.1371e-05, 1.0538e-05, 1.2762e-05,\n            1.3406e-05, 1.3613e-05, 1.3727e-05, 1.5256e-05, 1.1352e-05, 1.7619e-05,\n            1.0061e-05, 1.1223e-05, 1.5095e-05, 9.6367e-06, 1.5711e-05, 1.2071e-05,\n            9.4266e-06, 1.0279e-05, 2.3281e-05, 1.1356e-05, 1.2039e-05, 8.8091e-06,\n            1.4888e-05, 1.1136e-05, 9.6136e-06, 1.5383e-05, 8.9639e-06, 1.4505e-05,\n            1.5051e-05, 1.4276e-05, 1.2078e-05, 1.1473e-05, 1.0957e-05, 1.7346e-05,\n            1.4924e-05, 1.1705e-05, 1.6046e-05, 1.3313e-05, 7.9177e-06, 1.3003e-05,\n            1.4782e-05, 9.5135e-06, 1.3004e-05, 7.1499e-06, 9.1170e-06, 1.6940e-05,\n            9.3357e-06, 1.4616e-05, 1.1749e-05, 1.2616e-05], device='cuda:0')},\n   11: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-6.4373e-06]],\n    \n             [[-2.4703e-03]],\n    \n             [[-7.9859e-05]],\n    \n             ...,\n    \n             [[-1.2076e-03]],\n    \n             [[-3.3154e-04]],\n    \n             [[-1.3082e-03]]],\n    \n    \n            [[[ 1.6802e-04]],\n    \n             [[ 1.0641e-03]],\n    \n             [[ 7.1735e-05]],\n    \n             ...,\n    \n             [[-2.1079e-04]],\n    \n             [[ 9.0232e-04]],\n    \n             [[ 2.0482e-05]]],\n    \n    \n            [[[-8.2130e-05]],\n    \n             [[ 8.2838e-04]],\n    \n             [[-1.3243e-05]],\n    \n             ...,\n    \n             [[ 2.7298e-04]],\n    \n             [[ 5.7359e-04]],\n    \n             [[ 1.9813e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[-3.3413e-04]],\n    \n             [[-5.3515e-04]],\n    \n             [[ 1.1629e-04]],\n    \n             ...,\n    \n             [[ 4.5002e-04]],\n    \n             [[ 2.2814e-04]],\n    \n             [[ 4.7320e-04]]],\n    \n    \n            [[[-5.8987e-04]],\n    \n             [[-3.5966e-04]],\n    \n             [[-1.1664e-04]],\n    \n             ...,\n    \n             [[-6.8444e-05]],\n    \n             [[ 1.0578e-03]],\n    \n             [[-3.1379e-04]]],\n    \n    \n            [[[-2.0756e-05]],\n    \n             [[-9.8048e-06]],\n    \n             [[-3.1427e-04]],\n    \n             ...,\n    \n             [[ 6.8688e-04]],\n    \n             [[-4.4148e-04]],\n    \n             [[-7.4442e-04]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[2.4374e-06]],\n    \n             [[1.7255e-05]],\n    \n             [[2.1506e-06]],\n    \n             ...,\n    \n             [[4.9189e-06]],\n    \n             [[7.3876e-06]],\n    \n             [[3.0549e-06]]],\n    \n    \n            [[[8.5789e-07]],\n    \n             [[4.3748e-06]],\n    \n             [[7.0592e-07]],\n    \n             ...,\n    \n             [[1.2870e-06]],\n    \n             [[2.0149e-06]],\n    \n             [[1.1434e-06]]],\n    \n    \n            [[[1.7012e-06]],\n    \n             [[6.3093e-06]],\n    \n             [[2.1025e-06]],\n    \n             ...,\n    \n             [[3.6683e-06]],\n    \n             [[4.4160e-06]],\n    \n             [[4.2183e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[3.9572e-06]],\n    \n             [[1.4588e-05]],\n    \n             [[4.2136e-06]],\n    \n             ...,\n    \n             [[6.0137e-06]],\n    \n             [[9.0777e-06]],\n    \n             [[5.3428e-06]]],\n    \n    \n            [[[2.8890e-06]],\n    \n             [[1.0482e-05]],\n    \n             [[2.0389e-06]],\n    \n             ...,\n    \n             [[5.0853e-06]],\n    \n             [[8.5255e-06]],\n    \n             [[6.0226e-06]]],\n    \n    \n            [[[2.3416e-06]],\n    \n             [[6.7023e-06]],\n    \n             [[1.3538e-06]],\n    \n             ...,\n    \n             [[3.2100e-06]],\n    \n             [[6.7113e-06]],\n    \n             [[2.3658e-06]]]], device='cuda:0')},\n   12: {'step': tensor(10550.),\n    'exp_avg': tensor([-2.3227e-03,  3.7895e-04,  5.2808e-04, -1.3429e-03,  5.0137e-04,\n             7.4702e-04,  1.0627e-03, -6.3283e-04,  2.2633e-04, -1.1993e-04,\n            -1.0196e-03, -4.5276e-04, -4.1452e-04,  1.2741e-03,  1.0076e-03,\n             1.0066e-03, -4.5160e-04,  1.6795e-05, -1.3832e-03,  6.9467e-04,\n             3.4191e-04,  1.2526e-03,  1.3126e-04,  6.4936e-04,  2.8775e-04,\n             5.8118e-04,  1.9000e-05, -1.1225e-03, -1.6330e-03, -3.7345e-04,\n             1.3857e-03,  1.1932e-03,  1.3728e-03, -8.3648e-05,  1.7366e-03,\n             8.4587e-04, -1.3841e-03, -9.9916e-05,  1.9909e-04, -1.0167e-03,\n            -1.2053e-03,  3.1430e-06, -9.3652e-05, -3.9407e-04, -9.4840e-05,\n             2.1704e-04, -2.2300e-04,  3.0986e-04, -3.5904e-04, -4.5175e-04,\n             2.1025e-04, -1.4133e-03,  1.1619e-04,  3.0615e-04, -9.7688e-04,\n             2.1036e-03,  1.4071e-03,  8.9009e-04,  5.4900e-04,  6.3747e-04,\n             6.0907e-04,  1.4741e-04, -1.3871e-03, -6.1953e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([1.5714e-05, 4.5587e-06, 1.5777e-05, 7.4157e-06, 5.6945e-06, 6.3839e-06,\n            1.8668e-05, 2.6397e-05, 1.1409e-05, 1.0354e-05, 8.9386e-06, 2.2477e-05,\n            7.0153e-06, 5.9145e-06, 6.8254e-06, 1.2677e-05, 9.6054e-06, 9.7160e-06,\n            1.7050e-05, 4.6990e-06, 2.5542e-05, 1.7308e-05, 6.0763e-06, 2.4194e-05,\n            1.3979e-05, 7.4078e-06, 9.0364e-06, 1.9247e-05, 2.6510e-05, 8.1201e-06,\n            1.6068e-05, 2.3850e-05, 4.7262e-06, 4.7639e-06, 2.2415e-05, 1.5907e-05,\n            8.2482e-06, 9.2752e-06, 1.4901e-05, 1.0179e-05, 1.3854e-05, 9.6488e-06,\n            9.4302e-06, 9.6022e-06, 9.8150e-06, 1.1441e-05, 1.3798e-05, 3.8788e-06,\n            3.6536e-05, 1.2528e-05, 1.1446e-05, 1.1050e-05, 1.9436e-05, 6.3977e-06,\n            8.5329e-06, 4.7806e-06, 1.6770e-05, 1.9535e-05, 1.3234e-05, 6.7823e-06,\n            1.4074e-05, 1.9782e-05, 2.2398e-05, 7.2924e-06], device='cuda:0')},\n   13: {'step': tensor(10550.),\n    'exp_avg': tensor([ 1.0411e-03, -1.8026e-05,  1.4051e-03, -1.6057e-04,  1.5776e-03,\n            -4.0529e-05, -1.5922e-03, -2.1992e-04, -1.3371e-03,  2.9957e-04,\n            -3.5130e-04, -5.7795e-04, -1.2092e-03, -7.5815e-05,  8.7031e-04,\n             1.6891e-03, -3.7336e-04,  1.5248e-04,  2.9268e-06, -1.2560e-03,\n             2.8130e-04, -4.0375e-04, -4.0708e-05, -8.8025e-04,  5.2289e-04,\n             8.3110e-04, -1.6126e-04, -5.3401e-04,  2.3152e-03,  1.7221e-05,\n             1.6429e-04, -8.5800e-04,  1.4684e-03, -1.0376e-04,  1.0303e-03,\n             8.6198e-04,  9.2086e-06,  9.7756e-04, -6.1738e-05,  5.0648e-04,\n             7.0382e-04, -6.5801e-04, -2.0287e-04,  9.3810e-04, -1.6404e-04,\n             3.3909e-04,  1.4519e-03, -5.1168e-05,  9.0189e-04, -1.4871e-03,\n             8.0040e-04, -9.7881e-04, -3.2495e-03,  7.0173e-04, -7.2819e-04,\n            -1.6109e-03, -7.2557e-04, -8.0400e-04,  3.4226e-04, -2.9963e-04,\n            -4.7019e-04,  1.0300e-04, -1.1192e-07, -5.5543e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([1.3932e-05, 4.9104e-06, 9.8850e-06, 4.6994e-06, 1.1762e-05, 6.5036e-06,\n            1.5442e-05, 2.7976e-05, 6.1659e-06, 1.4564e-05, 8.1922e-06, 8.5394e-06,\n            1.0843e-05, 6.7767e-06, 4.2499e-06, 1.0629e-05, 1.2881e-05, 7.7105e-06,\n            1.0566e-05, 1.4442e-05, 6.6253e-06, 1.2222e-05, 9.6326e-06, 1.0751e-05,\n            6.8270e-06, 5.8040e-06, 1.1824e-05, 1.1035e-05, 1.4482e-05, 1.2010e-05,\n            6.8957e-06, 1.9259e-05, 1.0040e-05, 9.5525e-06, 8.1085e-06, 6.8553e-06,\n            6.9591e-06, 6.9068e-06, 7.4930e-06, 6.4533e-06, 5.5760e-06, 8.4614e-06,\n            1.0444e-05, 1.5206e-05, 5.5316e-06, 3.7183e-06, 6.0901e-06, 7.9504e-06,\n            1.4104e-05, 2.0256e-05, 1.6561e-05, 6.8250e-06, 1.2938e-05, 2.0554e-05,\n            7.6552e-06, 7.3672e-06, 7.6509e-06, 7.4454e-06, 6.5847e-06, 5.7115e-06,\n            1.3785e-05, 2.0393e-05, 1.2442e-05, 1.1800e-05], device='cuda:0')},\n   14: {'step': tensor(10550.),\n    'exp_avg': tensor([ 8.5247e-04,  6.5114e-04, -2.3774e-04, -2.3017e-05,  1.0635e-03,\n             3.4399e-04,  8.6257e-04,  1.0110e-03, -1.3110e-03, -2.1017e-04,\n            -1.1205e-03,  5.4106e-04, -3.0489e-04,  1.1965e-03,  4.6474e-04,\n            -1.3040e-03,  3.8944e-04,  5.9382e-04,  1.4494e-03, -1.4427e-03,\n            -6.6927e-04, -9.7966e-04,  3.2754e-04,  2.2750e-03, -1.0622e-03,\n             3.2083e-04, -1.2087e-03,  6.7237e-04,  2.2385e-04, -5.7734e-04,\n             3.8591e-04,  1.3456e-03,  2.8623e-04,  4.7233e-04, -2.8228e-04,\n            -7.9155e-04, -4.3062e-04, -1.4096e-03,  5.0530e-04,  9.2520e-04,\n             7.0868e-05, -3.0607e-04,  9.0573e-04, -1.3211e-03,  2.4902e-04,\n             6.3005e-04,  9.5311e-05, -5.5804e-04, -1.0811e-03,  1.2010e-03,\n            -2.5440e-05,  1.7835e-05, -8.3796e-04,  8.6216e-04, -3.5034e-04,\n             2.0250e-04,  8.8481e-04,  8.4847e-04, -6.3644e-04,  4.0940e-05,\n            -1.1025e-04, -3.8569e-04, -5.3687e-04, -8.2884e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([4.6797e-06, 3.5888e-06, 3.8197e-06, 4.4529e-06, 4.1399e-06, 4.0532e-06,\n            2.8069e-06, 9.3318e-06, 4.7965e-06, 1.9921e-06, 3.4226e-06, 2.2678e-06,\n            5.9446e-06, 2.6037e-06, 2.2882e-06, 1.3358e-05, 4.1390e-06, 3.5404e-06,\n            5.9677e-06, 5.5978e-06, 6.3510e-06, 4.7959e-06, 3.6911e-06, 7.8671e-06,\n            2.3197e-06, 3.9438e-06, 3.5522e-06, 2.3720e-06, 2.7506e-06, 3.6245e-06,\n            2.1565e-06, 4.9196e-06, 2.1347e-06, 2.4339e-06, 2.7581e-06, 3.4596e-06,\n            1.7221e-06, 3.3927e-06, 2.1260e-06, 2.4390e-06, 1.7261e-06, 1.5477e-06,\n            3.0054e-06, 4.8200e-06, 2.8234e-06, 3.3032e-06, 2.5608e-06, 3.2362e-06,\n            6.1759e-06, 8.0558e-06, 2.6859e-06, 3.1310e-06, 3.8436e-06, 2.4934e-06,\n            2.3933e-06, 2.4696e-06, 3.0425e-06, 2.9976e-06, 3.1027e-06, 2.9550e-06,\n            6.9315e-06, 2.1018e-06, 2.2987e-06, 1.6790e-06], device='cuda:0')},\n   15: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[ 4.6255e-05]],\n    \n             [[-5.1453e-05]],\n    \n             [[ 2.6348e-04]],\n    \n             ...,\n    \n             [[ 4.7034e-05]],\n    \n             [[-8.2852e-05]],\n    \n             [[-4.1211e-04]]],\n    \n    \n            [[[ 3.4513e-04]],\n    \n             [[-1.0115e-04]],\n    \n             [[-1.9409e-05]],\n    \n             ...,\n    \n             [[ 3.0382e-05]],\n    \n             [[-2.8375e-05]],\n    \n             [[ 3.8811e-04]]],\n    \n    \n            [[[ 1.2637e-04]],\n    \n             [[-5.1246e-04]],\n    \n             [[ 5.4323e-04]],\n    \n             ...,\n    \n             [[-5.3337e-04]],\n    \n             [[ 4.6637e-04]],\n    \n             [[-7.5293e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[ 1.8255e-04]],\n    \n             [[ 1.0244e-04]],\n    \n             [[ 2.5143e-06]],\n    \n             ...,\n    \n             [[-6.9437e-05]],\n    \n             [[ 1.4112e-04]],\n    \n             [[-2.8539e-04]]],\n    \n    \n            [[[ 1.6960e-04]],\n    \n             [[-3.5782e-04]],\n    \n             [[ 8.6101e-04]],\n    \n             ...,\n    \n             [[-1.5906e-04]],\n    \n             [[ 3.0499e-04]],\n    \n             [[ 6.5276e-04]]],\n    \n    \n            [[[-2.9620e-05]],\n    \n             [[ 5.9565e-05]],\n    \n             [[ 1.3894e-04]],\n    \n             ...,\n    \n             [[ 8.3750e-05]],\n    \n             [[-1.1238e-04]],\n    \n             [[-8.4237e-05]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[1.8284e-07]],\n    \n             [[6.6557e-07]],\n    \n             [[1.8129e-06]],\n    \n             ...,\n    \n             [[8.7947e-07]],\n    \n             [[2.3293e-06]],\n    \n             [[1.7201e-06]]],\n    \n    \n            [[[1.7934e-06]],\n    \n             [[1.7400e-06]],\n    \n             [[1.1847e-06]],\n    \n             ...,\n    \n             [[2.7206e-06]],\n    \n             [[2.1659e-06]],\n    \n             [[2.8136e-06]]],\n    \n    \n            [[[2.4607e-06]],\n    \n             [[2.3798e-06]],\n    \n             [[1.0751e-06]],\n    \n             ...,\n    \n             [[1.7541e-06]],\n    \n             [[1.7060e-06]],\n    \n             [[2.1927e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[7.9299e-07]],\n    \n             [[6.1971e-07]],\n    \n             [[4.0060e-07]],\n    \n             ...,\n    \n             [[7.6957e-07]],\n    \n             [[1.1162e-06]],\n    \n             [[5.2090e-07]]],\n    \n    \n            [[[1.0263e-06]],\n    \n             [[7.7363e-06]],\n    \n             [[8.6066e-06]],\n    \n             ...,\n    \n             [[1.7072e-06]],\n    \n             [[5.8510e-06]],\n    \n             [[3.8339e-06]]],\n    \n    \n            [[[3.9703e-07]],\n    \n             [[1.2882e-07]],\n    \n             [[2.4567e-07]],\n    \n             ...,\n    \n             [[3.6416e-07]],\n    \n             [[3.6169e-07]],\n    \n             [[2.5602e-07]]]], device='cuda:0')},\n   16: {'step': tensor(10550.),\n    'exp_avg': tensor([ 4.4373e-04,  1.1508e-04,  1.6112e-04, -1.8186e-04,  4.8154e-05,\n             3.6125e-05,  3.4786e-05, -3.2610e-05,  1.7783e-03, -2.2451e-05,\n            -1.5239e-04, -1.5303e-05,  1.1871e-03, -2.3286e-04,  1.2596e-05,\n             3.4444e-05,  1.4952e-05, -1.7415e-05,  1.5821e-04, -1.1615e-05,\n            -7.8341e-06,  1.8100e-05, -2.3665e-04,  2.2084e-05,  2.6844e-06,\n             2.7622e-08, -2.0565e-04,  9.4582e-05,  4.2570e-04,  2.9312e-04,\n            -2.7713e-04,  1.5546e-04, -6.5316e-07, -2.9408e-05,  9.1898e-05,\n            -5.3087e-05, -3.0033e-05,  2.1904e-05,  1.4032e-05,  3.3288e-04,\n             5.7243e-05, -8.6113e-05, -1.9461e-04,  7.6283e-05,  2.3918e-04,\n            -2.8574e-04, -8.5317e-05, -1.9989e-04, -8.1852e-06, -1.6294e-03,\n            -5.5930e-04,  5.1110e-05, -8.8635e-04,  1.3032e-05,  9.5112e-04,\n            -2.0637e-05,  2.9049e-03, -3.8802e-05,  1.7866e-05,  5.7881e-06,\n            -3.1192e-05, -9.3219e-04,  2.7286e-04,  3.9344e-05,  2.8979e-04,\n            -9.1116e-04, -6.8396e-05,  2.3689e-03, -2.5746e-05, -1.8830e-07,\n            -1.6166e-04, -1.5863e-04,  1.7991e-04, -1.2170e-05, -4.0817e-04,\n             1.4339e-04,  3.9782e-05,  2.2781e-05, -1.4215e-05, -1.4423e-05,\n             4.4294e-05, -1.2773e-05, -1.0304e-04, -2.5921e-05, -2.0857e-04,\n             5.2073e-04, -1.5650e-05, -2.3487e-06, -1.1753e-03,  1.6258e-04,\n             5.0884e-05,  1.1987e-05, -5.2405e-05,  9.5571e-05, -5.1945e-05,\n            -6.8934e-05,  1.1295e-04, -1.5672e-04, -4.9103e-05, -1.2073e-03,\n             2.4927e-05,  1.6710e-04, -1.3383e-05,  1.1820e-05,  1.0517e-04,\n             3.4809e-05, -4.1197e-05, -8.5326e-05, -3.1680e-04, -6.7009e-07,\n             1.5599e-05, -3.2427e-04,  1.6585e-05,  3.1371e-04,  1.2755e-04,\n             7.5162e-06,  4.0920e-05,  2.7909e-04, -3.7830e-05, -2.9450e-05,\n             3.5245e-05, -1.9501e-05,  3.4536e-05,  1.2153e-04, -5.5085e-04,\n            -1.4609e-05,  3.4361e-04, -4.1158e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([2.5804e-06, 8.8954e-07, 4.5556e-07, 7.2452e-08, 1.1434e-07, 1.3013e-07,\n            3.3886e-08, 2.2254e-07, 1.4957e-05, 4.0542e-09, 7.9167e-07, 6.7603e-08,\n            7.9934e-06, 4.2136e-06, 3.2189e-07, 2.9273e-08, 4.2760e-09, 2.4593e-08,\n            1.1630e-07, 2.1809e-08, 6.7778e-09, 1.2180e-08, 9.8370e-07, 2.2076e-08,\n            9.9493e-10, 1.6934e-08, 4.6908e-06, 2.0833e-07, 7.1739e-07, 7.7248e-07,\n            8.4657e-07, 1.0441e-07, 5.5328e-08, 4.2202e-08, 9.1439e-08, 1.0840e-07,\n            4.1559e-08, 4.9485e-08, 6.6672e-07, 8.0285e-07, 4.2964e-07, 5.8438e-08,\n            2.6503e-06, 7.4173e-08, 6.2645e-07, 5.0552e-06, 4.6347e-07, 1.8097e-07,\n            7.9578e-09, 5.7022e-06, 2.3457e-06, 4.8836e-07, 1.0424e-06, 2.9023e-09,\n            1.3351e-06, 1.7964e-08, 2.5482e-05, 1.3212e-08, 2.2110e-08, 8.1466e-07,\n            5.9048e-08, 5.9974e-06, 5.8533e-07, 2.9998e-06, 6.0454e-06, 8.1446e-06,\n            1.0277e-07, 8.5431e-06, 1.1564e-08, 1.0557e-08, 4.5709e-07, 3.0075e-07,\n            8.9210e-07, 6.7534e-09, 1.1792e-05, 7.9763e-08, 7.7236e-08, 6.3994e-09,\n            3.5632e-08, 6.1067e-08, 2.9654e-07, 4.4139e-07, 3.3963e-07, 6.1028e-08,\n            1.3827e-06, 1.0572e-05, 3.0961e-08, 1.8885e-09, 5.2273e-06, 2.3219e-07,\n            2.7270e-07, 5.4066e-08, 1.1259e-07, 1.3693e-07, 8.7694e-08, 1.2476e-07,\n            3.3818e-07, 9.4253e-07, 3.1975e-07, 1.1216e-06, 4.3189e-07, 2.2424e-07,\n            2.6354e-06, 1.6759e-07, 1.4476e-06, 2.4973e-08, 2.4847e-08, 1.3033e-07,\n            8.2009e-07, 2.1969e-08, 1.0831e-08, 6.1037e-07, 1.7282e-09, 5.9469e-07,\n            2.7422e-07, 4.8145e-07, 7.9390e-09, 5.0385e-06, 2.4794e-08, 1.4479e-07,\n            2.1671e-07, 7.3189e-09, 7.7668e-09, 3.1939e-07, 2.7605e-06, 3.0541e-08,\n            2.0022e-05, 3.7172e-08], device='cuda:0')},\n   17: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[ 8.4204e-05,  1.3616e-04,  3.0055e-04],\n              [ 3.2837e-04,  2.7173e-05, -1.4001e-04],\n              [-9.6268e-05,  2.8832e-04,  2.4445e-04]]],\n    \n    \n            [[[ 3.4394e-04, -4.5437e-04,  7.3943e-06],\n              [-2.4586e-04,  2.7471e-04, -1.3908e-04],\n              [-7.7866e-04,  3.4331e-04, -6.3953e-05]]],\n    \n    \n            [[[-3.3855e-04, -5.6181e-05, -1.1101e-03],\n              [-5.4373e-04, -3.3585e-04, -7.2566e-04],\n              [-3.6364e-04, -6.5089e-04, -9.4999e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[-2.0987e-04,  1.5013e-04, -2.8278e-04],\n              [-2.5301e-04,  2.7872e-04, -2.5805e-04],\n              [-3.7321e-04,  4.0455e-04,  3.8344e-05]]],\n    \n    \n            [[[-4.7504e-04, -1.7386e-04, -2.6053e-04],\n              [-3.8579e-04, -1.0570e-04, -6.6793e-05],\n              [-1.4498e-04, -1.6452e-04, -3.4057e-05]]],\n    \n    \n            [[[ 8.1608e-05,  9.0117e-05, -1.9900e-05],\n              [ 2.0402e-04,  1.9394e-04,  2.0590e-05],\n              [-1.1379e-04, -3.3292e-05,  1.0808e-04]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[2.7177e-06, 2.4712e-06, 1.6472e-06],\n              [4.7465e-06, 1.7162e-06, 1.3190e-06],\n              [2.0349e-06, 2.2534e-06, 2.0245e-06]]],\n    \n    \n            [[[1.3628e-06, 1.7557e-06, 3.6112e-06],\n              [2.5720e-06, 2.9798e-06, 1.1885e-05],\n              [9.6907e-06, 1.1206e-05, 5.4080e-06]]],\n    \n    \n            [[[3.7589e-06, 7.5342e-06, 1.0319e-05],\n              [2.2818e-06, 3.4462e-06, 6.9019e-06],\n              [2.3834e-06, 3.3320e-06, 8.1958e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[1.8997e-06, 2.4584e-06, 1.1578e-06],\n              [1.2804e-06, 2.7178e-06, 1.2478e-06],\n              [1.3011e-06, 2.1877e-06, 1.3547e-06]]],\n    \n    \n            [[[5.9100e-06, 4.7204e-06, 4.3914e-06],\n              [4.1430e-06, 4.2358e-06, 3.7930e-06],\n              [4.4966e-06, 4.5274e-06, 3.6071e-06]]],\n    \n    \n            [[[1.0637e-06, 1.8966e-06, 1.5574e-06],\n              [1.5351e-06, 9.1496e-07, 8.0501e-07],\n              [6.4299e-07, 5.7465e-07, 5.8110e-07]]]], device='cuda:0')},\n   18: {'step': tensor(10550.),\n    'exp_avg': tensor([ 3.9418e-04, -6.2704e-05,  2.8363e-04, -5.2643e-04,  9.2211e-05,\n             5.3540e-05,  1.6692e-04, -8.1142e-05,  9.7165e-04, -7.7314e-05,\n             2.1518e-04, -4.9429e-05,  1.0506e-03,  2.0836e-04, -1.7721e-05,\n            -1.4317e-04, -1.1894e-04,  3.6213e-05,  3.4227e-04,  1.9755e-04,\n             6.9835e-05,  1.7369e-04,  3.2235e-04,  7.1065e-05,  4.0498e-05,\n            -1.9506e-06, -2.3156e-04, -1.8017e-04,  7.6549e-04, -3.7851e-04,\n            -7.2548e-04, -4.9630e-04, -2.2998e-04, -1.9680e-05,  1.9810e-04,\n             1.0840e-04,  7.2298e-04,  4.8553e-05,  1.7863e-05, -5.5941e-04,\n             8.8878e-05, -4.5982e-05,  4.4063e-04, -4.7193e-04, -2.7081e-04,\n             3.8999e-04, -1.0721e-04,  2.9881e-04,  1.5376e-04,  1.1711e-03,\n            -3.9267e-04,  2.1359e-04,  8.3479e-04, -5.4738e-05, -1.1996e-03,\n            -9.1925e-05,  1.8306e-03,  3.0645e-04,  9.7407e-06, -4.2009e-05,\n            -1.4220e-04, -6.9437e-04,  4.0206e-04,  9.3857e-05,  1.8793e-04,\n             6.3776e-04,  4.1152e-04, -1.9130e-03, -2.1262e-04,  2.3916e-05,\n             2.3049e-04,  3.6541e-04, -2.0256e-04,  2.2531e-04,  2.5119e-04,\n            -6.2476e-04,  1.5730e-04,  6.7689e-04,  9.4720e-05,  7.4462e-04,\n            -9.2239e-05, -4.0653e-05, -1.8137e-04,  3.0382e-04, -2.9792e-04,\n             3.6763e-04, -9.9968e-05,  1.0206e-04, -9.6898e-04,  3.5728e-04,\n            -8.4585e-05, -4.7073e-05, -1.8334e-04, -3.4237e-04,  1.2398e-04,\n            -3.5828e-04,  2.4068e-04,  1.4927e-04,  1.5290e-04, -1.1900e-03,\n            -3.6555e-04,  6.7561e-04, -1.3223e-05, -3.6436e-05,  1.1456e-04,\n             8.0046e-05,  9.1410e-05, -1.6084e-04,  3.8041e-04,  4.4696e-06,\n            -1.2468e-04,  4.1788e-04,  7.6571e-05, -3.8675e-04, -1.9375e-04,\n             6.2084e-05, -3.5612e-04,  1.3961e-04,  2.6744e-04,  5.3883e-05,\n             4.9774e-05, -7.8709e-05, -1.9079e-04, -2.0628e-04, -4.9598e-04,\n            -9.4914e-05,  2.3762e-04, -2.7338e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([2.4687e-06, 1.5189e-06, 1.5376e-06, 1.3027e-06, 5.4270e-07, 1.1724e-06,\n            6.2946e-07, 1.5760e-06, 4.7153e-06, 2.1351e-07, 1.5985e-06, 1.0081e-06,\n            5.8820e-06, 3.1961e-06, 1.8232e-06, 2.7638e-07, 6.9132e-07, 4.4274e-07,\n            6.8593e-07, 1.0178e-06, 4.0560e-07, 9.6159e-07, 1.9097e-06, 1.1958e-06,\n            2.1407e-07, 1.7344e-07, 2.7346e-06, 1.0567e-06, 2.5982e-06, 1.8346e-06,\n            5.4192e-06, 1.5560e-06, 8.9060e-07, 1.7301e-06, 5.2770e-07, 3.1966e-07,\n            2.0003e-06, 3.9058e-07, 8.2860e-07, 3.0760e-06, 2.0628e-06, 1.1479e-06,\n            4.6488e-06, 1.1912e-06, 8.9438e-07, 5.7040e-06, 6.5533e-07, 5.0661e-07,\n            5.1421e-07, 3.0229e-06, 1.2942e-06, 1.6460e-06, 9.8060e-07, 1.5013e-07,\n            1.6990e-06, 5.2722e-07, 9.8901e-06, 7.5735e-07, 2.1042e-07, 1.1731e-06,\n            5.5842e-07, 2.9991e-06, 1.9760e-06, 2.7620e-06, 3.0054e-06, 4.1264e-06,\n            2.6520e-06, 5.7870e-06, 5.2671e-07, 6.2942e-07, 8.5090e-07, 9.0917e-07,\n            1.0707e-06, 7.7776e-07, 5.0479e-06, 1.1838e-06, 8.4504e-07, 1.0691e-06,\n            8.8570e-07, 2.5847e-06, 6.3363e-07, 3.5355e-06, 1.1886e-06, 1.5914e-06,\n            1.7520e-06, 3.8253e-06, 5.1455e-07, 1.2930e-07, 3.2803e-06, 7.8919e-07,\n            1.5873e-06, 7.0500e-07, 7.4313e-07, 9.4683e-07, 4.4600e-07, 8.5870e-07,\n            8.8938e-07, 1.1300e-06, 3.2528e-06, 1.4178e-06, 4.3587e-06, 2.0782e-06,\n            1.8311e-06, 7.8433e-07, 1.3156e-06, 3.1039e-07, 2.7596e-07, 7.9574e-07,\n            9.1598e-07, 4.1116e-07, 4.0491e-07, 1.1986e-06, 4.6306e-07, 1.1800e-06,\n            5.8303e-07, 3.1121e-06, 3.1299e-07, 3.3417e-06, 4.8231e-07, 1.2938e-06,\n            1.2796e-06, 1.4813e-06, 5.2794e-07, 6.5128e-07, 2.4080e-06, 4.2905e-07,\n            9.0909e-06, 4.3657e-07], device='cuda:0')},\n   19: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-1.7272e-04]],\n    \n             [[-1.5882e-04]],\n    \n             [[-2.9446e-04]],\n    \n             ...,\n    \n             [[ 3.9500e-04]],\n    \n             [[-1.7330e-04]],\n    \n             [[-8.5565e-05]]],\n    \n    \n            [[[-1.9852e-04]],\n    \n             [[-4.3154e-05]],\n    \n             [[ 6.3473e-05]],\n    \n             ...,\n    \n             [[ 7.8961e-05]],\n    \n             [[ 5.1781e-06]],\n    \n             [[ 4.8870e-05]]],\n    \n    \n            [[[-2.2418e-04]],\n    \n             [[ 2.6819e-04]],\n    \n             [[-1.6748e-04]],\n    \n             ...,\n    \n             [[ 2.1975e-04]],\n    \n             [[-2.0234e-04]],\n    \n             [[-1.0237e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[-1.2369e-04]],\n    \n             [[ 1.3325e-04]],\n    \n             [[-4.0394e-06]],\n    \n             ...,\n    \n             [[ 1.1217e-04]],\n    \n             [[-4.1281e-05]],\n    \n             [[ 3.5573e-05]]],\n    \n    \n            [[[ 1.6381e-04]],\n    \n             [[-2.2648e-04]],\n    \n             [[-6.4676e-05]],\n    \n             ...,\n    \n             [[ 4.9935e-06]],\n    \n             [[-6.7107e-05]],\n    \n             [[-4.0413e-05]]],\n    \n    \n            [[[ 1.3162e-04]],\n    \n             [[-8.8618e-05]],\n    \n             [[-9.8539e-05]],\n    \n             ...,\n    \n             [[-1.2264e-05]],\n    \n             [[ 2.4342e-04]],\n    \n             [[ 8.6132e-05]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[5.8631e-07]],\n    \n             [[7.5482e-07]],\n    \n             [[1.1663e-06]],\n    \n             ...,\n    \n             [[5.1412e-07]],\n    \n             [[1.0360e-06]],\n    \n             [[3.0803e-07]]],\n    \n    \n            [[[2.5882e-07]],\n    \n             [[4.3534e-07]],\n    \n             [[6.9205e-07]],\n    \n             ...,\n    \n             [[2.4831e-07]],\n    \n             [[6.0353e-07]],\n    \n             [[2.0884e-07]]],\n    \n    \n            [[[4.8031e-07]],\n    \n             [[6.2698e-07]],\n    \n             [[1.0267e-06]],\n    \n             ...,\n    \n             [[3.9571e-07]],\n    \n             [[9.3043e-07]],\n    \n             [[3.2848e-07]]],\n    \n    \n            ...,\n    \n    \n            [[[2.8856e-07]],\n    \n             [[5.7382e-07]],\n    \n             [[7.6089e-07]],\n    \n             ...,\n    \n             [[2.7116e-07]],\n    \n             [[7.2315e-07]],\n    \n             [[2.6168e-07]]],\n    \n    \n            [[[3.3452e-07]],\n    \n             [[4.8152e-07]],\n    \n             [[7.1575e-07]],\n    \n             ...,\n    \n             [[3.0977e-07]],\n    \n             [[6.6988e-07]],\n    \n             [[2.4533e-07]]],\n    \n    \n            [[[2.9093e-07]],\n    \n             [[4.6721e-07]],\n    \n             [[9.1192e-07]],\n    \n             ...,\n    \n             [[2.4259e-07]],\n    \n             [[6.8480e-07]],\n    \n             [[2.2596e-07]]]], device='cuda:0')},\n   20: {'step': tensor(10550.),\n    'exp_avg': tensor([ 4.2803e-05,  2.5282e-05,  5.3343e-04,  3.1905e-04,  1.1495e-04,\n            -7.4877e-05, -5.3973e-04, -3.3469e-04, -2.4292e-04,  1.2171e-04,\n             6.9271e-04,  1.6763e-04, -4.4733e-04, -4.3238e-05,  3.0434e-04,\n             3.1865e-04, -2.1327e-04, -3.1238e-04,  1.3931e-04,  3.0784e-04,\n            -2.6329e-05, -3.5149e-04,  3.7831e-04,  3.5558e-04,  3.0961e-04,\n             4.7300e-05, -1.0627e-04, -5.9381e-05, -2.8115e-04,  7.4931e-05,\n            -9.3044e-05,  9.6029e-05, -2.1415e-04, -4.9351e-04, -1.0255e-04,\n            -3.0671e-05, -3.9215e-04, -7.9642e-05,  1.0299e-04,  4.8821e-04,\n            -1.1342e-04, -4.9766e-04,  4.8058e-04,  1.4596e-04,  1.6818e-04,\n            -4.0012e-05, -2.5736e-04, -6.8536e-05, -1.3633e-04,  2.5816e-04,\n             2.0874e-04, -2.5896e-04,  1.2681e-04, -1.6250e-04,  1.6305e-04,\n             3.8054e-04, -1.0058e-04,  1.6126e-04,  2.3531e-04, -2.8734e-04,\n            -6.4718e-05, -1.6578e-04, -1.8519e-04, -3.9221e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([2.0327e-06, 1.3271e-06, 1.8155e-06, 2.3391e-06, 2.0255e-06, 1.5469e-06,\n            2.3531e-06, 1.7979e-06, 2.0702e-06, 1.7638e-06, 1.6330e-06, 1.4826e-06,\n            1.5303e-06, 1.5188e-06, 1.7513e-06, 2.4401e-06, 1.7030e-06, 1.5601e-06,\n            1.8501e-06, 1.5905e-06, 1.7620e-06, 2.1494e-06, 2.0054e-06, 1.7877e-06,\n            2.0359e-06, 1.4865e-06, 1.2747e-06, 1.8104e-06, 1.5334e-06, 1.6175e-06,\n            1.8752e-06, 2.2515e-06, 1.8602e-06, 1.6224e-06, 1.3164e-06, 2.2033e-06,\n            1.8494e-06, 1.4834e-06, 1.8164e-06, 1.5694e-06, 1.6250e-06, 1.6089e-06,\n            1.8558e-06, 2.1636e-06, 2.4754e-06, 2.2312e-06, 1.5702e-06, 1.7750e-06,\n            1.4045e-06, 1.7687e-06, 1.8226e-06, 1.6190e-06, 1.0249e-06, 1.0162e-06,\n            1.8528e-06, 1.5931e-06, 1.6272e-06, 2.1196e-06, 1.4802e-06, 1.9092e-06,\n            1.7330e-06, 1.7725e-06, 1.4464e-06, 1.5325e-06], device='cuda:0')},\n   21: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[ 2.4187e-05, -8.3125e-06, -3.7317e-06],\n              [-2.9120e-07, -3.1991e-05,  2.0585e-05],\n              [ 4.2305e-05, -5.4230e-06,  1.2807e-05]],\n    \n             [[ 9.8127e-07,  7.5759e-06,  3.8988e-06],\n              [-1.3283e-05,  5.3406e-06, -1.1383e-05],\n              [ 2.5117e-05,  2.1237e-05,  1.4692e-05]],\n    \n             [[ 9.8193e-06,  4.6658e-06,  4.9855e-06],\n              [ 1.4733e-05, -1.1631e-05,  4.7502e-06],\n              [ 1.3355e-05, -6.3230e-06,  3.6214e-06]],\n    \n             ...,\n    \n             [[-6.1698e-06,  1.6355e-05, -3.3607e-06],\n              [-1.4119e-05,  2.6247e-06, -1.9358e-05],\n              [ 1.6991e-05,  7.5495e-06, -9.6383e-06]],\n    \n             [[ 1.3346e-05,  5.7089e-06, -2.6788e-07],\n              [ 2.4795e-05,  6.0433e-06,  8.9298e-06],\n              [-3.3449e-06,  4.1729e-06,  5.6328e-06]],\n    \n             [[ 1.3086e-05, -8.6731e-06,  1.6335e-05],\n              [-2.6612e-05,  3.8685e-05, -1.2514e-05],\n              [ 2.0090e-05,  2.5866e-05, -3.4505e-06]]],\n    \n    \n            [[[-1.4703e-05, -1.7080e-05, -1.5190e-05],\n              [-5.7945e-05, -1.6689e-05, -1.6423e-05],\n              [-2.3154e-05, -2.6091e-05,  4.2278e-06]],\n    \n             [[-1.4783e-05, -1.9489e-05,  1.9721e-06],\n              [ 8.2211e-06, -1.7287e-05, -7.6922e-06],\n              [-1.2652e-05,  2.0449e-05, -2.2620e-05]],\n    \n             [[-5.5141e-05, -2.9889e-05, -2.4071e-05],\n              [-7.3175e-06,  5.0523e-06, -2.1614e-05],\n              [ 1.2831e-05, -2.5160e-05, -1.4560e-05]],\n    \n             ...,\n    \n             [[ 8.9811e-06, -5.6146e-06,  4.8954e-06],\n              [ 2.0986e-05, -1.3883e-05,  6.1509e-07],\n              [ 8.6273e-06,  2.2165e-05, -1.2189e-05]],\n    \n             [[ 2.0093e-05,  2.2445e-05, -7.2714e-06],\n              [-3.8006e-05,  2.7478e-05, -3.4074e-06],\n              [ 7.0255e-07, -1.1741e-06,  1.6889e-05]],\n    \n             [[ 4.6589e-06,  7.7207e-06, -2.3410e-06],\n              [ 2.4109e-05, -4.1461e-05,  1.0261e-05],\n              [ 3.0604e-05,  2.5872e-05,  6.2642e-06]]],\n    \n    \n            [[[-2.9941e-05,  1.1715e-05,  4.6084e-07],\n              [ 1.2984e-05,  2.5200e-05, -2.6394e-05],\n              [-3.3976e-05,  5.5795e-07, -1.1941e-05]],\n    \n             [[-1.9615e-05, -2.7984e-05, -7.3376e-06],\n              [-7.0492e-07, -2.1777e-05, -5.1395e-06],\n              [-4.4163e-05, -4.4779e-05, -6.5573e-05]],\n    \n             [[-4.5093e-06, -2.3793e-07, -8.9872e-06],\n              [-1.7488e-05,  6.7266e-06, -1.2710e-05],\n              [-1.2926e-05,  1.3336e-05,  4.7375e-06]],\n    \n             ...,\n    \n             [[-9.0044e-07, -1.2478e-05,  1.1814e-05],\n              [ 1.6369e-05, -1.5219e-06,  2.6765e-05],\n              [-1.9181e-05, -2.6007e-05,  5.7890e-06]],\n    \n             [[-1.3172e-05, -7.6137e-06, -3.7472e-06],\n              [-1.7919e-05,  1.6437e-06, -8.4552e-06],\n              [ 1.1896e-05, -1.9856e-06,  1.0178e-07]],\n    \n             [[-3.1643e-05, -4.5846e-06, -1.2675e-05],\n              [ 1.5753e-05, -6.0485e-05,  3.7135e-06],\n              [-3.9340e-05, -5.3367e-05, -1.1964e-05]]],\n    \n    \n            ...,\n    \n    \n            [[[-3.2509e-05, -2.0042e-05, -1.7690e-05],\n              [-8.1322e-06,  2.6417e-06, -1.2231e-05],\n              [-7.9775e-05, -1.2517e-05, -6.1918e-05]],\n    \n             [[ 3.0363e-05, -1.6638e-05,  5.0449e-06],\n              [-5.5497e-05, -1.7642e-05, -4.1435e-05],\n              [-6.4002e-05, -1.9804e-05, -4.5408e-05]],\n    \n             [[-3.2325e-05,  2.0172e-05, -1.7406e-05],\n              [-7.8321e-06, -3.2238e-05, -1.0973e-05],\n              [-4.8848e-05, -3.4974e-06, -4.5140e-05]],\n    \n             ...,\n    \n             [[ 2.9693e-05, -1.3280e-05,  6.5138e-06],\n              [-9.9343e-06, -3.1089e-05, -3.8694e-05],\n              [-1.1825e-05, -6.6633e-05, -4.9630e-06]],\n    \n             [[-7.0554e-06, -4.1517e-06, -9.4688e-06],\n              [ 1.0653e-05, -6.6882e-06,  9.5181e-06],\n              [ 2.0900e-05,  2.6970e-05,  1.5099e-05]],\n    \n             [[ 2.0212e-05,  3.9094e-06,  2.2476e-05],\n              [-2.0641e-05,  3.3601e-07, -2.5543e-05],\n              [-3.0648e-05, -1.7672e-05, -2.4606e-05]]],\n    \n    \n            [[[-1.0546e-05,  5.6130e-07, -1.9411e-05],\n              [-2.8841e-05, -3.0476e-05, -8.7095e-06],\n              [ 2.4038e-05, -8.2924e-06,  4.1524e-07]],\n    \n             [[-1.2646e-05,  1.6259e-05,  2.4946e-07],\n              [-5.0181e-05,  1.0653e-05, -3.7441e-05],\n              [-3.2337e-05,  2.4720e-05, -1.4770e-05]],\n    \n             [[-5.1818e-06,  6.2633e-06, -1.5917e-05],\n              [-3.5001e-06, -1.0779e-05, -9.9034e-06],\n              [ 1.1909e-05,  2.9824e-06, -1.3437e-06]],\n    \n             ...,\n    \n             [[-4.3226e-06,  9.2324e-06,  2.4222e-07],\n              [-4.7933e-06,  6.9247e-06, -6.3708e-06],\n              [-6.2494e-06, -6.9535e-06,  6.0690e-06]],\n    \n             [[ 2.6706e-05,  2.6172e-06,  1.8949e-05],\n              [ 2.5317e-05, -5.0014e-07,  1.2179e-05],\n              [ 1.7799e-05,  5.6194e-06,  1.1812e-05]],\n    \n             [[ 5.2146e-06,  1.0110e-06,  3.2584e-05],\n              [-2.2000e-05,  2.8463e-05, -8.4117e-06],\n              [ 1.0575e-05,  3.5514e-06,  7.1820e-06]]],\n    \n    \n            [[[-6.0461e-05, -4.6881e-05, -3.1382e-05],\n              [-5.7786e-05, -1.8382e-05, -9.2442e-05],\n              [-7.6551e-05, -4.9394e-05, -4.8333e-05]],\n    \n             [[-9.7830e-05, -6.3295e-05, -5.1921e-05],\n              [-4.8121e-05, -6.5213e-05, -5.7394e-05],\n              [-9.1175e-05, -6.5106e-05, -1.2582e-04]],\n    \n             [[-5.7619e-05, -1.0436e-05, -2.2482e-05],\n              [-4.9707e-05, -3.0262e-05, -6.7008e-05],\n              [-6.2817e-05, -1.1112e-05, -3.2457e-05]],\n    \n             ...,\n    \n             [[-2.8208e-06, -2.6153e-05, -5.1471e-06],\n              [ 1.7410e-05, -1.5192e-05,  1.6843e-05],\n              [-3.0942e-05, -6.8515e-05,  1.4980e-05]],\n    \n             [[ 3.0107e-05,  1.9098e-05,  5.6920e-06],\n              [ 3.1080e-05,  3.0158e-05,  7.7259e-06],\n              [ 3.5510e-05,  2.7835e-05,  1.9747e-05]],\n    \n             [[-4.8364e-05, -1.0478e-05, -9.7236e-06],\n              [-1.9484e-06, -8.5956e-05, -1.1569e-05],\n              [-5.6524e-05, -8.2144e-05, -4.1311e-05]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[5.6069e-09, 5.7180e-09, 6.3754e-09],\n              [5.4427e-09, 6.0917e-09, 6.3586e-09],\n              [5.8602e-09, 5.8307e-09, 5.3357e-09]],\n    \n             [[5.0661e-09, 5.5417e-09, 5.0503e-09],\n              [4.6183e-09, 4.9165e-09, 4.9379e-09],\n              [5.8718e-09, 5.0352e-09, 5.0010e-09]],\n    \n             [[6.0027e-09, 4.8827e-09, 4.9273e-09],\n              [5.0749e-09, 4.0362e-09, 4.3923e-09],\n              [4.8167e-09, 3.7541e-09, 4.7918e-09]],\n    \n             ...,\n    \n             [[4.4462e-09, 5.1086e-09, 4.4162e-09],\n              [4.7335e-09, 4.8254e-09, 5.4238e-09],\n              [4.8115e-09, 4.2747e-09, 5.0447e-09]],\n    \n             [[4.1067e-09, 3.6242e-09, 3.9487e-09],\n              [4.1820e-09, 3.6686e-09, 3.7792e-09],\n              [4.0804e-09, 3.8852e-09, 3.7933e-09]],\n    \n             [[6.4846e-09, 6.2716e-09, 5.8782e-09],\n              [5.9046e-09, 6.7002e-09, 5.9799e-09],\n              [6.2107e-09, 6.3684e-09, 6.2140e-09]]],\n    \n    \n            [[[1.4197e-08, 1.6350e-08, 1.4775e-08],\n              [1.5034e-08, 1.5160e-08, 1.5460e-08],\n              [1.3816e-08, 1.5097e-08, 1.5163e-08]],\n    \n             [[1.0955e-08, 1.1326e-08, 1.1440e-08],\n              [1.1487e-08, 1.1817e-08, 1.1575e-08],\n              [1.0917e-08, 1.3046e-08, 1.2365e-08]],\n    \n             [[1.1504e-08, 1.1203e-08, 1.2132e-08],\n              [1.0434e-08, 1.1699e-08, 1.1027e-08],\n              [1.2028e-08, 1.1125e-08, 1.2941e-08]],\n    \n             ...,\n    \n             [[1.1640e-08, 9.6676e-09, 1.1225e-08],\n              [1.0170e-08, 1.0318e-08, 1.0736e-08],\n              [1.0289e-08, 9.5060e-09, 1.0153e-08]],\n    \n             [[9.4249e-09, 8.8675e-09, 1.0112e-08],\n              [1.0154e-08, 1.0720e-08, 1.1342e-08],\n              [1.0499e-08, 1.0531e-08, 1.0923e-08]],\n    \n             [[1.2905e-08, 1.3307e-08, 1.3387e-08],\n              [1.3901e-08, 1.4705e-08, 1.2984e-08],\n              [1.4194e-08, 1.6020e-08, 1.3648e-08]]],\n    \n    \n            [[[7.7095e-09, 7.8836e-09, 8.6814e-09],\n              [7.4139e-09, 8.1085e-09, 8.5821e-09],\n              [8.5162e-09, 8.3934e-09, 7.6147e-09]],\n    \n             [[6.8991e-09, 8.3999e-09, 7.1579e-09],\n              [7.1765e-09, 7.5167e-09, 7.5846e-09],\n              [9.1502e-09, 8.3378e-09, 7.8424e-09]],\n    \n             [[7.8468e-09, 6.7340e-09, 7.1898e-09],\n              [6.9204e-09, 5.3903e-09, 6.0088e-09],\n              [6.1604e-09, 5.7676e-09, 6.5192e-09]],\n    \n             ...,\n    \n             [[5.8140e-09, 6.3204e-09, 6.3310e-09],\n              [6.8368e-09, 7.2210e-09, 8.2735e-09],\n              [6.8390e-09, 6.7502e-09, 7.1986e-09]],\n    \n             [[5.7742e-09, 5.0760e-09, 5.6957e-09],\n              [5.8428e-09, 5.7314e-09, 5.2812e-09],\n              [6.4221e-09, 5.8501e-09, 6.3120e-09]],\n    \n             [[8.5605e-09, 8.5399e-09, 8.4345e-09],\n              [8.8955e-09, 9.6327e-09, 8.9909e-09],\n              [8.9569e-09, 9.2661e-09, 9.5124e-09]]],\n    \n    \n            ...,\n    \n    \n            [[[2.6432e-08, 2.3927e-08, 2.8154e-08],\n              [2.3376e-08, 2.5355e-08, 1.9886e-08],\n              [2.5781e-08, 2.2199e-08, 2.4622e-08]],\n    \n             [[1.9736e-08, 2.2361e-08, 1.9039e-08],\n              [2.0909e-08, 1.9196e-08, 2.0211e-08],\n              [2.1072e-08, 2.0105e-08, 1.7329e-08]],\n    \n             [[2.5820e-08, 1.9440e-08, 2.1186e-08],\n              [2.1058e-08, 2.0727e-08, 1.8129e-08],\n              [2.4395e-08, 1.9001e-08, 2.3223e-08]],\n    \n             ...,\n    \n             [[1.7950e-08, 1.7142e-08, 1.5064e-08],\n              [1.9573e-08, 1.8390e-08, 2.0603e-08],\n              [1.8972e-08, 1.5169e-08, 1.6293e-08]],\n    \n             [[1.6624e-08, 1.4639e-08, 1.5450e-08],\n              [1.6951e-08, 1.4345e-08, 1.2998e-08],\n              [1.6713e-08, 1.4151e-08, 1.3540e-08]],\n    \n             [[2.0654e-08, 2.2912e-08, 1.9362e-08],\n              [2.3878e-08, 2.0856e-08, 2.1569e-08],\n              [2.0866e-08, 2.2323e-08, 1.7982e-08]]],\n    \n    \n            [[[7.4634e-09, 7.7445e-09, 8.7938e-09],\n              [7.7519e-09, 8.3334e-09, 7.2990e-09],\n              [6.5412e-09, 7.5494e-09, 7.1961e-09]],\n    \n             [[6.0367e-09, 5.8878e-09, 5.3687e-09],\n              [5.1558e-09, 4.4395e-09, 5.0598e-09],\n              [5.7836e-09, 6.1222e-09, 5.6325e-09]],\n    \n             [[7.7404e-09, 7.9552e-09, 6.7367e-09],\n              [7.7569e-09, 6.8879e-09, 6.5815e-09],\n              [6.8860e-09, 5.2701e-09, 6.0902e-09]],\n    \n             ...,\n    \n             [[5.7624e-09, 6.0570e-09, 5.8821e-09],\n              [5.3785e-09, 5.8253e-09, 5.6426e-09],\n              [5.6839e-09, 5.3027e-09, 5.6177e-09]],\n    \n             [[4.7399e-09, 4.5963e-09, 5.2296e-09],\n              [4.5653e-09, 4.5502e-09, 4.5401e-09],\n              [5.1804e-09, 5.0401e-09, 4.9941e-09]],\n    \n             [[7.6489e-09, 7.2998e-09, 7.3757e-09],\n              [6.8331e-09, 7.8509e-09, 8.2380e-09],\n              [7.9798e-09, 8.0702e-09, 7.5181e-09]]],\n    \n    \n            [[[2.1508e-08, 2.1487e-08, 1.9639e-08],\n              [2.6071e-08, 2.1088e-08, 2.6638e-08],\n              [2.3492e-08, 2.5233e-08, 2.1127e-08]],\n    \n             [[1.8192e-08, 2.0910e-08, 1.9236e-08],\n              [2.0850e-08, 2.0997e-08, 1.7552e-08],\n              [2.6068e-08, 2.3305e-08, 2.1927e-08]],\n    \n             [[2.0685e-08, 1.9692e-08, 1.8127e-08],\n              [2.0990e-08, 2.2086e-08, 1.9275e-08],\n              [2.1488e-08, 2.1903e-08, 1.9681e-08]],\n    \n             ...,\n    \n             [[1.5340e-08, 1.6874e-08, 1.6778e-08],\n              [1.6172e-08, 1.6245e-08, 1.7738e-08],\n              [1.9936e-08, 1.5982e-08, 1.6843e-08]],\n    \n             [[1.4085e-08, 1.3146e-08, 1.2720e-08],\n              [1.7712e-08, 1.5804e-08, 1.6234e-08],\n              [1.7546e-08, 1.5833e-08, 1.6374e-08]],\n    \n             [[1.9850e-08, 1.8824e-08, 2.1229e-08],\n              [2.2816e-08, 2.2910e-08, 2.2251e-08],\n              [2.2691e-08, 2.7776e-08, 2.6198e-08]]]], device='cuda:0')},\n   22: {'step': tensor(10550.),\n    'exp_avg': tensor([ 1.0744e-04,  1.0850e-04,  1.4056e-04, -9.6895e-05, -2.1236e-05,\n             9.1962e-05, -6.3227e-05, -2.9239e-04, -4.1559e-05, -1.4001e-05,\n             1.9785e-04,  2.6127e-04,  1.2348e-05, -3.4860e-04, -2.3827e-06,\n            -6.4589e-04, -9.6827e-05, -6.8636e-04,  1.9520e-04,  1.1294e-04,\n             3.2091e-04, -1.5966e-04,  1.4364e-04,  1.2482e-04,  1.9827e-04,\n            -2.3753e-04, -1.0559e-04,  4.3507e-05,  3.2129e-04, -6.8408e-04,\n            -6.8420e-06, -1.0999e-04,  2.5867e-05, -6.2080e-04,  1.1596e-04,\n            -8.1725e-05, -3.1789e-05,  5.2867e-05,  5.4581e-05, -3.5011e-05,\n            -1.4348e-04, -4.7316e-04,  2.3094e-04, -6.0119e-05,  6.3970e-05,\n            -1.4855e-04,  9.9105e-05, -1.9973e-04,  2.4293e-04,  1.8582e-04,\n             1.4969e-04, -1.9843e-04, -4.5372e-06,  5.4068e-05, -3.8140e-04,\n            -2.1753e-05,  6.7754e-05,  4.3267e-05,  1.2188e-04,  3.1010e-05,\n             2.1582e-06, -1.2126e-04, -1.9215e-04,  1.1357e-04,  1.2440e-04,\n            -7.7644e-05,  4.3320e-05, -2.2674e-04,  2.1277e-04,  4.7127e-06,\n            -1.2808e-05, -1.2006e-04,  1.8671e-05,  1.7336e-04,  2.8695e-05,\n            -1.1430e-04,  8.5679e-05, -3.8217e-05, -8.1370e-05, -1.5339e-04,\n             2.8818e-04,  2.7099e-04, -5.1302e-04,  7.8592e-05,  1.4708e-04,\n            -1.9208e-05, -5.3125e-06,  1.3897e-05,  1.1417e-04, -1.6779e-04,\n            -1.5025e-04,  2.4683e-04,  4.2758e-05,  3.0141e-04, -4.2616e-05,\n            -1.7588e-04,  7.3187e-05,  1.6296e-04, -3.2482e-05,  1.7248e-04,\n             1.1417e-04, -4.6051e-05,  1.9264e-04,  2.5980e-04, -2.2617e-04,\n             9.4281e-05,  9.0811e-05, -9.2550e-05, -2.6868e-04, -2.4626e-05,\n             2.0337e-04,  2.6499e-04,  5.9127e-05, -3.6053e-04, -8.4494e-05,\n            -4.8809e-04,  8.3192e-06, -1.0273e-04, -1.7120e-04, -1.1372e-04,\n            -2.4084e-04,  2.4295e-05,  3.1735e-05, -1.1321e-04,  2.3015e-05,\n            -8.2328e-05, -7.2367e-05, -5.9985e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([2.6028e-07, 2.6615e-07, 3.2619e-07, 2.1845e-07, 3.3115e-07, 2.5599e-07,\n            3.4778e-07, 4.0821e-07, 2.7495e-07, 2.5308e-07, 2.5455e-07, 3.4902e-07,\n            1.8851e-07, 4.4972e-07, 3.4989e-07, 3.6474e-07, 3.2553e-07, 4.1063e-07,\n            2.9725e-07, 2.2462e-07, 2.6534e-07, 3.1258e-07, 3.0997e-07, 2.9475e-07,\n            4.3412e-07, 3.0148e-07, 3.2695e-07, 3.3244e-07, 2.7019e-07, 4.5899e-07,\n            2.5764e-07, 3.4205e-07, 2.8012e-07, 3.8858e-07, 2.0209e-07, 3.2018e-07,\n            3.3058e-07, 2.8374e-07, 3.9037e-07, 3.8512e-07, 3.0390e-07, 4.3031e-07,\n            2.5666e-07, 2.1216e-07, 2.4309e-07, 2.6753e-07, 2.7538e-07, 4.6157e-07,\n            3.0753e-07, 4.6157e-07, 2.5959e-07, 2.5660e-07, 3.8673e-07, 3.0742e-07,\n            3.7283e-07, 3.7332e-07, 3.4032e-07, 2.6654e-07, 3.8073e-07, 3.2370e-07,\n            4.8831e-07, 2.4794e-07, 4.0908e-07, 4.6293e-07, 2.8599e-07, 4.6391e-07,\n            2.7587e-07, 4.1502e-07, 2.7387e-07, 4.1645e-07, 2.4496e-07, 2.8445e-07,\n            3.6128e-07, 3.8313e-07, 4.4338e-07, 4.8476e-07, 4.4040e-07, 3.6652e-07,\n            3.1499e-07, 3.1901e-07, 3.0756e-07, 2.4200e-07, 2.8154e-07, 2.7929e-07,\n            2.6274e-07, 2.9838e-07, 3.1645e-07, 3.2921e-07, 3.7443e-07, 3.5994e-07,\n            3.6804e-07, 2.6422e-07, 3.2842e-07, 2.6126e-07, 3.8764e-07, 3.4025e-07,\n            3.0310e-07, 2.7324e-07, 3.1173e-07, 3.7186e-07, 2.3531e-07, 1.7778e-07,\n            3.8432e-07, 2.8245e-07, 3.1772e-07, 2.6781e-07, 3.2388e-07, 3.1821e-07,\n            2.4331e-07, 7.1433e-07, 4.1279e-07, 3.7351e-07, 3.0156e-07, 3.5347e-07,\n            3.7336e-07, 5.4037e-07, 2.7135e-07, 3.8502e-07, 3.6317e-07, 2.2685e-07,\n            3.7051e-07, 4.0674e-07, 2.0929e-07, 3.1328e-07, 4.3980e-07, 4.5144e-07,\n            2.5090e-07, 3.2807e-07], device='cuda:0')},\n   23: {'step': tensor(10550.),\n    'exp_avg': tensor([-1.4404e-04, -9.7995e-05,  9.1266e-05,  9.7787e-05,  1.8659e-05,\n             2.5801e-05, -5.5079e-05,  1.1854e-06,  1.3421e-05,  1.9380e-04,\n            -4.1737e-05, -2.9371e-05,  1.7220e-04,  1.5615e-04,  1.0256e-05,\n             1.0467e-04, -3.2495e-05, -1.3147e-04, -1.6239e-05, -2.5937e-05,\n            -3.2679e-05, -6.8130e-05,  2.2337e-04,  3.5271e-05,  1.2139e-04,\n             1.0284e-04, -1.2397e-04,  1.5412e-04, -4.2828e-06,  1.5943e-04,\n             3.6738e-05, -6.5168e-05,  6.6053e-05,  1.3332e-04, -3.8144e-05,\n             3.6910e-05,  1.1848e-04, -8.0251e-05,  4.0757e-07, -6.8183e-05,\n             1.3558e-05, -2.2539e-04, -2.5913e-05,  1.9770e-04,  5.3345e-05,\n            -2.0207e-04,  5.4529e-05, -9.0609e-05, -3.9362e-05,  1.8679e-04,\n            -1.1620e-04, -2.6724e-04, -5.7600e-06, -7.2224e-05,  1.8743e-04,\n            -4.6293e-06, -4.9793e-05, -3.0002e-05, -1.5045e-04, -1.5327e-04,\n             9.1184e-06, -1.7533e-04,  7.5840e-05,  1.1409e-04,  4.7586e-05,\n            -7.5847e-05, -1.4476e-04,  4.7474e-05,  2.6965e-05,  1.3162e-06,\n             7.6589e-05,  1.9359e-04,  1.4696e-04, -1.3303e-04,  1.2900e-04,\n             2.7875e-05,  9.0532e-05, -1.8427e-04, -1.6532e-04,  4.5734e-05,\n            -8.7845e-05, -7.1534e-05, -2.3785e-04, -9.8051e-05,  1.6083e-04,\n             6.5174e-05, -1.4330e-04, -3.4684e-05, -3.8854e-05,  1.5946e-04,\n             5.4026e-05, -1.2048e-04, -7.2033e-05, -9.6402e-05,  1.9709e-04,\n             9.3961e-05,  6.3650e-05, -2.0783e-04,  1.2101e-05,  8.0202e-06,\n            -1.0107e-04, -1.9762e-04, -8.3013e-05,  1.2502e-04,  1.2550e-04,\n             8.3974e-05, -1.2989e-04, -6.2164e-05, -1.6242e-04, -5.7194e-06,\n             5.9769e-06,  1.1449e-04,  9.1678e-05,  5.0756e-05,  5.4165e-05,\n            -1.5097e-04,  5.0031e-05,  2.6260e-05, -5.6975e-05, -4.0096e-06,\n             1.4428e-04,  5.4094e-05, -1.1046e-04,  4.7316e-05,  4.3118e-05,\n             8.3631e-05, -1.4402e-04,  1.3574e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([3.6678e-07, 1.8324e-07, 3.8513e-07, 1.7672e-07, 1.4632e-07, 2.3825e-07,\n            1.9773e-07, 1.3910e-07, 1.5126e-07, 2.9476e-07, 2.0286e-07, 2.6532e-07,\n            2.1575e-07, 2.4814e-07, 2.2791e-07, 1.8701e-07, 2.2660e-07, 1.5397e-07,\n            2.0394e-07, 1.9292e-07, 2.4552e-07, 2.0323e-07, 3.4797e-07, 2.7538e-07,\n            2.1304e-07, 1.6783e-07, 1.5381e-07, 1.9818e-07, 3.3060e-07, 1.8912e-07,\n            1.1417e-07, 1.7728e-07, 2.7867e-07, 1.8006e-07, 1.2916e-07, 2.7968e-07,\n            2.1284e-07, 2.6454e-07, 3.8795e-07, 2.2170e-07, 1.5045e-07, 2.1046e-07,\n            2.3042e-07, 1.9682e-07, 1.8312e-07, 1.1586e-07, 1.2689e-07, 2.0913e-07,\n            2.8135e-07, 1.8571e-07, 2.4376e-07, 2.4846e-07, 1.5934e-07, 2.2263e-07,\n            2.6676e-07, 1.2636e-07, 2.0462e-07, 1.4092e-07, 2.9221e-07, 1.7757e-07,\n            1.7893e-07, 1.6679e-07, 1.6617e-07, 2.6001e-07, 1.2437e-07, 1.4999e-07,\n            2.5830e-07, 2.3852e-07, 2.3778e-07, 2.5528e-07, 1.5690e-07, 2.4130e-07,\n            2.9651e-07, 2.5498e-07, 1.3020e-07, 2.0114e-07, 2.2920e-07, 2.4634e-07,\n            2.2324e-07, 1.8950e-07, 1.8466e-07, 2.7280e-07, 1.6708e-07, 1.4641e-07,\n            3.9250e-07, 1.8512e-07, 2.6620e-07, 2.0032e-07, 1.8260e-07, 2.6654e-07,\n            1.7218e-07, 1.8339e-07, 2.3495e-07, 2.7567e-07, 3.5742e-07, 1.4659e-07,\n            1.9368e-07, 2.7106e-07, 2.3591e-07, 1.7494e-07, 2.3899e-07, 1.8818e-07,\n            2.2383e-07, 2.5354e-07, 1.5448e-07, 1.7535e-07, 1.5495e-07, 2.4411e-07,\n            2.4857e-07, 3.6779e-07, 1.8615e-07, 2.6118e-07, 2.1807e-07, 1.9239e-07,\n            3.5790e-07, 2.7970e-07, 1.4316e-07, 1.7776e-07, 1.5206e-07, 9.6154e-08,\n            1.6274e-07, 3.4871e-07, 2.1681e-07, 2.3144e-07, 3.6589e-07, 1.6599e-07,\n            2.3710e-07, 2.0713e-07], device='cuda:0')},\n   24: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-2.3079e-05]],\n    \n             [[-9.7089e-05]],\n    \n             [[ 4.4733e-05]],\n    \n             ...,\n    \n             [[ 1.0228e-04]],\n    \n             [[ 7.2532e-05]],\n    \n             [[-1.4458e-05]]],\n    \n    \n            [[[-3.5659e-05]],\n    \n             [[-8.1615e-06]],\n    \n             [[ 2.8356e-05]],\n    \n             ...,\n    \n             [[-3.5454e-05]],\n    \n             [[-8.5029e-05]],\n    \n             [[-1.1503e-04]]],\n    \n    \n            [[[-9.2136e-05]],\n    \n             [[-2.2410e-05]],\n    \n             [[ 1.1558e-04]],\n    \n             ...,\n    \n             [[ 1.5083e-04]],\n    \n             [[ 1.0461e-06]],\n    \n             [[ 1.5468e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[-4.9747e-05]],\n    \n             [[ 5.5761e-05]],\n    \n             [[ 3.5507e-05]],\n    \n             ...,\n    \n             [[-2.1561e-05]],\n    \n             [[-8.1857e-05]],\n    \n             [[-1.2343e-05]]],\n    \n    \n            [[[ 1.6483e-04]],\n    \n             [[-1.2377e-04]],\n    \n             [[-1.3137e-04]],\n    \n             ...,\n    \n             [[ 2.6211e-04]],\n    \n             [[ 2.3403e-04]],\n    \n             [[-8.9479e-06]]],\n    \n    \n            [[[-1.1524e-05]],\n    \n             [[-2.2521e-06]],\n    \n             [[ 5.2191e-06]],\n    \n             ...,\n    \n             [[-3.6589e-05]],\n    \n             [[-3.4458e-06]],\n    \n             [[ 5.3399e-06]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[1.1710e-07]],\n    \n             [[2.6370e-07]],\n    \n             [[1.4208e-07]],\n    \n             ...,\n    \n             [[3.9640e-07]],\n    \n             [[2.3590e-07]],\n    \n             [[2.3017e-07]]],\n    \n    \n            [[[1.4309e-07]],\n    \n             [[1.2681e-07]],\n    \n             [[1.5546e-07]],\n    \n             ...,\n    \n             [[1.8830e-07]],\n    \n             [[1.8480e-07]],\n    \n             [[1.6726e-07]]],\n    \n    \n            [[[2.2434e-08]],\n    \n             [[4.3039e-08]],\n    \n             [[2.4770e-08]],\n    \n             ...,\n    \n             [[6.1695e-08]],\n    \n             [[3.5735e-08]],\n    \n             [[5.4438e-08]]],\n    \n    \n            ...,\n    \n    \n            [[[2.5547e-07]],\n    \n             [[2.3446e-07]],\n    \n             [[2.8169e-07]],\n    \n             ...,\n    \n             [[2.8680e-07]],\n    \n             [[2.8166e-07]],\n    \n             [[2.3718e-07]]],\n    \n    \n            [[[2.0330e-07]],\n    \n             [[2.6873e-07]],\n    \n             [[2.2936e-07]],\n    \n             ...,\n    \n             [[4.5261e-07]],\n    \n             [[2.3921e-07]],\n    \n             [[4.2286e-07]]],\n    \n    \n            [[[1.0019e-07]],\n    \n             [[2.2477e-07]],\n    \n             [[1.1367e-07]],\n    \n             ...,\n    \n             [[1.5071e-07]],\n    \n             [[1.5676e-07]],\n    \n             [[1.8461e-07]]]], device='cuda:0')},\n   25: {'step': tensor(10550.),\n    'exp_avg': tensor([ 4.8997e-05, -8.3997e-05,  4.7044e-05, -2.3092e-05, -2.5810e-05,\n             1.3040e-05, -4.9244e-05,  1.9269e-05,  5.9881e-06, -3.7753e-05,\n             9.1305e-05, -5.2361e-05,  1.3566e-05,  2.8445e-05, -1.0687e-05,\n             1.2590e-04, -3.2110e-05, -3.8684e-06,  4.6957e-05,  3.5029e-05,\n            -1.9540e-05, -9.5052e-06, -3.0826e-05, -4.3176e-05, -8.1985e-05,\n            -6.9435e-06, -1.9882e-05, -9.6769e-06, -1.4780e-05,  1.6683e-04,\n             1.9592e-04, -6.0214e-05,  1.8212e-05,  1.4259e-04, -1.2708e-04,\n            -6.0236e-05, -9.1554e-05, -3.9380e-04,  1.3710e-05,  4.7962e-05,\n            -7.5567e-05, -4.8780e-05, -5.5920e-05, -5.0250e-05,  5.1419e-05,\n            -1.7455e-09, -1.7588e-05, -5.1473e-05,  1.7855e-05, -2.2034e-05,\n            -1.6210e-04, -1.6928e-05,  1.7080e-04,  3.1014e-06, -3.3403e-05,\n             9.5952e-05,  1.0543e-05, -7.6434e-05, -2.2196e-05, -3.1038e-05,\n            -1.0732e-04,  1.3599e-04,  6.6439e-05,  1.3887e-04, -4.8711e-05,\n             1.6710e-04, -4.4503e-05,  7.2613e-05, -8.6711e-05,  8.7970e-05,\n            -6.6962e-05, -5.6259e-05, -4.2751e-05, -1.0451e-04,  3.9670e-05,\n            -1.8863e-05, -1.4921e-04,  5.8132e-06,  1.3197e-05, -3.7388e-05,\n            -4.8162e-05,  1.8013e-05,  1.4822e-05, -8.3959e-05, -3.3448e-05,\n             2.8037e-05,  5.3836e-05,  3.2268e-06,  8.2628e-06, -7.3234e-05,\n            -6.3999e-06, -1.7486e-05, -7.9802e-05, -2.2003e-04, -5.2146e-05,\n            -4.3935e-06,  3.2430e-05,  5.2796e-05, -3.8774e-05, -7.1466e-05,\n            -3.1184e-05, -3.8629e-05, -2.0195e-06, -4.0506e-05, -1.4013e-05,\n             4.7899e-05,  6.4061e-05, -2.1229e-05,  3.1079e-05,  1.7930e-05,\n             5.1447e-05, -2.2710e-05, -5.4538e-05,  1.3039e-05, -7.9138e-05,\n             4.5219e-05,  6.5875e-05, -4.5666e-05,  1.2270e-05,  6.2915e-05,\n             6.5196e-05, -3.2388e-05, -1.1187e-05, -1.8542e-04, -1.0175e-04,\n            -9.1069e-06, -9.7727e-05, -5.1074e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([1.1671e-07, 6.6745e-08, 2.8859e-08, 6.4732e-07, 6.3630e-08, 3.6555e-08,\n            7.0130e-08, 8.3540e-08, 3.7554e-08, 7.3470e-08, 2.1583e-07, 8.3778e-08,\n            6.3379e-08, 7.4993e-08, 2.8200e-07, 1.0186e-07, 2.7514e-08, 2.7046e-08,\n            3.1673e-08, 6.3442e-08, 1.0326e-07, 4.0453e-08, 5.6716e-08, 3.2450e-08,\n            2.8891e-08, 1.3761e-07, 9.3735e-08, 4.1318e-08, 4.5479e-08, 4.9426e-08,\n            1.1628e-07, 3.3969e-08, 7.1535e-08, 4.2876e-08, 6.9079e-08, 1.6562e-07,\n            8.2152e-08, 2.1892e-07, 5.3181e-08, 1.9624e-07, 5.9807e-08, 1.1386e-07,\n            1.0174e-07, 7.3880e-08, 3.3359e-08, 9.2325e-09, 1.1260e-08, 1.5077e-07,\n            4.4346e-08, 2.0284e-07, 1.1649e-07, 4.4366e-08, 8.8124e-08, 3.1341e-08,\n            4.8033e-08, 8.1650e-08, 7.4494e-08, 5.5892e-08, 1.7503e-07, 1.1926e-07,\n            1.0168e-07, 6.5962e-08, 8.9922e-08, 2.7915e-07, 1.3194e-07, 1.1318e-07,\n            7.9824e-08, 4.0461e-08, 7.8485e-08, 5.6649e-08, 3.8957e-08, 3.3656e-08,\n            5.6505e-08, 3.9956e-08, 5.4753e-08, 1.3072e-07, 3.6881e-08, 2.4696e-08,\n            4.1538e-08, 1.5598e-08, 1.1632e-07, 7.1382e-08, 4.9831e-08, 5.9832e-08,\n            1.4629e-07, 2.3024e-08, 8.6475e-08, 1.5200e-07, 3.6537e-08, 1.1495e-07,\n            2.8534e-08, 1.1847e-07, 1.4923e-07, 1.0864e-07, 2.9310e-08, 1.5181e-07,\n            4.4997e-08, 1.7721e-08, 1.2208e-07, 8.5438e-08, 5.2614e-08, 1.5323e-07,\n            1.7824e-08, 3.1917e-08, 3.5382e-08, 8.3822e-08, 1.2149e-07, 1.3576e-07,\n            2.8026e-08, 4.8426e-08, 4.8488e-08, 1.8045e-07, 6.2039e-08, 7.5361e-08,\n            8.3110e-08, 2.1888e-08, 2.7505e-08, 2.9088e-08, 3.4050e-08, 2.1591e-07,\n            4.8302e-08, 3.1833e-08, 2.1788e-07, 7.0403e-08, 1.0955e-07, 1.5650e-07,\n            1.8489e-07, 8.6520e-08], device='cuda:0')},\n   26: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-3.8900e-04, -9.1874e-04, -6.0691e-04, -2.1503e-04, -1.3826e-05],\n              [-3.5672e-04, -7.6533e-04, -8.8364e-04, -3.8295e-04,  5.8558e-05],\n              [-8.9388e-05, -3.2336e-04, -5.3796e-04, -5.3562e-04, -2.1489e-04],\n              [ 1.1078e-05, -7.1359e-05, -2.3851e-04, -4.0948e-04, -4.0620e-04],\n              [-4.1092e-07,  6.7197e-06,  2.9614e-05, -1.3720e-05, -1.2167e-04]]],\n    \n    \n            [[[-1.4216e-05,  4.2625e-06,  2.0641e-05, -2.9758e-05, -9.1649e-06],\n              [-1.2061e-04, -1.6818e-05, -4.9978e-05, -3.1462e-05, -5.2963e-06],\n              [-6.8313e-05, -3.7657e-05,  1.0709e-05, -4.0217e-06, -6.3929e-05],\n              [-5.5823e-05, -2.5175e-05, -2.0279e-05, -5.1127e-05, -4.6687e-05],\n              [ 5.1769e-05,  6.5019e-05, -7.3680e-05, -3.4008e-05, -1.5259e-05]]],\n    \n    \n            [[[ 1.4193e-04,  6.0621e-04,  1.4765e-03,  1.3905e-03,  5.3335e-04],\n              [ 2.8711e-04,  9.6218e-04,  1.7014e-03,  1.5792e-03,  6.2423e-04],\n              [-1.3221e-04,  3.9772e-04,  1.1430e-03,  9.0018e-04,  4.1680e-04],\n              [-1.8198e-04,  9.1381e-05,  2.1373e-04,  7.8922e-05, -3.9572e-05],\n              [-1.3470e-04, -2.9820e-04,  4.0101e-05,  3.5846e-04,  1.8078e-04]]],\n    \n    \n            ...,\n    \n    \n            [[[-2.7070e-06, -9.5438e-05, -7.7629e-05, -1.2763e-04,  2.1941e-05],\n              [ 5.1494e-05, -5.5125e-05, -1.5815e-04, -1.4347e-04,  5.5015e-05],\n              [-1.7574e-04, -3.0001e-04, -4.1912e-04, -2.6666e-04,  5.2397e-06],\n              [-1.8166e-04, -3.4598e-04, -5.2902e-04, -4.0159e-04,  5.4589e-06],\n              [-1.3667e-04, -1.5001e-04, -1.6632e-04, -6.6860e-05,  4.7372e-05]]],\n    \n    \n            [[[-3.4346e-05, -3.7781e-05, -1.1833e-04, -1.2913e-04, -1.0970e-04],\n              [-4.3850e-05, -4.3592e-04, -5.6110e-04, -5.2041e-04, -2.9073e-05],\n              [ 3.2884e-04, -5.9240e-04, -1.2162e-03, -6.2541e-04, -1.7239e-04],\n              [-8.9261e-05, -1.0607e-04, -4.6730e-04, -6.4673e-04, -5.4655e-04],\n              [-4.1665e-05, -2.2173e-05, -1.3284e-04, -1.2884e-04, -1.0866e-04]]],\n    \n    \n            [[[-3.9617e-05, -8.9063e-05, -1.6382e-04, -3.7042e-05,  4.0307e-05],\n              [-1.6151e-05, -1.2936e-04, -2.6379e-04, -2.1636e-04,  5.9932e-05],\n              [-4.1691e-05, -1.6908e-04, -3.4939e-04, -3.3339e-04, -1.8583e-04],\n              [-5.2531e-05, -1.6797e-04, -1.6606e-04, -2.7988e-04, -1.7303e-04],\n              [-9.9425e-05, -1.6767e-04, -1.1810e-04, -1.0016e-04, -5.6615e-07]]]],\n           device='cuda:0'),\n    'exp_avg_sq': tensor([[[[3.1269e-07, 8.7309e-07, 1.4356e-06, 1.5623e-06, 7.6867e-07],\n              [8.7458e-07, 1.5743e-06, 1.9478e-06, 2.0146e-06, 1.7740e-06],\n              [1.0150e-06, 1.8680e-06, 2.5419e-06, 3.8342e-06, 2.5587e-06],\n              [8.1639e-07, 1.9044e-06, 2.6484e-06, 4.1209e-06, 3.1334e-06],\n              [4.8707e-07, 1.2426e-06, 1.8668e-06, 2.5034e-06, 3.4595e-06]]],\n    \n    \n            [[[8.6015e-08, 1.7211e-07, 2.6204e-07, 2.2714e-07, 1.2618e-07],\n              [3.0164e-07, 6.8097e-07, 9.5911e-07, 9.2013e-07, 8.7053e-07],\n              [3.3736e-07, 6.4885e-07, 9.7530e-07, 1.3128e-06, 1.0606e-06],\n              [2.6522e-07, 6.6262e-07, 1.3127e-06, 1.3530e-06, 7.4690e-07],\n              [2.6982e-07, 8.7933e-07, 1.2057e-06, 8.8964e-07, 6.4209e-07]]],\n    \n    \n            [[[4.8675e-07, 8.7243e-07, 1.6321e-06, 1.6745e-06, 8.1767e-07],\n              [1.1611e-06, 2.2926e-06, 3.5518e-06, 2.9591e-06, 1.3406e-06],\n              [1.3248e-06, 2.5313e-06, 3.4621e-06, 2.9856e-06, 1.7408e-06],\n              [1.3075e-06, 2.0046e-06, 2.5496e-06, 2.2659e-06, 1.6534e-06],\n              [6.7593e-07, 1.0865e-06, 1.1551e-06, 1.2133e-06, 1.0842e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[4.6688e-07, 1.1144e-06, 1.4459e-06, 1.0718e-06, 5.8499e-07],\n              [1.2172e-06, 2.3405e-06, 2.6745e-06, 2.1774e-06, 1.7960e-06],\n              [1.2867e-06, 2.8663e-06, 4.4498e-06, 3.1462e-06, 2.3419e-06],\n              [1.1715e-06, 2.3751e-06, 3.4916e-06, 2.8562e-06, 2.4404e-06],\n              [7.8531e-07, 1.4772e-06, 2.1014e-06, 1.7438e-06, 1.9497e-06]]],\n    \n    \n            [[[6.6363e-07, 1.8545e-06, 3.1880e-06, 3.0146e-06, 1.9929e-06],\n              [1.7977e-06, 4.9865e-06, 7.6439e-06, 6.8756e-06, 4.3336e-06],\n              [2.6181e-06, 7.5337e-06, 1.2192e-05, 1.1828e-05, 8.6194e-06],\n              [3.6603e-06, 6.9757e-06, 9.4054e-06, 1.2466e-05, 1.3409e-05],\n              [3.7406e-06, 7.6844e-06, 1.6090e-05, 1.5097e-05, 9.7725e-06]]],\n    \n    \n            [[[6.5358e-07, 1.0323e-06, 1.0156e-06, 9.1493e-07, 8.1299e-07],\n              [7.8274e-07, 2.0103e-06, 2.6004e-06, 2.1600e-06, 1.2019e-06],\n              [1.2639e-06, 3.5713e-06, 5.3809e-06, 5.2424e-06, 3.3840e-06],\n              [1.0773e-06, 2.5015e-06, 3.8118e-06, 4.3298e-06, 3.2629e-06],\n              [9.3939e-07, 2.4823e-06, 4.1002e-06, 3.4900e-06, 2.4925e-06]]]],\n           device='cuda:0')},\n   27: {'step': tensor(10550.),\n    'exp_avg': tensor([-9.0222e-05, -1.4612e-05, -1.7325e-05,  3.7741e-05,  8.3340e-05,\n            -3.8265e-05,  1.4983e-04,  8.0443e-05,  4.3264e-05,  4.8169e-05,\n            -9.4867e-05, -2.6697e-05, -1.2096e-04, -9.4070e-05, -3.1025e-05,\n             1.4225e-04, -1.0067e-05, -2.0633e-05,  3.9544e-06,  8.4829e-05,\n             3.5951e-05, -5.4889e-05,  6.9228e-05, -3.9354e-06, -2.3818e-04,\n             6.2230e-06,  1.2068e-04, -6.5232e-06, -6.2337e-05,  4.9181e-05,\n            -7.2698e-07,  6.3177e-05, -6.6902e-05, -7.7184e-05, -2.0974e-04,\n            -8.1027e-05, -1.1733e-05, -3.4045e-04,  2.8288e-05, -8.9295e-05,\n            -3.1529e-05, -6.1513e-05,  1.6596e-04, -1.9630e-05, -2.8819e-05,\n            -1.2753e-04, -6.0197e-06, -2.5874e-05,  1.4820e-05,  7.7774e-05,\n             2.2012e-04, -3.1575e-04, -8.8817e-05,  9.8943e-06,  6.8269e-05,\n             5.7835e-05,  6.0001e-05, -1.4858e-04, -1.0305e-04, -2.4031e-04,\n             2.0822e-04,  1.6855e-04, -2.7533e-05, -1.6259e-04,  1.2288e-05,\n             3.4260e-04, -4.0949e-05, -3.5100e-05, -2.2862e-05, -8.3681e-06,\n             1.6095e-04,  1.6115e-05,  7.8085e-06,  3.6064e-04,  1.4195e-04,\n             1.5539e-05,  5.2443e-05, -5.1093e-06,  1.1627e-05,  8.4351e-05,\n             2.5493e-05,  4.5055e-06, -1.2474e-05,  3.4622e-06,  3.7015e-05,\n             1.3340e-05,  4.1553e-05, -3.1560e-05, -1.1048e-05, -2.7485e-05,\n             1.5403e-04,  1.2742e-04,  1.1206e-05,  1.8274e-04,  1.1641e-06,\n             1.5739e-04, -4.1483e-05, -7.7660e-05,  1.4813e-05,  4.3022e-05,\n            -9.6539e-05,  1.8060e-04,  1.4052e-05, -2.1247e-04, -1.4208e-05,\n             5.5816e-05,  1.1046e-04,  2.8315e-05, -8.6251e-05, -1.6677e-05,\n            -6.0528e-05,  3.7963e-05, -6.8685e-05, -1.3745e-04,  8.4088e-05,\n            -2.9378e-05,  7.7391e-05,  3.6489e-05,  6.5727e-05, -1.3925e-04,\n            -5.5557e-05, -1.0736e-04,  4.7607e-05, -2.0702e-04, -7.8849e-05,\n            -2.5730e-05, -1.9412e-04, -1.0083e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([1.4111e-07, 1.2914e-07, 1.2169e-07, 4.5462e-07, 3.3420e-07, 4.4064e-08,\n            1.2418e-07, 1.4693e-07, 1.3451e-07, 1.0831e-07, 1.4579e-07, 1.2229e-07,\n            3.4371e-07, 3.0913e-07, 2.9870e-07, 2.4215e-07, 5.3306e-08, 6.2628e-08,\n            3.1345e-08, 2.9792e-07, 3.1316e-07, 1.7324e-07, 5.0116e-08, 4.9152e-08,\n            1.4801e-07, 1.1107e-07, 2.3743e-07, 1.3287e-07, 6.9224e-08, 1.6672e-07,\n            2.3022e-07, 8.8199e-08, 1.4128e-07, 9.7811e-08, 2.4203e-07, 3.0920e-07,\n            6.3418e-08, 4.8653e-07, 4.2513e-08, 1.9955e-07, 7.0889e-08, 7.8414e-08,\n            2.3852e-07, 3.9683e-07, 2.9114e-07, 2.2027e-08, 2.1358e-08, 1.6616e-07,\n            4.1740e-08, 4.3534e-07, 3.9336e-07, 2.7353e-07, 1.0705e-07, 6.7828e-08,\n            4.0073e-08, 1.3023e-07, 5.2929e-08, 1.6874e-07, 3.8109e-07, 6.8943e-07,\n            3.6913e-07, 1.6471e-07, 1.7524e-07, 1.6198e-07, 3.8625e-07, 4.9555e-07,\n            1.0222e-07, 8.1631e-08, 2.5003e-07, 9.7492e-08, 9.9562e-08, 2.2170e-08,\n            7.5355e-08, 2.1145e-07, 1.0812e-07, 2.6148e-07, 1.4544e-07, 4.8150e-08,\n            1.7032e-07, 8.2445e-08, 3.2543e-07, 2.5979e-07, 1.2186e-07, 2.6201e-07,\n            1.2189e-07, 7.9484e-08, 2.5164e-07, 7.2225e-07, 4.9172e-08, 1.9237e-07,\n            7.2296e-08, 3.1295e-07, 1.2285e-07, 3.5571e-07, 8.3727e-08, 3.8673e-07,\n            4.1136e-08, 3.8020e-08, 6.2618e-08, 2.2060e-07, 9.3607e-08, 3.1655e-07,\n            4.3390e-08, 9.1870e-08, 6.7169e-08, 1.7310e-07, 4.4186e-07, 1.9250e-07,\n            1.1840e-07, 1.3110e-07, 9.2268e-08, 1.6882e-07, 5.5517e-08, 2.3817e-07,\n            8.9757e-08, 8.6010e-08, 1.1835e-07, 8.5456e-08, 7.1585e-08, 3.9089e-07,\n            1.2943e-07, 1.9644e-07, 3.4113e-07, 7.6704e-08, 4.1349e-07, 3.6436e-07,\n            4.5436e-07, 3.2427e-07], device='cuda:0')},\n   28: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n              [ 0.0000e+00,  6.4181e-05, -2.5988e-04,  7.1373e-06,  0.0000e+00],\n              [ 0.0000e+00,  1.9360e-04,  8.0615e-04, -1.1465e-04,  0.0000e+00],\n              [ 0.0000e+00, -5.0677e-05,  3.5273e-04,  2.1200e-05,  0.0000e+00],\n              [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n    \n    \n            [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n              [ 0.0000e+00,  9.1526e-05, -4.8895e-05,  5.3935e-05,  0.0000e+00],\n              [ 0.0000e+00,  3.8161e-05, -1.6238e-05, -1.2957e-05,  0.0000e+00],\n              [ 0.0000e+00,  3.9744e-06, -7.0276e-05,  2.4832e-05,  0.0000e+00],\n              [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n    \n    \n            [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n              [ 0.0000e+00, -6.9683e-04, -3.4524e-03, -5.6874e-04,  0.0000e+00],\n              [ 0.0000e+00, -1.3027e-03, -3.4498e-03, -4.2194e-04,  0.0000e+00],\n              [ 0.0000e+00, -1.5929e-04, -1.7092e-03,  6.4870e-05,  0.0000e+00],\n              [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n    \n    \n            ...,\n    \n    \n            [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n              [ 0.0000e+00, -2.4652e-05, -1.7954e-04, -2.6253e-05,  0.0000e+00],\n              [ 0.0000e+00,  6.4718e-05, -7.5426e-04, -2.6601e-04,  0.0000e+00],\n              [ 0.0000e+00, -4.0125e-05, -3.5287e-04, -9.6343e-06,  0.0000e+00],\n              [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n    \n    \n            [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n              [ 0.0000e+00,  7.7666e-05,  1.6697e-04,  7.0219e-05,  0.0000e+00],\n              [ 0.0000e+00,  1.1519e-04,  1.0211e-03, -3.7186e-05,  0.0000e+00],\n              [ 0.0000e+00,  1.0269e-04,  3.1841e-04,  5.4508e-05,  0.0000e+00],\n              [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n    \n    \n            [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n              [ 0.0000e+00, -2.7048e-05, -7.4708e-06,  3.5655e-05,  0.0000e+00],\n              [ 0.0000e+00, -1.9143e-04, -5.1578e-04,  7.7487e-05,  0.0000e+00],\n              [ 0.0000e+00,  9.0411e-05,  2.3021e-04,  1.0326e-04,  0.0000e+00],\n              [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]],\n           device='cuda:0'),\n    'exp_avg_sq': tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n              [0.0000e+00, 3.6512e-07, 6.1680e-06, 1.5398e-07, 0.0000e+00],\n              [0.0000e+00, 1.5117e-06, 1.0877e-05, 4.9197e-07, 0.0000e+00],\n              [0.0000e+00, 2.7687e-07, 1.9950e-06, 5.6171e-08, 0.0000e+00],\n              [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n    \n    \n            [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n              [0.0000e+00, 2.9011e-07, 3.1778e-06, 2.9623e-07, 0.0000e+00],\n              [0.0000e+00, 1.7854e-06, 3.6580e-06, 2.0010e-07, 0.0000e+00],\n              [0.0000e+00, 2.4843e-07, 6.4315e-07, 3.2906e-08, 0.0000e+00],\n              [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n    \n    \n            [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n              [0.0000e+00, 1.6559e-06, 1.2321e-05, 6.7330e-07, 0.0000e+00],\n              [0.0000e+00, 5.5230e-06, 3.0374e-05, 1.6248e-06, 0.0000e+00],\n              [0.0000e+00, 1.4467e-06, 5.7842e-06, 2.5353e-07, 0.0000e+00],\n              [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n    \n    \n            ...,\n    \n    \n            [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n              [0.0000e+00, 4.8794e-07, 9.4236e-06, 3.1042e-07, 0.0000e+00],\n              [0.0000e+00, 4.1955e-06, 1.4961e-05, 8.3649e-07, 0.0000e+00],\n              [0.0000e+00, 7.9349e-07, 4.3399e-06, 1.4805e-07, 0.0000e+00],\n              [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n    \n    \n            [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n              [0.0000e+00, 2.0874e-06, 2.1932e-05, 8.3295e-07, 0.0000e+00],\n              [0.0000e+00, 5.0551e-06, 2.3604e-05, 8.0567e-07, 0.0000e+00],\n              [0.0000e+00, 1.5616e-06, 6.6012e-06, 1.8700e-07, 0.0000e+00],\n              [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n    \n    \n            [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n              [0.0000e+00, 1.5714e-06, 2.0474e-05, 7.2655e-07, 0.0000e+00],\n              [0.0000e+00, 9.7489e-06, 3.8261e-05, 1.8326e-06, 0.0000e+00],\n              [0.0000e+00, 1.9639e-06, 1.2990e-05, 5.0486e-07, 0.0000e+00],\n              [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]]],\n           device='cuda:0')},\n   29: {'step': tensor(10550.),\n    'exp_avg': tensor([-5.5201e-04,  2.0186e-04,  3.5288e-04, -9.0455e-04, -5.3630e-04,\n            -4.2167e-05,  1.5977e-03,  6.4923e-04, -4.2111e-04, -1.1701e-04,\n            -3.8897e-04,  4.6597e-05, -2.8242e-04,  6.8160e-05, -3.1011e-04,\n             4.2499e-04,  1.8055e-04, -6.6193e-04,  2.8900e-04, -7.4142e-04,\n            -2.3339e-04, -8.7444e-04,  8.6421e-04,  1.6602e-04, -4.9246e-04,\n            -8.0655e-04, -7.2056e-04,  2.9053e-04, -2.0965e-04, -6.4387e-04,\n             6.8826e-05,  1.3259e-03,  7.3636e-04,  1.1834e-03, -1.4782e-03,\n            -2.9420e-04,  6.9935e-04, -1.8754e-03, -6.7213e-04,  7.1109e-04,\n            -8.7363e-05,  5.8368e-04, -7.8367e-04,  1.2850e-03, -7.5151e-04,\n             1.4417e-03,  1.0117e-05,  5.5230e-04, -1.0788e-03,  3.9553e-04,\n             2.8830e-04, -8.0868e-04,  7.0328e-04,  3.5960e-04,  1.2227e-04,\n            -7.5558e-04, -2.9203e-04, -4.1469e-04,  9.8714e-04,  1.2783e-03,\n             1.3456e-03, -7.1894e-04, -5.8466e-04, -1.0986e-04, -7.7653e-05,\n            -7.9140e-04, -1.7547e-04, -7.5649e-05, -2.9559e-04, -1.5288e-04,\n            -4.8174e-04,  7.3008e-04, -4.1719e-04, -1.7208e-03,  6.8659e-04,\n             3.7512e-04,  1.8093e-03, -3.4542e-04, -8.6034e-04,  1.4077e-03,\n            -8.5634e-04, -1.4303e-04, -2.7027e-05,  2.8402e-04, -1.4862e-03,\n            -3.1313e-04, -1.1888e-03, -2.8354e-04,  3.7363e-04,  2.6493e-04,\n            -1.5417e-03, -5.6704e-04,  1.6060e-04,  3.6747e-04,  5.5614e-04,\n            -1.1005e-03, -5.5664e-04, -4.3742e-04,  1.5869e-04,  6.2477e-04,\n             1.0838e-03,  3.3637e-04, -4.9284e-04,  1.0514e-03,  2.7032e-04,\n             6.5008e-05,  1.0932e-03,  5.9555e-04, -7.9228e-04, -4.4792e-04,\n             5.8973e-04, -4.2909e-04,  1.1315e-05,  1.0259e-03,  4.5110e-04,\n            -1.3476e-04,  8.9076e-04,  1.2025e-03, -3.2205e-04, -4.1956e-04,\n            -1.1048e-04, -5.9945e-04,  8.2755e-05, -7.7004e-04, -6.1110e-04,\n            -1.8578e-04, -7.7057e-04, -1.2833e-03], device='cuda:0'),\n    'exp_avg_sq': tensor([9.6094e-06, 6.4347e-06, 1.3238e-05, 1.2557e-05, 1.7479e-05, 5.6930e-06,\n            1.5485e-05, 8.9050e-06, 1.0587e-05, 8.8104e-06, 1.3585e-05, 1.2984e-05,\n            1.0918e-05, 1.6725e-05, 1.5182e-05, 1.4320e-05, 4.8194e-06, 6.3418e-06,\n            9.1040e-06, 1.1510e-05, 1.3588e-05, 9.9900e-06, 4.4421e-06, 5.8827e-06,\n            8.3460e-06, 1.3237e-05, 1.6662e-05, 9.4773e-06, 5.1758e-06, 2.0759e-05,\n            1.1320e-05, 7.7235e-06, 7.2497e-06, 6.2763e-06, 1.3438e-05, 1.2695e-05,\n            4.5231e-06, 1.1355e-05, 4.4163e-06, 1.4955e-05, 4.5790e-06, 7.6563e-06,\n            1.2146e-05, 1.6373e-05, 1.3917e-05, 4.5823e-06, 4.1808e-06, 1.3688e-05,\n            5.5500e-06, 2.1103e-05, 9.6582e-06, 1.0832e-05, 5.4815e-06, 8.0295e-06,\n            9.0989e-06, 1.2910e-05, 7.9927e-06, 6.0278e-06, 1.6405e-05, 8.8209e-06,\n            1.5321e-05, 8.1716e-06, 9.7672e-06, 1.1142e-05, 1.6135e-05, 1.2020e-05,\n            9.7736e-06, 8.5656e-06, 1.5530e-05, 6.1279e-06, 7.8289e-06, 7.2872e-06,\n            4.7456e-06, 1.2952e-05, 1.1312e-05, 1.1250e-05, 9.1107e-06, 5.2972e-06,\n            6.6047e-06, 6.4816e-06, 1.3023e-05, 1.9602e-05, 8.4418e-06, 1.4314e-05,\n            1.2635e-05, 6.6434e-06, 1.2434e-05, 2.4948e-05, 4.8832e-06, 6.5169e-06,\n            1.0954e-05, 1.1422e-05, 6.0228e-06, 1.0284e-05, 1.5347e-05, 1.5343e-05,\n            7.7597e-06, 3.6630e-06, 7.9610e-06, 1.0205e-05, 8.8942e-06, 1.3254e-05,\n            5.4323e-06, 3.9571e-06, 5.9052e-06, 9.7169e-06, 1.1606e-05, 1.1296e-05,\n            5.6814e-06, 5.2961e-06, 8.8603e-06, 1.7781e-05, 7.7061e-06, 9.2215e-06,\n            7.6510e-06, 7.3100e-06, 5.5460e-06, 1.0218e-05, 5.6447e-06, 1.7231e-05,\n            9.1938e-06, 1.0792e-05, 1.4496e-05, 6.2721e-06, 1.5308e-05, 1.4013e-05,\n            1.7405e-05, 2.0649e-05], device='cuda:0')},\n   30: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-7.8405e-05]],\n    \n             [[ 2.4287e-06]],\n    \n             [[-1.4861e-04]],\n    \n             ...,\n    \n             [[-1.5622e-04]],\n    \n             [[-9.6149e-06]],\n    \n             [[ 1.5735e-05]]],\n    \n    \n            [[[ 4.6553e-05]],\n    \n             [[-3.1604e-05]],\n    \n             [[ 9.5182e-05]],\n    \n             ...,\n    \n             [[-5.2112e-05]],\n    \n             [[ 1.6131e-04]],\n    \n             [[ 1.9043e-06]]],\n    \n    \n            [[[-1.8196e-06]],\n    \n             [[ 2.0064e-05]],\n    \n             [[ 2.5631e-04]],\n    \n             ...,\n    \n             [[ 2.1023e-05]],\n    \n             [[ 9.8424e-05]],\n    \n             [[ 4.2714e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[ 8.9589e-06]],\n    \n             [[-8.9777e-06]],\n    \n             [[ 7.3054e-06]],\n    \n             ...,\n    \n             [[-3.6852e-05]],\n    \n             [[-5.4006e-05]],\n    \n             [[-3.1617e-05]]],\n    \n    \n            [[[ 9.1218e-05]],\n    \n             [[-3.5636e-05]],\n    \n             [[-9.0211e-05]],\n    \n             ...,\n    \n             [[ 1.3197e-04]],\n    \n             [[-4.2527e-05]],\n    \n             [[-1.8054e-05]]],\n    \n    \n            [[[ 2.4619e-05]],\n    \n             [[-5.5438e-06]],\n    \n             [[ 8.3227e-05]],\n    \n             ...,\n    \n             [[ 1.4728e-04]],\n    \n             [[-4.1392e-05]],\n    \n             [[-4.6688e-05]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[1.1173e-07]],\n    \n             [[7.0156e-08]],\n    \n             [[1.3980e-07]],\n    \n             ...,\n    \n             [[2.8486e-07]],\n    \n             [[3.6746e-07]],\n    \n             [[1.3301e-07]]],\n    \n    \n            [[[2.6127e-07]],\n    \n             [[1.6047e-07]],\n    \n             [[3.8538e-07]],\n    \n             ...,\n    \n             [[4.5161e-07]],\n    \n             [[1.4116e-06]],\n    \n             [[6.3962e-07]]],\n    \n    \n            [[[9.9589e-08]],\n    \n             [[4.2971e-08]],\n    \n             [[2.1661e-07]],\n    \n             ...,\n    \n             [[6.6450e-07]],\n    \n             [[3.3396e-07]],\n    \n             [[1.3947e-07]]],\n    \n    \n            ...,\n    \n    \n            [[[1.4027e-07]],\n    \n             [[6.8670e-08]],\n    \n             [[1.0016e-07]],\n    \n             ...,\n    \n             [[2.9899e-07]],\n    \n             [[3.6794e-07]],\n    \n             [[1.3453e-07]]],\n    \n    \n            [[[2.0242e-07]],\n    \n             [[1.6224e-07]],\n    \n             [[4.5943e-07]],\n    \n             ...,\n    \n             [[2.8964e-07]],\n    \n             [[5.3530e-07]],\n    \n             [[4.6988e-07]]],\n    \n    \n            [[[3.0375e-07]],\n    \n             [[1.7478e-07]],\n    \n             [[2.0272e-07]],\n    \n             ...,\n    \n             [[4.6866e-07]],\n    \n             [[5.5057e-07]],\n    \n             [[2.8416e-07]]]], device='cuda:0')},\n   31: {'step': tensor(10550.),\n    'exp_avg': tensor([ 2.4777e-04,  1.4150e-04, -1.2663e-03, -2.7173e-04,  2.5954e-04,\n             1.3105e-04, -6.1564e-04,  1.9515e-04,  2.5815e-04, -4.3324e-05,\n            -3.0570e-04,  1.3928e-04, -3.1769e-04,  5.4467e-05,  5.6446e-05,\n            -1.6769e-04, -7.9716e-05,  9.8905e-05, -3.6371e-04, -1.7943e-04,\n            -1.2743e-03,  9.5962e-05, -1.5014e-05, -2.3794e-05, -6.4513e-05,\n            -1.2686e-04, -2.6765e-04, -7.4694e-05, -6.2657e-05, -7.9160e-05,\n             2.6857e-03, -1.1140e-04, -8.3333e-05, -2.2590e-04,  2.8601e-04,\n            -2.6605e-04, -4.7346e-04,  1.2512e-04,  1.1879e-04, -9.2611e-05,\n             1.3606e-04, -9.3011e-04,  8.5581e-04,  3.2879e-04, -1.9727e-04,\n            -2.0921e-04, -4.8970e-05,  6.2164e-05, -3.2552e-04,  1.8364e-04,\n            -2.3871e-04, -1.4715e-05,  1.3078e-03, -1.1233e-04,  8.2595e-05,\n            -2.1144e-04, -7.4351e-04, -6.2367e-04, -3.4417e-04,  1.2199e-04,\n            -5.3563e-04,  3.8028e-04, -5.0319e-04,  8.2685e-04,  9.6560e-05,\n             1.4520e-04,  1.3304e-04, -6.1595e-05,  3.7122e-06, -2.8332e-05,\n            -5.3433e-05,  1.2481e-04, -3.7448e-05, -6.5295e-05, -5.2703e-05,\n            -2.6842e-04,  3.6218e-04, -4.9329e-05, -6.0395e-04,  6.2246e-06,\n             1.0590e-03, -7.2112e-05,  7.5769e-05,  9.7109e-05, -4.5568e-04,\n            -1.2030e-04,  8.7523e-05,  2.6098e-04, -3.1387e-04,  2.2224e-04,\n            -5.5155e-05,  1.5063e-04,  4.1307e-04, -1.4159e-04,  1.5943e-04,\n             2.2947e-04,  1.6776e-04, -3.1115e-04,  1.2755e-03,  4.9128e-05,\n             2.0025e-04,  3.2513e-04,  5.6095e-04, -9.7197e-04, -7.5928e-05,\n            -5.8274e-05,  3.6423e-05, -3.8662e-04, -4.3783e-05, -1.8934e-04,\n            -5.1645e-04, -1.3318e-04, -1.8641e-04, -8.3914e-05, -1.2742e-04,\n            -1.4800e-04,  9.7663e-05, -6.2690e-04, -6.6819e-05, -8.1072e-05,\n            -7.6097e-05, -2.5880e-05,  2.2662e-05, -3.2410e-04,  6.3316e-04,\n            -2.9091e-04, -3.3066e-04, -1.6500e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([2.0108e-06, 4.6713e-06, 4.0521e-06, 4.5540e-06, 2.9206e-07, 2.6815e-06,\n            2.2014e-06, 1.6500e-06, 1.5462e-06, 7.7917e-06, 2.4038e-06, 6.8976e-06,\n            2.2446e-06, 2.6401e-06, 9.8999e-06, 1.4766e-06, 1.8960e-06, 3.2864e-06,\n            2.2497e-06, 1.3312e-06, 5.5511e-06, 1.3076e-06, 4.4597e-06, 3.4233e-06,\n            1.4159e-06, 2.1352e-06, 6.9213e-06, 1.5511e-06, 2.5980e-06, 9.7650e-07,\n            4.6444e-06, 6.7517e-07, 1.7111e-06, 2.1801e-06, 2.8768e-06, 9.3006e-07,\n            6.5273e-06, 1.9667e-06, 4.4311e-06, 1.6449e-06, 4.2474e-06, 2.9279e-06,\n            3.5926e-06, 2.5424e-06, 2.4724e-06, 1.1498e-06, 6.5011e-07, 1.9116e-06,\n            1.6162e-06, 1.3505e-06, 6.4059e-06, 2.2601e-07, 5.0892e-06, 1.6826e-06,\n            4.2627e-06, 5.0889e-06, 1.7873e-06, 5.0241e-06, 5.6144e-06, 1.6742e-06,\n            1.4741e-06, 8.3873e-07, 2.7712e-06, 1.2799e-05, 1.3524e-06, 3.4581e-06,\n            3.1033e-06, 3.3934e-06, 1.0403e-06, 2.8925e-06, 2.2936e-06, 8.6415e-07,\n            5.2128e-06, 2.4367e-07, 2.3830e-06, 2.8835e-06, 8.8878e-07, 5.3474e-06,\n            3.4724e-06, 8.4371e-07, 1.5143e-06, 2.7151e-06, 3.0256e-06, 1.5505e-06,\n            1.4682e-06, 4.0081e-06, 2.4622e-06, 3.0111e-06, 3.3684e-06, 1.7473e-06,\n            1.2523e-06, 1.8800e-06, 4.1913e-06, 2.3003e-06, 9.7936e-07, 5.1183e-06,\n            3.1304e-06, 1.2083e-06, 2.3654e-05, 2.8440e-06, 4.6522e-06, 3.5464e-06,\n            1.0934e-06, 1.3581e-06, 4.9318e-06, 2.7623e-06, 9.1027e-07, 7.4188e-06,\n            1.3085e-06, 6.0580e-06, 3.3889e-06, 4.8681e-06, 3.1535e-06, 2.0580e-06,\n            2.4098e-06, 3.7122e-07, 2.8500e-06, 2.0337e-06, 2.2872e-06, 3.2385e-06,\n            1.1576e-06, 9.8550e-07, 2.9608e-06, 1.5745e-06, 2.9328e-06, 1.8824e-06,\n            2.0684e-06, 2.5618e-06], device='cuda:0')},\n   32: {'step': tensor(10550.),\n    'exp_avg': tensor([ 9.4104e-05,  9.9713e-05,  5.8932e-04, -4.7005e-04, -9.5265e-05,\n            -2.0034e-04,  4.7295e-04, -2.2566e-04,  3.4433e-04, -2.1303e-04,\n             9.4166e-05,  4.5456e-04,  2.9384e-04,  2.3739e-05, -2.4199e-04,\n            -1.3930e-04,  2.4273e-05, -2.5533e-05, -1.3404e-04,  4.6112e-04,\n             2.2126e-04, -3.3956e-04,  2.0118e-04, -2.8926e-04, -2.1190e-05,\n             2.4122e-04, -3.6393e-04, -2.1317e-05, -4.1531e-05,  3.8333e-04,\n            -6.1696e-04,  2.6791e-04, -9.5871e-05,  8.2826e-04, -3.5050e-05,\n            -2.1578e-04, -2.3502e-04, -3.2102e-04, -5.2269e-04, -1.1123e-04,\n             7.6496e-05,  7.1462e-05, -5.1589e-04,  1.6920e-04,  8.2611e-05,\n             5.1255e-05, -1.5701e-04, -3.2020e-05, -1.0086e-04,  2.7692e-04,\n             1.2662e-04, -4.8620e-05, -1.0222e-04,  1.2643e-04,  2.5975e-05,\n             6.4223e-04,  3.3776e-04, -2.5641e-04,  5.8673e-04,  1.3212e-04,\n             4.9004e-04,  5.3307e-04, -5.0706e-05,  1.3462e-04, -2.2019e-04,\n            -1.4579e-05,  1.9203e-05, -2.0732e-04, -4.3441e-05,  6.4129e-04,\n            -1.6360e-05, -2.9332e-04,  2.5789e-06, -1.6075e-04, -4.7621e-06,\n            -1.8732e-04,  3.5645e-04,  2.6176e-04,  1.7449e-05, -1.5496e-04,\n            -3.0138e-04, -1.2506e-04,  2.5493e-05,  6.3931e-06,  2.4116e-05,\n             2.1481e-04,  6.7881e-05, -2.4019e-04, -1.1797e-05, -1.7723e-04,\n             1.0802e-04, -1.5969e-04,  1.4237e-05,  4.5625e-04,  9.5966e-05,\n             3.1546e-05, -3.9327e-05,  3.1991e-04, -1.0399e-04, -1.0672e-03,\n             7.4455e-05,  3.1848e-04, -2.0771e-05,  2.5438e-05, -4.0158e-05,\n             6.9038e-05,  7.3657e-05,  2.6832e-04, -4.6314e-05,  1.9916e-04,\n            -3.5876e-04,  5.9417e-04,  1.8249e-04, -1.1901e-04, -1.5812e-04,\n             1.0734e-04,  2.4056e-04, -5.5890e-05,  2.0579e-05, -1.7699e-04,\n             2.9255e-04, -1.4113e-04, -1.1621e-03,  2.7783e-04, -1.3011e-04,\n             1.0642e-04,  9.5983e-05, -9.9381e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([7.9436e-07, 1.0313e-06, 1.3948e-06, 3.6031e-06, 1.1957e-06, 1.1344e-06,\n            1.0227e-06, 1.2118e-06, 2.0896e-06, 2.4192e-06, 1.5535e-06, 2.8455e-06,\n            1.2027e-06, 1.8367e-06, 2.4724e-06, 8.7538e-07, 9.4338e-07, 1.6704e-06,\n            1.5472e-06, 1.4611e-06, 2.3735e-06, 1.1102e-06, 2.3523e-06, 2.5337e-06,\n            3.9886e-07, 2.1448e-06, 2.1717e-06, 8.7494e-07, 2.1331e-06, 1.3218e-06,\n            1.7227e-06, 7.8785e-07, 1.1452e-06, 1.7635e-06, 1.6442e-06, 6.9278e-07,\n            2.2474e-06, 1.1608e-06, 1.9287e-06, 2.8502e-06, 2.3414e-06, 1.5415e-06,\n            2.3092e-06, 1.4904e-06, 8.2181e-07, 6.2648e-07, 8.5215e-07, 1.3084e-06,\n            7.4339e-07, 5.2126e-07, 3.6200e-06, 4.7243e-07, 1.2912e-06, 5.5671e-07,\n            2.4415e-06, 2.9067e-06, 1.3828e-06, 1.0695e-06, 2.6343e-06, 9.8354e-07,\n            1.0826e-06, 8.1161e-07, 1.4237e-06, 6.3016e-06, 1.6582e-06, 1.2150e-06,\n            1.2784e-06, 2.6037e-06, 7.3059e-07, 2.8355e-06, 2.5665e-06, 5.7207e-07,\n            2.2717e-06, 5.3200e-07, 8.4133e-07, 1.1173e-06, 1.0701e-06, 2.1085e-06,\n            9.3889e-07, 8.7399e-07, 4.6832e-07, 1.1151e-06, 2.5538e-06, 4.8802e-07,\n            1.8594e-06, 1.1870e-06, 7.9532e-07, 1.2222e-06, 2.6810e-06, 1.3407e-06,\n            4.2601e-07, 1.4956e-06, 3.3458e-06, 1.2719e-06, 9.8978e-07, 1.1686e-06,\n            1.4057e-06, 5.2265e-07, 4.2117e-06, 4.6331e-06, 1.8359e-06, 2.0798e-06,\n            6.8301e-07, 1.0344e-06, 1.0599e-06, 1.2291e-06, 9.4565e-07, 5.8974e-06,\n            8.1796e-07, 2.0851e-06, 1.1317e-06, 5.5610e-06, 1.5233e-06, 1.0918e-06,\n            3.2877e-06, 1.2514e-06, 1.8484e-06, 9.5497e-07, 1.3034e-06, 9.5019e-07,\n            1.0731e-06, 6.4827e-07, 2.7266e-06, 1.9006e-06, 6.8475e-07, 8.5591e-07,\n            1.4990e-06, 1.5705e-06], device='cuda:0')},\n   33: {'step': tensor(10550.),\n    'exp_avg': tensor([ 3.6433e-04,  7.4773e-05,  9.0569e-04, -9.8182e-04,  6.0949e-04,\n             8.6728e-04, -3.2514e-04,  2.0600e-04,  9.9009e-04, -2.9403e-04,\n             1.0966e-03,  4.8115e-04,  1.4202e-03, -2.7682e-04,  6.1689e-04,\n             5.4960e-04,  3.1375e-04, -4.5823e-04,  7.5089e-04, -9.6601e-04,\n             2.2211e-04,  1.2221e-04,  4.9348e-04,  1.0455e-03, -3.4625e-04,\n            -6.3587e-04, -6.4633e-04,  5.3157e-04, -7.3156e-04,  9.6867e-04,\n             1.4406e-03, -1.2140e-04, -1.1683e-04,  1.4808e-03, -2.7690e-05,\n             1.0227e-04, -8.4592e-04, -2.1958e-04, -1.2888e-04, -1.2857e-04,\n             4.7644e-04,  7.6590e-05,  8.9727e-04,  2.2062e-04, -5.0515e-04,\n            -1.7142e-04, -4.2248e-04,  4.8512e-04,  9.0778e-04,  2.1656e-04,\n            -3.8341e-04, -2.0158e-04, -2.1897e-04,  3.2824e-04, -1.3568e-03,\n             1.1631e-04,  2.3368e-03, -3.0177e-04, -3.7609e-04, -6.7153e-04,\n             1.4793e-03,  8.2671e-04, -2.0219e-04, -6.2997e-04, -2.9762e-04,\n             6.9893e-04, -1.0566e-03,  1.1882e-03, -1.0268e-04, -1.3598e-03,\n            -5.1339e-04,  4.0013e-05, -3.9510e-05, -2.6104e-04,  3.2022e-05,\n             5.4599e-05,  2.6152e-04,  7.9005e-04,  1.0327e-03,  1.9526e-04,\n             1.0668e-03,  3.5828e-04, -3.5528e-04, -1.9838e-04,  1.1788e-03,\n            -4.9391e-04,  3.4451e-04, -1.4986e-03,  1.3508e-03,  4.9379e-04,\n             3.1814e-04, -2.1249e-04,  1.1476e-03, -9.6349e-04, -2.9468e-04,\n             7.9226e-04,  1.1475e-03,  4.3271e-04, -8.2453e-05, -1.4171e-03,\n            -8.7442e-04,  6.7086e-04,  2.1857e-04, -5.9794e-04,  1.8419e-04,\n             6.6318e-04, -3.6329e-04, -1.0439e-03,  6.0681e-04,  1.0045e-03,\n            -1.3440e-03, -6.8281e-04, -1.0600e-03,  5.4489e-04,  2.5583e-04,\n            -8.9940e-04,  7.1530e-04, -2.5621e-04,  5.2115e-05,  5.8056e-04,\n            -5.2843e-04,  1.6480e-04, -2.1622e-03, -4.6579e-05,  1.6140e-06,\n            -7.4888e-04, -7.0112e-04,  1.1518e-04], device='cuda:0'),\n    'exp_avg_sq': tensor([2.6469e-05, 3.6386e-05, 2.5373e-05, 3.2117e-05, 1.3774e-05, 2.1230e-05,\n            2.0826e-05, 1.5374e-05, 2.6077e-05, 3.6297e-05, 2.0409e-05, 4.5244e-05,\n            2.1463e-05, 2.1422e-05, 7.3470e-05, 2.2509e-05, 1.9453e-05, 1.9913e-05,\n            1.9642e-05, 2.3979e-05, 3.1685e-05, 1.5768e-05, 3.4171e-05, 2.3709e-05,\n            1.3432e-05, 1.9514e-05, 4.4591e-05, 8.5055e-06, 3.0688e-05, 1.4404e-05,\n            2.5634e-05, 6.8350e-06, 1.9168e-05, 2.5944e-05, 1.6250e-05, 9.8796e-06,\n            3.4143e-05, 2.6082e-05, 4.3092e-05, 2.3004e-05, 3.1593e-05, 2.9652e-05,\n            3.7306e-05, 1.5687e-05, 1.4729e-05, 9.5026e-06, 1.4495e-05, 2.5733e-05,\n            2.2480e-05, 8.0066e-06, 4.6061e-05, 1.2804e-05, 2.1294e-05, 1.9750e-05,\n            3.5161e-05, 3.4812e-05, 3.3187e-05, 2.1438e-05, 4.0167e-05, 1.7227e-05,\n            2.3947e-05, 1.2357e-05, 2.1724e-05, 8.6812e-05, 2.2523e-05, 1.8775e-05,\n            2.0756e-05, 2.8601e-05, 2.3828e-05, 2.0185e-05, 2.9012e-05, 1.1675e-05,\n            2.5753e-05, 5.4824e-06, 2.0992e-05, 2.6665e-05, 1.2404e-05, 1.9186e-05,\n            2.0688e-05, 1.4703e-05, 1.2980e-05, 2.5420e-05, 2.5635e-05, 1.2907e-05,\n            3.0991e-05, 2.3212e-05, 1.7297e-05, 2.8780e-05, 2.8237e-05, 3.7675e-05,\n            7.8195e-06, 1.7094e-05, 4.1448e-05, 1.8645e-05, 1.2684e-05, 2.7399e-05,\n            2.3054e-05, 1.2990e-05, 7.6541e-05, 3.3084e-05, 3.8781e-05, 5.6460e-05,\n            1.5549e-05, 1.2925e-05, 1.8658e-05, 1.8627e-05, 9.9210e-06, 5.5655e-05,\n            9.0901e-06, 1.9534e-05, 3.0918e-05, 5.2381e-05, 2.2095e-05, 1.8261e-05,\n            2.0490e-05, 1.4157e-05, 1.9642e-05, 1.8232e-05, 1.5024e-05, 1.8433e-05,\n            2.3982e-05, 1.2841e-05, 4.8525e-05, 2.7840e-05, 2.2345e-05, 1.3634e-05,\n            1.9293e-05, 2.0083e-05], device='cuda:0')},\n   34: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[ 2.9679e-05]],\n    \n             [[ 3.2860e-05]],\n    \n             [[-1.2011e-05]],\n    \n             ...,\n    \n             [[ 8.9077e-06]],\n    \n             [[ 4.4136e-05]],\n    \n             [[-9.5712e-06]]],\n    \n    \n            [[[-7.9376e-06]],\n    \n             [[ 9.5684e-06]],\n    \n             [[-4.5013e-06]],\n    \n             ...,\n    \n             [[ 8.3339e-06]],\n    \n             [[ 1.6070e-05]],\n    \n             [[-5.7114e-06]]],\n    \n    \n            [[[-8.3728e-06]],\n    \n             [[-1.4290e-06]],\n    \n             [[-2.3478e-06]],\n    \n             ...,\n    \n             [[ 4.1939e-06]],\n    \n             [[-1.3116e-05]],\n    \n             [[-7.8895e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[ 1.2839e-06]],\n    \n             [[-5.4981e-07]],\n    \n             [[-1.1622e-07]],\n    \n             ...,\n    \n             [[ 1.0235e-06]],\n    \n             [[ 2.5213e-06]],\n    \n             [[ 1.6057e-06]]],\n    \n    \n            [[[ 7.6149e-06]],\n    \n             [[ 4.9269e-06]],\n    \n             [[-5.1319e-06]],\n    \n             ...,\n    \n             [[ 5.4782e-06]],\n    \n             [[ 1.0311e-05]],\n    \n             [[-5.8667e-06]]],\n    \n    \n            [[[-5.4300e-06]],\n    \n             [[ 6.7595e-06]],\n    \n             [[-1.1330e-05]],\n    \n             ...,\n    \n             [[ 7.8760e-06]],\n    \n             [[-1.2413e-05]],\n    \n             [[-2.6935e-06]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[1.7023e-08]],\n    \n             [[5.1335e-08]],\n    \n             [[4.2359e-08]],\n    \n             ...,\n    \n             [[5.1493e-08]],\n    \n             [[2.0150e-08]],\n    \n             [[2.4885e-08]]],\n    \n    \n            [[[2.8975e-09]],\n    \n             [[3.6896e-09]],\n    \n             [[3.1555e-09]],\n    \n             ...,\n    \n             [[4.0728e-09]],\n    \n             [[1.8739e-09]],\n    \n             [[1.7878e-09]]],\n    \n    \n            [[[5.3138e-09]],\n    \n             [[4.7827e-09]],\n    \n             [[2.3039e-09]],\n    \n             ...,\n    \n             [[3.7681e-09]],\n    \n             [[3.8452e-09]],\n    \n             [[4.3148e-09]]],\n    \n    \n            ...,\n    \n    \n            [[[1.6293e-10]],\n    \n             [[1.7123e-10]],\n    \n             [[9.0587e-11]],\n    \n             ...,\n    \n             [[1.0788e-10]],\n    \n             [[1.0371e-10]],\n    \n             [[1.1908e-10]]],\n    \n    \n            [[[1.0461e-09]],\n    \n             [[7.6407e-10]],\n    \n             [[3.3909e-10]],\n    \n             ...,\n    \n             [[9.3891e-10]],\n    \n             [[1.2453e-09]],\n    \n             [[7.3807e-10]]],\n    \n    \n            [[[9.8333e-09]],\n    \n             [[1.5375e-08]],\n    \n             [[4.2940e-08]],\n    \n             ...,\n    \n             [[2.2862e-08]],\n    \n             [[1.9279e-08]],\n    \n             [[2.0844e-08]]]], device='cuda:0')},\n   35: {'step': tensor(10550.),\n    'exp_avg': tensor([-7.6266e-05, -1.6837e-05, -3.4086e-05,  2.5615e-05, -3.8062e-04,\n            -3.3156e-05, -4.1474e-05, -2.1503e-06,  1.3052e-05, -5.1743e-05,\n             5.6899e-06,  5.6371e-06, -1.2826e-04,  2.4240e-05,  1.0093e-05,\n            -9.4476e-05,  6.7721e-06, -1.4610e-05, -8.1073e-06, -1.2849e-06,\n             6.2611e-06,  5.9102e-05,  9.8794e-07, -4.3795e-06,  5.3653e-05,\n            -1.8108e-04, -9.2971e-06, -1.0789e-06, -7.9597e-05, -1.0336e-05,\n            -2.5100e-05,  3.4495e-04, -5.7865e-07, -3.5898e-04,  7.4462e-06,\n             5.9912e-06,  5.3100e-05, -2.7859e-05, -2.7246e-06,  2.7080e-06,\n             4.3286e-05, -1.1273e-04, -3.4796e-06,  1.5989e-05,  1.2411e-04,\n            -6.3887e-06, -1.5062e-04,  4.9184e-07,  2.9659e-06, -2.1301e-05,\n             4.6795e-05, -8.3147e-06,  7.8039e-05,  8.2980e-05, -1.4995e-04,\n             7.2689e-05, -1.6000e-05, -8.0975e-06, -4.4669e-05, -1.9603e-05,\n            -4.2633e-05, -4.7337e-05,  4.1486e-05, -2.3541e-06,  2.0958e-04,\n            -9.7342e-05,  1.3940e-05,  7.1065e-06,  6.7202e-05, -9.6548e-06,\n             8.1935e-06,  1.2898e-04,  6.8796e-07,  9.7510e-06, -5.5138e-05,\n             5.3878e-06, -3.0440e-06, -8.9647e-05, -3.9335e-05, -3.0288e-05,\n             4.7178e-07, -4.1545e-05, -3.5257e-05,  4.1627e-06, -3.5975e-05,\n            -3.1471e-04, -1.0225e-05,  5.4593e-05,  1.2959e-04, -1.8496e-05,\n             6.6689e-06,  7.8468e-05, -8.4377e-05,  3.8067e-05,  5.0333e-05,\n            -1.8759e-05,  1.1642e-05,  8.7396e-05,  1.5835e-06,  1.0870e-04,\n             2.3236e-05, -5.5036e-07, -5.4017e-05, -4.9711e-05, -5.9941e-05,\n            -1.2799e-06,  1.3172e-04,  1.2372e-05, -7.9860e-06, -3.7797e-05,\n            -2.0083e-05,  1.4631e-05,  3.6817e-05, -5.4555e-06, -1.9350e-05,\n            -6.4479e-06, -2.6130e-05, -2.1966e-05,  1.7810e-04, -2.8784e-05,\n            -1.7062e-06,  4.4415e-06, -9.7386e-05, -1.2570e-04, -1.2180e-04,\n            -1.8205e-05, -5.7419e-05, -1.3677e-04, -8.8312e-06, -1.0062e-04,\n            -2.5498e-05,  4.7927e-07,  3.8535e-06, -6.6322e-05,  3.8226e-05,\n             6.9835e-05,  8.2207e-05,  1.9008e-04, -7.1009e-05,  5.1676e-05,\n             1.0338e-04, -1.7417e-05,  6.5757e-05,  3.4936e-04, -2.3527e-04,\n            -2.3119e-05, -7.7846e-06,  1.5545e-06,  4.3346e-05, -6.5336e-06,\n            -7.1423e-05,  3.6160e-05,  4.5021e-05,  5.0421e-06, -5.0496e-05,\n            -4.4599e-05, -2.6794e-05, -9.9592e-05,  3.0430e-06, -5.2081e-05,\n             7.5573e-06, -3.6327e-05,  6.4728e-06,  1.4108e-04,  9.0577e-06,\n            -3.6228e-05,  1.3109e-06,  6.5390e-05, -1.9690e-04,  1.8526e-06,\n             6.5124e-05,  1.9421e-05,  1.5386e-05,  4.8087e-05,  1.2107e-05,\n            -8.1202e-05,  1.7839e-05,  2.3273e-05, -5.5356e-05, -6.0221e-05,\n             3.4604e-05, -2.1809e-04, -5.7705e-05, -6.5748e-06, -1.3917e-05,\n            -3.8098e-05,  5.1448e-07, -1.1326e-04,  7.1336e-05,  1.0484e-05,\n            -4.5133e-05,  1.5137e-06,  2.0497e-05, -1.7915e-05,  1.0538e-05,\n             2.2799e-05, -5.8776e-05,  1.9356e-05, -6.2438e-06, -1.5117e-06,\n            -3.2794e-05, -3.7165e-05, -5.5091e-05, -6.0111e-05, -1.2681e-04,\n             2.1817e-05, -2.1021e-05, -2.0800e-05,  2.6573e-05, -1.7249e-05,\n             4.9266e-04,  8.7664e-07,  3.9944e-04,  9.4600e-07, -6.8278e-05,\n            -7.3281e-07,  2.7668e-05,  3.7765e-06, -5.1206e-05,  1.6780e-04,\n            -7.2737e-05,  4.7125e-05, -1.6124e-04, -1.3374e-04,  7.7378e-07,\n            -1.9396e-05, -1.2213e-04,  1.1558e-04,  6.6471e-07, -1.2819e-05,\n             1.1757e-05, -2.3345e-05,  1.2971e-05,  3.1563e-04, -1.8010e-05,\n             8.5393e-05, -7.3496e-06, -1.1450e-04,  3.4987e-05,  9.3916e-06,\n             3.7323e-05, -8.4170e-05, -2.4447e-07, -3.1334e-05,  1.4258e-05,\n            -7.0203e-06, -1.6728e-04, -1.9128e-05, -1.1812e-07,  8.0953e-05,\n            -1.8262e-04,  5.1598e-05, -1.4587e-06,  4.7324e-06, -3.7835e-04,\n            -4.4348e-06,  1.0432e-04, -2.1857e-05, -9.4192e-06, -7.9792e-06,\n             2.1644e-05, -1.1223e-04, -7.9097e-05, -2.9578e-06,  7.8833e-05,\n             1.1670e-04, -2.3593e-05, -4.4420e-05, -4.1279e-05, -1.3836e-07,\n            -2.4876e-06,  1.0063e-04, -4.2485e-06,  2.4863e-05,  1.3980e-04,\n            -1.4050e-06,  2.7978e-05, -1.0598e-04, -8.8285e-06, -1.6050e-04,\n             1.2485e-05, -1.8019e-05, -5.6321e-05, -4.9178e-05,  3.1706e-05,\n             2.5892e-05, -4.8325e-05, -1.6961e-04,  5.9417e-05,  1.8966e-05,\n             1.1407e-04,  3.0158e-07, -6.6321e-05,  4.9388e-05, -8.6777e-06,\n             1.3583e-04, -8.9609e-05, -1.4005e-07,  5.6954e-06, -6.0969e-06,\n             5.7064e-05,  1.0124e-06,  7.7821e-05,  1.2131e-06, -1.3027e-05,\n             2.2484e-04, -3.0454e-06,  1.3010e-05,  1.1646e-04,  7.2987e-05,\n            -9.0936e-05,  2.3696e-05, -1.6008e-05,  2.6282e-05, -5.6442e-05,\n             5.3340e-06,  9.2602e-05, -1.2126e-05, -3.4396e-05,  4.7466e-05,\n             4.1938e-06, -1.5210e-04, -2.7779e-07, -4.4075e-05,  1.8289e-05,\n             4.6089e-05,  2.7181e-05,  2.6983e-05, -1.9241e-04, -6.2966e-05,\n            -1.0585e-05, -1.7569e-06, -9.5995e-05, -3.6585e-06, -3.9144e-05,\n             2.5744e-05,  8.5329e-05,  2.4840e-04, -8.2057e-05, -2.5490e-05,\n             2.2683e-05, -2.7279e-05,  1.7897e-05, -1.3262e-04, -2.3793e-05,\n             1.2148e-04,  6.4805e-05, -2.8846e-05, -3.2429e-05,  2.5969e-06,\n            -2.4822e-04, -1.3658e-04,  9.5545e-05, -1.3856e-04,  4.2458e-05,\n            -1.1105e-06, -8.2227e-05,  3.0685e-06,  3.8212e-05, -4.7275e-06,\n            -4.0253e-06, -3.4026e-04, -4.5184e-05,  7.7775e-07, -1.5504e-04,\n            -3.0496e-05,  6.4728e-06, -1.0665e-04, -9.9515e-06, -1.3150e-05,\n             3.1245e-05,  4.4874e-04,  1.7378e-06, -7.6621e-05, -2.4334e-05,\n             2.4725e-05,  9.2238e-05,  7.2403e-05,  2.2922e-05,  4.9217e-05,\n            -2.2135e-05, -1.4600e-05, -5.5099e-06,  1.2597e-05,  1.3366e-04,\n             7.2673e-07, -7.7879e-05, -1.7919e-05, -9.6676e-06,  1.4111e-04,\n            -6.8017e-05, -2.3864e-07,  2.8709e-05,  6.3129e-05, -4.2686e-05,\n            -5.5873e-06, -5.1309e-05,  1.3446e-05,  5.0940e-05,  2.3771e-05,\n             2.2967e-05, -7.2987e-05,  1.3359e-05, -3.6786e-06, -5.8480e-07,\n            -3.0796e-06,  6.2977e-05, -2.5297e-05,  2.4733e-05,  8.6571e-05,\n            -9.6635e-05, -2.2931e-05, -3.9889e-05, -2.6875e-04,  3.0655e-05,\n            -3.3715e-05, -3.1962e-05, -3.2014e-05,  1.5730e-04,  5.4893e-05,\n             3.0847e-06,  7.5751e-06, -4.7512e-05,  1.4075e-05,  5.8192e-06,\n            -9.3418e-06, -3.8544e-05, -1.7653e-06, -2.8062e-06, -2.4560e-04,\n            -3.1410e-05,  5.9533e-06,  2.0773e-05, -1.7701e-04, -7.8879e-05,\n             1.5599e-06,  2.9166e-04,  4.6285e-05,  2.0921e-05, -8.3782e-05,\n            -5.0258e-05,  7.3381e-06, -1.9837e-05, -1.5401e-06,  1.3430e-05,\n             1.3327e-04, -9.3093e-08, -4.9909e-06,  2.4733e-06, -1.8220e-06,\n            -3.9799e-05,  1.6999e-05, -3.1187e-05,  2.6166e-06, -5.1895e-06,\n             1.2734e-06, -4.7247e-05, -2.1084e-05, -1.1281e-05, -1.7075e-04,\n             2.3474e-05,  6.3107e-05,  5.6207e-06,  5.0543e-05, -1.2021e-04,\n            -2.7640e-05,  6.8675e-05, -1.9494e-05, -1.6012e-07, -2.0423e-05,\n            -1.2769e-05, -1.1531e-05,  2.6125e-05,  2.8107e-05, -1.2655e-04,\n            -3.0933e-05, -4.9626e-05, -2.0959e-04, -1.2375e-06,  1.1123e-05,\n            -6.0620e-05, -5.6455e-06, -1.5690e-04, -5.0675e-06,  4.3063e-05,\n             9.4536e-06,  3.3133e-05, -5.3231e-06,  3.9800e-06, -8.2627e-05,\n            -4.7696e-05,  8.9379e-05,  7.1177e-05,  9.6463e-06, -2.7219e-05,\n             3.9869e-05, -1.9631e-05, -4.0328e-04, -1.7999e-06, -4.8704e-05,\n            -8.8198e-07, -1.6224e-06,  4.8691e-06,  1.7312e-05, -4.4549e-05,\n            -7.4435e-06,  3.3349e-06, -3.1702e-05,  1.4486e-06,  6.3349e-06,\n            -1.8530e-05, -4.9314e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([3.6789e-07, 4.7093e-09, 9.9147e-08, 4.2536e-08, 7.0805e-07, 1.3950e-07,\n            1.1611e-07, 2.2838e-08, 8.8536e-09, 9.8859e-08, 4.8053e-09, 5.7667e-09,\n            3.8824e-06, 1.0310e-07, 1.9531e-07, 1.9665e-07, 4.0449e-07, 2.7172e-08,\n            4.7162e-08, 6.2472e-10, 1.6636e-07, 1.7267e-07, 1.9972e-10, 3.0408e-09,\n            3.9974e-08, 3.4749e-07, 4.8818e-08, 6.5549e-10, 1.7377e-07, 3.8148e-09,\n            1.8686e-07, 1.4207e-06, 6.5434e-09, 9.3452e-07, 1.8231e-08, 7.4979e-09,\n            1.2609e-06, 4.8055e-08, 1.3471e-09, 8.6841e-09, 1.1185e-07, 2.5018e-07,\n            2.0904e-08, 2.0035e-07, 1.2367e-07, 4.2135e-08, 9.2826e-06, 3.4338e-10,\n            5.9275e-09, 3.8205e-08, 6.9074e-08, 2.3468e-07, 8.4422e-08, 1.8701e-07,\n            1.1554e-06, 1.6448e-07, 6.4271e-09, 1.8463e-07, 4.7026e-08, 1.7271e-07,\n            5.1706e-08, 6.0712e-07, 7.0512e-08, 1.4584e-08, 2.2323e-06, 1.2395e-06,\n            2.0415e-08, 2.0625e-07, 3.2759e-07, 9.2815e-07, 8.8312e-10, 4.0229e-07,\n            3.3045e-09, 7.0025e-08, 6.1039e-08, 7.5954e-08, 1.7705e-08, 2.1474e-07,\n            9.6637e-08, 2.8001e-07, 3.8880e-10, 1.3494e-08, 4.3445e-07, 3.3598e-09,\n            4.5082e-08, 1.4425e-06, 9.8191e-09, 6.7661e-08, 5.5796e-07, 8.8088e-08,\n            3.7530e-08, 5.8067e-08, 8.2423e-07, 2.4368e-07, 1.2475e-07, 1.2165e-07,\n            2.5793e-08, 6.0509e-07, 6.8570e-09, 1.2579e-07, 7.2109e-09, 2.7830e-08,\n            2.3920e-07, 4.6873e-08, 9.6060e-08, 5.2749e-08, 7.2472e-07, 5.8926e-08,\n            8.2352e-09, 2.0693e-07, 6.9185e-07, 1.5557e-08, 2.2758e-07, 2.9754e-09,\n            3.2588e-08, 1.3647e-08, 8.9935e-08, 2.4951e-08, 1.4916e-06, 3.7366e-08,\n            3.8262e-09, 2.3832e-07, 1.4340e-07, 1.9846e-06, 1.7631e-07, 1.7739e-08,\n            9.8426e-08, 3.2168e-07, 3.0099e-07, 7.0606e-07, 4.1754e-08, 2.9792e-10,\n            2.1249e-09, 1.2325e-07, 7.7797e-08, 1.4134e-07, 5.9928e-08, 5.8624e-07,\n            1.1311e-07, 1.4378e-07, 2.7848e-07, 7.2394e-08, 1.1573e-07, 1.6773e-06,\n            7.7450e-06, 6.2453e-08, 4.6139e-09, 1.9355e-10, 2.4123e-08, 5.7347e-09,\n            8.8629e-08, 2.4197e-07, 1.3276e-07, 7.4793e-09, 2.8115e-08, 2.0148e-07,\n            1.7870e-07, 5.2769e-07, 2.2216e-08, 6.0445e-08, 2.2068e-09, 3.0957e-07,\n            6.8156e-08, 8.5867e-07, 1.2567e-08, 4.2145e-08, 3.9361e-09, 5.8980e-08,\n            8.4020e-07, 1.6532e-09, 5.5319e-08, 6.1835e-08, 3.9353e-07, 2.1479e-07,\n            2.7039e-07, 1.7818e-06, 1.1304e-08, 1.9302e-07, 1.8443e-07, 2.3430e-07,\n            6.0841e-08, 2.5908e-07, 2.3867e-08, 2.2206e-10, 4.4245e-07, 2.7883e-08,\n            1.3212e-09, 3.5327e-07, 2.0687e-07, 1.1629e-07, 9.4270e-08, 2.0845e-10,\n            1.3867e-08, 3.1309e-08, 5.5875e-09, 1.2312e-08, 3.1468e-06, 8.6042e-09,\n            1.2394e-07, 1.9433e-08, 2.2209e-07, 6.3623e-08, 1.9076e-07, 1.2626e-07,\n            9.9566e-07, 3.3066e-09, 9.0807e-09, 1.6791e-08, 1.8506e-08, 5.0337e-08,\n            3.4954e-06, 3.3248e-10, 3.4917e-06, 6.3639e-08, 1.6371e-08, 1.1872e-10,\n            2.5646e-08, 2.6467e-07, 2.5679e-08, 1.6854e-06, 1.4645e-07, 6.2989e-07,\n            4.1496e-07, 5.0625e-07, 2.3340e-09, 4.3149e-09, 9.4507e-07, 6.3574e-08,\n            7.6888e-09, 1.9833e-08, 1.3686e-08, 1.5471e-08, 1.6718e-09, 3.5312e-06,\n            1.9638e-08, 5.3120e-08, 3.3774e-09, 1.6153e-07, 1.8296e-08, 5.2347e-08,\n            2.7533e-08, 1.2411e-07, 1.5951e-09, 3.2096e-08, 5.2777e-09, 3.5623e-08,\n            1.8679e-06, 1.7828e-07, 3.9047e-11, 4.6828e-07, 1.4509e-06, 6.9067e-08,\n            3.5499e-08, 8.7757e-08, 3.8064e-06, 3.6794e-08, 2.1631e-07, 1.2999e-08,\n            4.0071e-08, 4.3068e-09, 1.4490e-08, 1.2781e-06, 2.0702e-07, 2.4404e-09,\n            1.0029e-07, 1.5231e-07, 1.1598e-08, 1.0455e-07, 2.0126e-07, 2.1004e-10,\n            4.4219e-09, 7.4854e-07, 3.2271e-07, 6.4186e-08, 9.2795e-07, 4.1380e-08,\n            4.2689e-08, 2.6376e-07, 1.9156e-09, 3.3535e-07, 4.2933e-08, 1.0488e-06,\n            5.9694e-08, 1.3467e-07, 3.3780e-08, 5.9344e-08, 3.0435e-08, 2.1703e-06,\n            1.1678e-07, 4.0440e-07, 1.1577e-07, 1.1216e-08, 5.5604e-08, 1.0007e-06,\n            1.6683e-08, 2.3627e-06, 5.1445e-07, 2.1692e-11, 1.7539e-09, 5.4988e-09,\n            5.9147e-08, 2.0750e-10, 4.3524e-07, 2.4016e-08, 8.9421e-08, 2.0827e-06,\n            3.6117e-09, 4.7340e-08, 1.6967e-06, 3.5870e-07, 7.1033e-07, 3.1030e-08,\n            3.4585e-07, 8.3355e-08, 4.7171e-07, 8.6873e-09, 1.5520e-07, 2.1035e-08,\n            2.2984e-07, 2.3702e-08, 6.9427e-10, 1.1535e-06, 5.4256e-11, 3.1628e-08,\n            3.6482e-07, 3.1523e-07, 3.8535e-07, 1.3357e-07, 2.7066e-06, 8.6078e-08,\n            4.8785e-09, 1.8784e-09, 3.0023e-07, 4.7821e-08, 3.9264e-07, 2.3956e-08,\n            2.0070e-07, 6.1655e-07, 3.7536e-06, 3.7944e-08, 1.3724e-07, 3.4880e-08,\n            2.8303e-08, 2.4738e-07, 7.7346e-08, 1.3422e-06, 1.1667e-07, 3.5317e-08,\n            1.5702e-07, 1.2676e-09, 6.3208e-07, 3.1922e-06, 2.3832e-07, 1.7370e-07,\n            2.0172e-07, 1.1535e-09, 1.4526e-07, 9.3676e-09, 1.7149e-07, 2.7670e-09,\n            5.1746e-09, 1.0274e-06, 1.6810e-07, 2.1796e-10, 3.1710e-07, 8.1675e-08,\n            5.3437e-09, 1.2771e-07, 6.4141e-08, 3.8095e-09, 5.6323e-08, 2.4102e-06,\n            9.0574e-10, 9.5334e-08, 7.9473e-09, 2.6050e-08, 5.4769e-07, 6.9839e-07,\n            2.6933e-06, 2.5795e-08, 5.9415e-08, 1.2381e-08, 1.0587e-08, 1.8490e-08,\n            6.9658e-07, 4.4226e-09, 4.3733e-07, 1.3257e-08, 1.4360e-08, 2.1467e-06,\n            5.6588e-07, 2.1450e-09, 2.0532e-08, 1.7053e-07, 1.3898e-07, 4.0255e-08,\n            7.0435e-08, 1.2126e-07, 9.3906e-08, 7.8063e-07, 9.8216e-08, 2.0522e-06,\n            3.5255e-08, 4.6838e-08, 6.9599e-08, 1.7170e-09, 2.2910e-07, 1.4900e-08,\n            3.8863e-08, 1.3932e-07, 5.2060e-08, 5.0534e-08, 4.2708e-07, 3.0680e-06,\n            1.5777e-07, 5.1990e-08, 8.4653e-08, 2.2702e-07, 8.6955e-07, 9.4267e-08,\n            3.7250e-09, 6.6492e-08, 1.4758e-07, 1.7229e-08, 7.3761e-09, 2.2216e-08,\n            1.7580e-07, 7.5797e-09, 2.1576e-09, 8.8638e-07, 2.2107e-08, 5.0513e-08,\n            9.9488e-09, 3.8364e-07, 1.6752e-06, 1.1674e-09, 1.9302e-06, 6.5271e-07,\n            2.6636e-07, 1.6544e-07, 3.5258e-07, 1.1556e-07, 1.0413e-07, 4.7312e-10,\n            1.8823e-09, 3.8485e-07, 6.5968e-10, 4.5346e-09, 8.1573e-09, 1.7393e-10,\n            2.9178e-07, 1.4196e-08, 2.1163e-08, 1.1237e-09, 9.1178e-07, 4.4207e-10,\n            7.9364e-08, 8.7232e-09, 3.9291e-08, 1.0505e-06, 1.3997e-08, 1.4672e-06,\n            3.1862e-08, 7.2369e-08, 5.3863e-07, 6.3539e-08, 8.4517e-08, 1.4882e-08,\n            4.0478e-11, 3.3491e-08, 3.1836e-07, 6.7039e-09, 1.0715e-07, 8.7391e-09,\n            2.5359e-07, 7.9863e-08, 1.8726e-07, 7.7517e-07, 1.2811e-09, 6.2366e-08,\n            3.9376e-08, 6.0137e-09, 1.2962e-07, 2.1381e-08, 9.3526e-08, 2.6197e-08,\n            5.7868e-08, 7.1754e-08, 1.4067e-07, 1.5008e-06, 8.3125e-08, 6.4143e-08,\n            1.2036e-06, 4.8062e-09, 2.9660e-07, 1.6957e-06, 8.6371e-09, 2.3920e-06,\n            6.6597e-08, 8.0047e-08, 6.1036e-10, 7.3824e-08, 3.9930e-09, 2.9474e-09,\n            7.6889e-07, 2.1837e-08, 2.3486e-07, 3.1014e-08, 9.3779e-10, 1.8905e-09,\n            1.3448e-08, 4.3391e-07], device='cuda:0')},\n   36: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[-4.1371e-05, -7.3269e-05,  6.0149e-06],\n              [ 5.5566e-05,  3.2343e-05,  2.8228e-05],\n              [ 1.2690e-04,  1.2632e-04,  1.1419e-04]]],\n    \n    \n            [[[ 9.6220e-05,  9.0236e-05,  7.0910e-05],\n              [ 1.1781e-06, -5.7634e-06,  3.2062e-05],\n              [-4.2471e-05, -6.3926e-05, -4.6929e-05]]],\n    \n    \n            [[[ 9.6681e-06,  5.4416e-07,  3.6713e-08],\n              [-2.7019e-06, -1.3507e-05,  2.6292e-06],\n              [ 4.1928e-06, -4.1001e-06,  1.5655e-05]]],\n    \n    \n            ...,\n    \n    \n            [[[ 3.6016e-06,  8.2983e-06,  7.8935e-06],\n              [ 1.3120e-05,  1.9621e-05,  5.1528e-06],\n              [ 1.3942e-05,  5.9205e-06, -5.5071e-06]]],\n    \n    \n            [[[ 1.0616e-05,  1.1382e-06, -3.3545e-05],\n              [-1.1190e-05,  1.7927e-05,  5.1824e-06],\n              [-2.1390e-05, -1.1282e-05, -2.7806e-06]]],\n    \n    \n            [[[ 2.0841e-05,  3.3580e-05,  3.1506e-05],\n              [-3.6424e-06,  8.4680e-06,  2.3157e-05],\n              [ 2.1600e-05,  2.6624e-05,  1.1109e-05]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[2.7531e-07, 3.5417e-07, 3.3428e-07],\n              [1.6104e-07, 1.5533e-07, 1.0379e-07],\n              [2.3132e-07, 3.1924e-07, 2.7411e-07]]],\n    \n    \n            [[[2.4173e-08, 2.6429e-08, 1.8839e-08],\n              [1.2661e-08, 1.1074e-08, 1.0092e-08],\n              [1.7483e-08, 3.1177e-08, 1.7603e-08]]],\n    \n    \n            [[[1.1813e-08, 1.3131e-08, 1.6212e-08],\n              [1.5769e-08, 2.1280e-08, 1.4891e-08],\n              [1.4234e-08, 1.7967e-08, 4.4107e-08]]],\n    \n    \n            ...,\n    \n    \n            [[[2.2621e-09, 7.5342e-09, 2.0995e-09],\n              [5.5341e-09, 7.2836e-09, 2.5900e-09],\n              [7.1766e-09, 3.9981e-09, 3.6647e-09]]],\n    \n    \n            [[[8.2459e-09, 1.0945e-08, 8.1515e-09],\n              [1.5840e-08, 8.9781e-09, 1.3611e-08],\n              [1.6068e-08, 1.0332e-08, 2.1036e-08]]],\n    \n    \n            [[[7.6972e-08, 4.6536e-08, 6.0649e-08],\n              [1.4720e-07, 1.5874e-07, 1.0201e-07],\n              [4.2713e-08, 7.6113e-08, 1.6351e-07]]]], device='cuda:0')},\n   37: {'step': tensor(10550.),\n    'exp_avg': tensor([ 1.0104e-04,  1.7764e-04,  6.2352e-05, -4.6791e-05,  3.2499e-04,\n            -5.9957e-05,  4.1177e-05,  3.8731e-06, -4.8822e-05,  8.6818e-05,\n            -1.3352e-06, -3.5759e-05, -8.9078e-05,  2.9255e-05, -1.4397e-05,\n            -1.6768e-04, -9.4099e-06,  6.0170e-05,  2.4315e-05, -2.0568e-05,\n             9.8081e-06, -9.3987e-05,  2.8481e-05,  1.3316e-05,  1.8980e-04,\n            -1.7154e-04,  1.4411e-05, -6.2537e-05, -1.2794e-04,  9.5305e-05,\n             2.4698e-05, -2.6811e-04,  9.0418e-07, -3.5940e-04, -1.7072e-05,\n             2.8398e-05, -4.1089e-05,  7.5415e-05, -8.4570e-06, -2.0179e-05,\n             5.9258e-05,  1.2097e-04, -1.1026e-05,  3.3115e-05, -2.7893e-04,\n             1.7704e-05,  6.4726e-05,  6.9863e-06, -2.5625e-05,  7.4656e-05,\n             1.0029e-04,  1.4103e-05, -1.8188e-04,  1.0457e-04,  1.1009e-04,\n            -1.3778e-04, -1.2207e-04,  1.1277e-05,  1.2796e-04,  3.9038e-05,\n             9.0318e-05, -7.3945e-05,  6.5846e-05,  8.9182e-06,  1.6168e-04,\n            -9.0105e-05, -3.2171e-05, -1.1011e-05,  6.8193e-05,  7.9251e-06,\n            -6.4900e-05,  1.7782e-04, -6.7090e-06,  1.5065e-05,  1.5557e-04,\n            -1.1332e-05, -1.1974e-05,  1.4567e-04, -6.4782e-05,  3.5131e-05,\n             4.0047e-05,  2.1014e-04, -2.0815e-05,  6.9798e-05,  9.0083e-05,\n            -2.4588e-04,  3.9877e-05,  1.2167e-04, -1.0096e-04, -2.7300e-05,\n             1.6687e-05,  1.8982e-04,  7.4771e-05, -4.0359e-05, -1.4822e-04,\n             2.6482e-05, -2.3814e-05, -6.9763e-05, -6.4963e-06,  1.9661e-04,\n            -1.2660e-04,  6.8860e-06, -7.2598e-05, -1.3788e-04, -8.4561e-05,\n            -4.7199e-06, -1.0511e-04,  2.4767e-05,  6.2733e-05, -5.0647e-05,\n             2.2129e-05, -1.1452e-04, -5.7384e-05,  8.7447e-05,  3.8343e-05,\n             1.4273e-05, -4.6828e-05, -7.3512e-05,  1.5492e-04,  8.5250e-05,\n             1.1695e-05,  4.8158e-06,  1.0237e-04, -1.0918e-04,  1.9273e-04,\n            -6.0890e-05,  8.2597e-05,  1.3984e-04, -3.1416e-05,  9.5388e-05,\n             7.3695e-05, -1.8300e-06, -1.5296e-05, -1.1784e-04,  4.8260e-05,\n             1.0450e-04,  1.4101e-04,  2.7251e-04, -1.3123e-04,  5.4731e-05,\n             1.5463e-04,  2.7469e-05,  8.6917e-05, -2.5706e-04,  1.3864e-04,\n             2.2932e-05,  5.2608e-05, -1.0348e-05,  1.7956e-04,  5.2830e-05,\n             1.7826e-04,  3.3710e-05,  6.4371e-05,  1.2889e-05,  1.6192e-04,\n             4.6214e-05,  4.0585e-05,  5.9832e-05, -7.4628e-06,  1.5228e-04,\n            -5.5296e-05,  7.0121e-05,  2.3520e-05, -1.2523e-04, -2.8825e-05,\n             9.1068e-05,  6.9869e-06,  1.6527e-04,  1.9296e-04,  1.9742e-05,\n            -1.6118e-04,  3.8223e-05, -4.1870e-05,  7.8311e-05,  1.7357e-05,\n            -5.5590e-05,  1.6779e-04,  2.5267e-05,  8.2491e-05,  8.8073e-05,\n            -6.6259e-05,  2.6187e-04,  1.5259e-04, -8.0950e-05, -1.4258e-05,\n            -1.0188e-04,  7.3074e-06, -1.9349e-04,  7.1603e-05,  2.5295e-05,\n             6.2057e-05, -2.0020e-05, -5.6688e-05, -4.9385e-05, -8.4890e-05,\n            -7.9969e-05,  5.5453e-06,  8.1777e-05, -8.4073e-06, -1.4844e-05,\n             4.2342e-05, -7.2323e-05,  8.2536e-05, -1.6251e-04,  1.0812e-04,\n             1.2676e-04,  1.0031e-04,  1.4103e-04,  9.0461e-05,  4.7928e-05,\n             2.9392e-04, -1.2813e-05, -2.6008e-04, -1.0109e-05,  3.8505e-04,\n            -4.2447e-06,  6.1795e-05,  8.5185e-06,  1.1974e-04,  1.0267e-04,\n             1.3460e-04, -9.3267e-05,  1.8637e-04,  1.0345e-04, -3.2825e-06,\n            -1.6696e-04,  9.6179e-05,  2.6191e-04, -9.3976e-06, -5.1527e-05,\n             3.3944e-05, -6.8135e-05,  1.6559e-04, -1.7682e-04,  4.6116e-05,\n             2.0451e-04,  7.4108e-05, -1.6841e-04,  1.7694e-04, -2.1235e-05,\n            -9.1413e-05,  1.5102e-04,  1.1646e-05,  8.8399e-05,  6.6844e-05,\n            -1.7679e-05, -2.0225e-04,  2.7772e-05,  1.8494e-06, -7.3845e-05,\n             1.3830e-04, -1.6568e-04, -1.0804e-06,  3.1338e-05,  2.5596e-04,\n            -1.5977e-05,  1.4734e-04, -1.0635e-04, -4.2592e-05, -9.3297e-05,\n             5.8912e-05,  8.4540e-05,  1.0866e-04,  5.1436e-05,  1.3712e-04,\n             2.0257e-04, -6.0275e-05,  6.3450e-05, -6.0630e-05,  6.6990e-06,\n             1.5035e-05,  1.0538e-04, -1.9811e-05,  3.3712e-05,  1.0366e-04,\n            -5.9945e-06, -6.5059e-05, -1.3566e-04,  2.0843e-05,  2.0612e-04,\n             1.1396e-05,  1.4379e-05,  1.1951e-04, -8.9052e-05,  8.2662e-05,\n             4.4818e-05, -1.5598e-04, -1.6438e-04,  9.9522e-05, -1.5644e-05,\n             2.1360e-04, -3.2440e-06, -1.4323e-04,  4.0780e-05, -2.8448e-05,\n            -1.2375e-04, -9.2753e-05,  7.9966e-06,  2.2836e-05,  2.1145e-05,\n             1.3160e-04,  8.5931e-06,  6.2334e-05,  1.2556e-06, -4.1316e-06,\n            -1.2119e-04, -2.6163e-05,  3.0444e-05,  9.2987e-05, -8.2958e-05,\n             7.7045e-05,  6.3047e-05,  1.6700e-05, -4.7695e-05,  3.0338e-05,\n            -2.7668e-05,  1.0673e-04,  2.8374e-05,  4.3232e-05,  1.1365e-04,\n             4.6340e-05, -1.5485e-04,  8.2277e-06,  1.9542e-04,  1.1118e-05,\n             4.6538e-05,  3.2955e-05, -6.4170e-05,  1.2040e-04,  1.5586e-04,\n             3.0966e-05, -6.1098e-06,  1.1891e-04, -6.8127e-06, -5.1906e-05,\n             5.2669e-05, -1.2996e-04,  2.5538e-04, -4.9985e-05,  7.7611e-05,\n            -2.7381e-05,  5.4447e-05,  4.8942e-05, -1.9458e-04, -4.1905e-05,\n            -1.1453e-04,  1.1373e-04,  8.4469e-05,  3.4920e-05, -2.5640e-05,\n             3.0447e-04,  8.2584e-05, -1.2787e-04,  2.2005e-04,  6.1172e-05,\n             1.4842e-05, -1.3617e-04,  4.6327e-05,  3.4217e-05,  1.2810e-05,\n            -3.9828e-05, -2.8106e-04, -8.2111e-05, -2.0951e-05, -2.2501e-04,\n             6.3546e-05,  1.8808e-05,  1.1027e-04, -2.1479e-05,  7.2935e-05,\n            -7.2954e-05, -3.1259e-04,  1.8452e-05,  1.2501e-04, -9.2017e-05,\n             8.8901e-05, -8.4508e-05,  8.5990e-05, -6.3713e-05,  1.4754e-04,\n             4.0452e-05,  3.4381e-05,  1.4432e-05, -2.8114e-05,  1.2355e-04,\n             2.6178e-06,  8.6182e-05, -1.0106e-04,  2.2854e-05, -1.0847e-04,\n            -6.9016e-05, -4.5648e-06,  8.5650e-05,  7.8115e-05, -7.2398e-05,\n             1.2675e-05,  8.9374e-05, -1.7729e-05,  5.1827e-05, -2.6811e-05,\n             5.0305e-05, -5.1185e-05,  2.8226e-05,  1.3074e-05, -1.2141e-05,\n            -1.9521e-05, -6.1864e-05,  9.3392e-05,  6.9516e-05,  1.1529e-04,\n            -2.0239e-04,  5.4737e-05,  4.6202e-05,  1.6318e-04,  2.3297e-05,\n             4.9572e-05,  4.7610e-05, -4.1268e-05, -1.3111e-04, -7.3014e-05,\n            -1.6691e-05, -1.3495e-05, -6.0291e-05, -2.0863e-05,  1.9163e-05,\n             2.8271e-05, -6.6380e-05, -1.0709e-05,  2.0525e-05,  1.7363e-04,\n             1.5150e-04, -3.9599e-05,  7.9902e-05,  2.0979e-04,  3.9699e-05,\n            -5.7225e-05,  2.6783e-04, -4.6829e-05, -2.4272e-05, -1.0695e-04,\n            -6.2297e-05, -8.9645e-06, -5.1221e-05, -8.4128e-06,  1.0861e-04,\n            -1.9443e-04,  8.1822e-07, -3.2515e-05, -9.2485e-06,  6.9570e-06,\n            -4.6826e-05,  1.3677e-04, -7.9007e-05, -1.9453e-05,  1.3218e-05,\n             5.2716e-06, -1.4528e-04,  4.8491e-05,  2.2760e-05,  1.2822e-04,\n            -9.8005e-05, -5.0962e-05, -9.1292e-06,  1.1413e-04, -1.0775e-04,\n            -5.6937e-05, -1.2133e-04,  6.3494e-05, -8.0104e-06,  6.2826e-05,\n             5.8449e-06,  3.6505e-05, -3.6681e-05, -1.2794e-04,  1.6120e-04,\n             6.8260e-05, -6.2933e-05, -1.6839e-04,  2.7860e-05, -2.2250e-05,\n             1.2901e-04,  4.4021e-05,  2.9513e-04,  2.3985e-05,  7.4487e-05,\n             2.1975e-05, -9.7582e-05,  9.6791e-06,  3.1796e-05,  7.1092e-05,\n             7.9741e-05, -2.0923e-04,  5.3828e-05,  4.9269e-05,  2.2633e-05,\n             2.4003e-05,  8.3031e-05,  2.5397e-04, -8.2412e-06, -7.5294e-05,\n             1.0862e-05, -1.0169e-05,  1.2918e-05,  6.1073e-05, -4.3367e-05,\n            -1.3289e-05, -4.5817e-06, -5.5487e-05,  1.8160e-05, -3.2416e-05,\n             8.1512e-05,  5.8180e-05], device='cuda:0'),\n    'exp_avg_sq': tensor([4.3735e-07, 2.6527e-07, 2.2359e-07, 1.1457e-07, 4.7882e-07, 3.5910e-07,\n            1.4461e-07, 2.3194e-07, 9.1038e-08, 2.9734e-07, 1.5199e-07, 1.4753e-07,\n            1.9535e-06, 1.3994e-07, 2.4829e-07, 4.1902e-07, 3.8936e-07, 2.7777e-07,\n            2.0789e-07, 2.2850e-07, 9.4779e-07, 3.3753e-07, 4.5926e-08, 1.8908e-08,\n            2.6215e-07, 3.7737e-07, 1.2950e-07, 1.9856e-07, 3.4416e-07, 9.1592e-08,\n            1.8910e-07, 7.8297e-07, 5.6738e-08, 8.9864e-07, 9.3695e-08, 1.4084e-07,\n            7.2858e-07, 2.1601e-07, 8.7611e-09, 2.5843e-07, 1.9511e-07, 2.4662e-07,\n            1.3772e-07, 2.4351e-07, 5.2515e-07, 1.4354e-07, 1.8388e-06, 6.7199e-08,\n            1.2284e-07, 4.1766e-07, 2.5945e-07, 2.6163e-07, 3.0859e-07, 2.2623e-07,\n            5.5712e-07, 5.4107e-07, 1.2333e-07, 2.8081e-07, 2.2214e-07, 3.2340e-07,\n            1.9128e-07, 5.4589e-07, 1.4909e-07, 1.0622e-07, 1.3308e-06, 7.6529e-07,\n            8.4407e-08, 4.0288e-07, 3.2318e-07, 5.4089e-07, 4.2446e-08, 5.3665e-07,\n            5.4940e-08, 1.6581e-07, 4.3425e-07, 1.4393e-07, 1.4603e-07, 8.8519e-07,\n            3.2383e-07, 5.6037e-07, 5.0551e-08, 3.5897e-07, 2.6620e-07, 1.2101e-07,\n            2.0932e-07, 9.6234e-07, 9.0413e-08, 2.9259e-07, 3.4810e-07, 2.2255e-07,\n            1.7795e-07, 4.6971e-07, 5.9371e-07, 1.9837e-07, 7.0515e-07, 2.2991e-07,\n            1.7651e-07, 7.2773e-07, 1.1497e-07, 4.4840e-07, 1.4656e-07, 1.3833e-07,\n            3.4133e-07, 2.0126e-07, 1.7817e-07, 2.3425e-07, 4.5305e-07, 3.0187e-07,\n            9.1866e-08, 2.9034e-07, 4.2046e-07, 5.7507e-07, 3.4554e-07, 1.0405e-07,\n            9.5031e-08, 6.4450e-08, 2.6092e-07, 2.3590e-07, 1.0984e-06, 2.1226e-07,\n            1.7098e-07, 2.6310e-07, 1.5000e-07, 1.4499e-06, 3.6470e-07, 1.4561e-07,\n            1.8888e-07, 4.3476e-07, 5.3781e-07, 5.3688e-07, 2.2923e-07, 4.8398e-09,\n            2.6228e-08, 2.4127e-07, 1.3450e-07, 2.9534e-07, 1.7272e-07, 7.5229e-07,\n            5.2138e-07, 2.7197e-07, 4.1966e-07, 1.9934e-07, 1.9368e-07, 9.0260e-07,\n            3.2880e-06, 2.8939e-07, 9.0248e-08, 5.4959e-09, 2.6907e-07, 1.3522e-07,\n            3.6914e-07, 2.7878e-07, 1.5734e-07, 1.8804e-07, 2.5527e-07, 1.9943e-07,\n            3.7719e-07, 2.9597e-07, 1.5628e-07, 2.4946e-07, 1.9288e-07, 5.0744e-07,\n            3.4900e-07, 6.4564e-07, 1.3494e-07, 1.4754e-07, 2.7860e-08, 3.0621e-07,\n            6.6811e-07, 1.0739e-07, 2.6487e-07, 2.1479e-07, 4.5595e-07, 5.3422e-07,\n            4.0058e-07, 8.8270e-07, 2.1276e-07, 2.9112e-07, 2.4495e-07, 3.7471e-07,\n            1.7472e-07, 4.1022e-07, 1.7178e-07, 6.3777e-08, 4.3906e-07, 2.6067e-07,\n            1.3414e-07, 6.3622e-07, 2.2656e-07, 3.3434e-07, 1.3602e-07, 7.5868e-09,\n            6.3266e-08, 1.8549e-07, 8.2291e-08, 1.1827e-07, 1.6664e-06, 1.0663e-07,\n            2.8091e-07, 3.2295e-07, 2.7162e-07, 2.3022e-07, 3.6006e-07, 8.0522e-07,\n            8.2014e-07, 8.6162e-08, 1.0625e-07, 2.7337e-07, 1.9215e-07, 2.5272e-07,\n            1.4195e-06, 5.9005e-08, 1.5123e-06, 3.7487e-07, 9.2702e-07, 4.2224e-09,\n            1.1056e-07, 6.9430e-07, 9.9925e-08, 7.0476e-07, 3.8534e-07, 7.5005e-07,\n            4.1216e-07, 2.7929e-07, 2.6751e-08, 2.3314e-07, 4.8251e-07, 3.9765e-07,\n            2.6430e-07, 1.8488e-07, 4.7469e-08, 9.9699e-08, 4.1533e-07, 1.2306e-06,\n            1.0837e-07, 2.2531e-07, 1.1967e-07, 3.4061e-07, 1.8371e-07, 2.2041e-07,\n            2.0858e-07, 2.8395e-07, 9.7962e-08, 1.7097e-07, 8.4157e-08, 1.6598e-07,\n            1.4987e-06, 3.0524e-07, 8.1474e-09, 4.0179e-07, 9.6974e-07, 6.7641e-07,\n            1.1449e-07, 3.9321e-07, 1.6311e-06, 6.5842e-07, 4.7124e-07, 2.2014e-07,\n            3.8139e-07, 2.7902e-07, 8.7700e-08, 6.3109e-07, 3.0101e-07, 1.0975e-07,\n            3.2517e-07, 4.7307e-07, 6.9454e-08, 2.0293e-07, 3.3820e-07, 6.0587e-09,\n            5.8577e-08, 5.6069e-07, 5.5880e-07, 1.7611e-07, 4.8618e-07, 4.7044e-07,\n            2.6344e-07, 3.4715e-07, 1.7962e-08, 4.3939e-07, 3.5744e-07, 6.4964e-07,\n            2.4365e-07, 3.8219e-07, 2.1892e-07, 1.4336e-07, 2.6251e-07, 1.3672e-06,\n            2.7425e-07, 3.3601e-07, 2.9508e-07, 1.0852e-07, 2.6769e-07, 7.2395e-07,\n            1.3180e-07, 1.1679e-06, 5.5610e-07, 6.1151e-09, 1.6018e-08, 4.6026e-08,\n            3.0442e-07, 3.7859e-09, 2.8253e-07, 2.5728e-07, 3.0727e-07, 8.1391e-07,\n            8.2820e-08, 2.5317e-07, 9.0743e-07, 5.2756e-07, 4.5357e-07, 1.6003e-07,\n            3.5735e-07, 3.2829e-07, 2.3423e-07, 1.2864e-07, 1.9500e-07, 9.3797e-08,\n            3.9775e-07, 1.0096e-07, 7.2964e-08, 7.6247e-07, 1.1132e-08, 5.1062e-07,\n            4.0799e-07, 1.0318e-06, 4.9177e-07, 3.0090e-07, 1.0725e-06, 3.4762e-07,\n            3.0181e-08, 8.3282e-08, 4.4237e-07, 1.3057e-07, 6.7436e-07, 8.7947e-08,\n            3.6644e-07, 4.8795e-07, 1.5828e-06, 2.5955e-07, 2.6498e-07, 1.1616e-07,\n            1.9674e-07, 4.3093e-07, 1.7183e-07, 1.0611e-06, 4.1610e-07, 2.1051e-07,\n            3.2258e-07, 6.4546e-08, 9.4612e-07, 1.3731e-06, 3.3380e-07, 5.4100e-07,\n            3.6058e-07, 5.9752e-08, 2.7282e-07, 3.4203e-07, 1.7820e-07, 1.8768e-08,\n            1.9215e-07, 7.6060e-07, 3.7060e-07, 8.4734e-08, 6.0087e-07, 3.7097e-07,\n            6.8341e-08, 1.4323e-07, 1.8138e-07, 6.6817e-08, 1.8172e-07, 1.1769e-06,\n            5.9355e-08, 1.6746e-07, 8.7217e-08, 1.9817e-07, 4.5240e-07, 7.8245e-07,\n            1.5974e-06, 1.5104e-07, 1.4543e-07, 1.2068e-07, 9.0600e-08, 6.4944e-08,\n            6.0735e-07, 4.7251e-08, 5.0108e-07, 4.2671e-07, 9.8118e-08, 1.1396e-06,\n            6.3538e-07, 7.5251e-08, 1.6980e-07, 2.4295e-07, 2.4326e-07, 1.5116e-07,\n            1.6551e-07, 1.7959e-07, 1.5188e-07, 9.1534e-07, 4.5427e-07, 7.7205e-07,\n            1.8419e-07, 1.8361e-07, 5.1989e-07, 1.1810e-07, 2.4056e-07, 1.4003e-07,\n            2.0534e-07, 2.3807e-07, 5.0702e-07, 2.0981e-07, 4.1922e-07, 1.0117e-06,\n            2.1382e-07, 1.1595e-07, 1.6884e-07, 3.2470e-07, 7.2368e-07, 1.7480e-07,\n            3.1270e-08, 1.9785e-07, 1.9280e-07, 2.5681e-07, 6.9969e-08, 1.4344e-07,\n            4.7288e-07, 1.1962e-07, 3.7446e-08, 5.6517e-07, 2.2339e-07, 2.7395e-07,\n            2.6787e-07, 5.1595e-07, 7.5321e-07, 1.5362e-07, 1.4540e-06, 5.8959e-07,\n            3.8592e-07, 2.5568e-07, 4.1423e-07, 1.7292e-07, 2.8186e-07, 2.7866e-07,\n            9.7538e-08, 5.4861e-07, 5.5556e-09, 1.0159e-07, 1.9144e-07, 6.8308e-08,\n            2.9726e-07, 4.2855e-07, 1.8763e-07, 6.2611e-08, 5.6890e-07, 7.5627e-08,\n            3.4942e-07, 9.6255e-08, 1.2769e-07, 7.2593e-07, 1.8743e-07, 9.1485e-07,\n            1.1572e-07, 3.1921e-07, 3.9700e-07, 2.0866e-07, 2.5893e-07, 1.5193e-07,\n            2.6221e-09, 1.8962e-07, 1.9426e-07, 5.1089e-08, 1.7517e-07, 2.0928e-07,\n            5.3379e-07, 3.4083e-07, 2.6579e-07, 5.3771e-07, 1.7835e-07, 1.6655e-07,\n            1.8155e-07, 1.5545e-07, 5.0208e-07, 3.1936e-07, 2.0103e-07, 9.3990e-08,\n            4.2076e-07, 1.8701e-07, 3.6219e-07, 9.5604e-07, 2.4472e-07, 3.5875e-07,\n            1.4151e-06, 8.4433e-08, 3.1529e-07, 7.3700e-07, 1.4586e-07, 1.1741e-06,\n            4.7758e-07, 1.5122e-07, 1.2835e-08, 3.1150e-07, 2.6200e-08, 2.6817e-08,\n            6.9584e-07, 7.3725e-08, 3.3580e-07, 3.4758e-07, 1.7634e-08, 5.3252e-08,\n            1.7340e-07, 4.6516e-07], device='cuda:0')},\n   38: {'step': tensor(10550.),\n    'exp_avg': tensor([[[[ 5.8262e-05]],\n    \n             [[-2.5767e-06]],\n    \n             [[-3.2723e-07]],\n    \n             ...,\n    \n             [[-2.7754e-06]],\n    \n             [[ 2.0856e-07]],\n    \n             [[ 2.1314e-05]]],\n    \n    \n            [[[-3.5517e-05]],\n    \n             [[ 8.5742e-06]],\n    \n             [[-5.1338e-06]],\n    \n             ...,\n    \n             [[-4.6609e-06]],\n    \n             [[-2.3283e-06]],\n    \n             [[-1.4976e-05]]],\n    \n    \n            [[[ 1.6856e-05]],\n    \n             [[-2.5643e-06]],\n    \n             [[ 4.3353e-06]],\n    \n             ...,\n    \n             [[ 4.9745e-06]],\n    \n             [[ 1.3697e-06]],\n    \n             [[-1.8939e-06]]],\n    \n    \n            ...,\n    \n    \n            [[[ 6.3853e-05]],\n    \n             [[ 1.1510e-05]],\n    \n             [[ 9.3514e-08]],\n    \n             ...,\n    \n             [[-1.5322e-06]],\n    \n             [[-3.6504e-07]],\n    \n             [[ 2.4893e-05]]],\n    \n    \n            [[[-8.4433e-05]],\n    \n             [[-1.2850e-06]],\n    \n             [[ 5.5646e-06]],\n    \n             ...,\n    \n             [[-2.1991e-06]],\n    \n             [[ 8.7358e-07]],\n    \n             [[-2.8965e-05]]],\n    \n    \n            [[[ 6.5231e-05]],\n    \n             [[ 1.3068e-05]],\n    \n             [[-3.7223e-06]],\n    \n             ...,\n    \n             [[ 3.4155e-06]],\n    \n             [[-6.3885e-06]],\n    \n             [[ 3.8093e-05]]]], device='cuda:0'),\n    'exp_avg_sq': tensor([[[[4.5038e-08]],\n    \n             [[1.2771e-09]],\n    \n             [[1.9586e-09]],\n    \n             ...,\n    \n             [[2.4473e-10]],\n    \n             [[5.6661e-10]],\n    \n             [[2.0472e-08]]],\n    \n    \n            [[[2.8837e-08]],\n    \n             [[1.8506e-09]],\n    \n             [[3.8534e-09]],\n    \n             ...,\n    \n             [[1.9258e-10]],\n    \n             [[6.8519e-10]],\n    \n             [[1.5605e-08]]],\n    \n    \n            [[[2.6820e-08]],\n    \n             [[2.0292e-09]],\n    \n             [[5.6843e-09]],\n    \n             ...,\n    \n             [[3.6706e-10]],\n    \n             [[1.1110e-09]],\n    \n             [[1.6329e-08]]],\n    \n    \n            ...,\n    \n    \n            [[[4.3824e-08]],\n    \n             [[1.0510e-09]],\n    \n             [[3.5668e-09]],\n    \n             ...,\n    \n             [[3.7187e-10]],\n    \n             [[9.5827e-10]],\n    \n             [[1.9159e-08]]],\n    \n    \n            [[[8.1560e-08]],\n    \n             [[3.1583e-09]],\n    \n             [[2.0954e-09]],\n    \n             ...,\n    \n             [[5.4497e-10]],\n    \n             [[1.1116e-09]],\n    \n             [[4.2213e-08]]],\n    \n    \n            [[[9.0956e-08]],\n    \n             [[5.2898e-09]],\n    \n             [[6.3676e-09]],\n    \n             ...,\n    \n             [[3.9798e-10]],\n    \n             [[2.2424e-09]],\n    \n             [[3.7358e-08]]]], device='cuda:0')},\n   39: {'step': tensor(10550.),\n    'exp_avg': tensor([ 8.2444e-05, -4.1875e-06,  9.4597e-05,  5.8760e-05, -7.1530e-05,\n            -9.0314e-05, -9.5055e-06,  1.5649e-05,  6.7932e-05, -1.1271e-04,\n            -4.7743e-05,  2.3911e-04, -2.3049e-05,  2.5190e-05, -3.8316e-05,\n             8.6367e-05, -6.0034e-05, -1.3393e-05, -2.3840e-06,  5.4341e-05,\n             3.1821e-05, -2.0490e-04,  1.9784e-04, -9.0721e-05, -5.4258e-05,\n            -2.5933e-05,  1.2693e-05, -2.1825e-04, -8.9427e-05,  1.1276e-04,\n            -5.8355e-05, -1.7212e-04, -2.2210e-04, -2.6325e-05,  5.2794e-05,\n            -5.7544e-05, -2.0695e-05,  2.7890e-05, -1.5386e-04,  1.7986e-04,\n            -5.2095e-06,  5.1091e-05, -4.8311e-05, -4.9891e-05, -1.4342e-04,\n            -2.5839e-05,  1.9140e-04,  2.5656e-05,  1.3988e-04, -6.8291e-05,\n             7.6013e-05, -1.7888e-05, -1.9693e-04, -5.2095e-05,  1.9056e-05,\n            -1.4171e-06, -9.5760e-05,  9.9012e-06,  1.5682e-05,  1.5287e-04,\n            -1.7432e-04, -2.2343e-05,  4.5414e-05,  6.9893e-05, -1.3244e-04,\n            -4.3775e-05, -1.0448e-04, -6.2302e-06,  3.5501e-05,  1.2944e-04,\n            -9.6749e-05,  7.3126e-05,  6.9726e-05, -6.4598e-05,  1.5177e-04,\n             2.2966e-05,  7.2999e-05,  8.6602e-05,  7.3440e-05,  7.9380e-05,\n            -9.9106e-05, -1.2131e-04, -1.0401e-04,  6.0982e-05,  6.5138e-05,\n            -3.2960e-05, -2.4010e-06, -2.4817e-07,  1.6325e-04,  4.2449e-06,\n            -1.1188e-04, -1.8278e-06,  1.8361e-06,  2.1893e-04, -4.7757e-05,\n            -1.0660e-06, -1.2165e-04, -1.2457e-04, -1.0454e-04,  4.5100e-05,\n             1.2788e-04, -1.3387e-04, -8.3819e-05,  3.4518e-05, -6.8749e-05,\n            -1.3515e-04,  3.3636e-05, -1.1198e-04, -6.6272e-05, -8.7762e-05,\n             4.2754e-05, -1.1305e-04,  3.8444e-05,  1.6270e-05, -1.6300e-04,\n             1.4998e-05, -1.4500e-05, -1.2396e-04,  4.1940e-05,  3.7821e-05,\n            -2.1147e-05,  1.2328e-04, -8.2601e-05,  1.5723e-05, -4.6270e-05,\n             1.5368e-04,  2.4760e-05, -3.2596e-06], device='cuda:0'),\n    'exp_avg_sq': tensor([1.6779e-07, 1.6620e-07, 2.3958e-07, 2.5558e-07, 1.8854e-07, 2.7548e-07,\n            2.7317e-07, 2.2795e-07, 3.6333e-07, 4.3287e-07, 8.7312e-08, 2.5257e-07,\n            2.5891e-07, 1.2151e-07, 2.1093e-07, 3.3707e-07, 2.8346e-07, 1.5033e-07,\n            2.2672e-07, 1.7260e-07, 1.7934e-07, 2.7874e-07, 4.5987e-07, 1.9089e-07,\n            2.6450e-07, 2.4902e-07, 3.6937e-07, 3.2526e-07, 2.4809e-07, 5.5539e-07,\n            1.6279e-07, 2.6309e-07, 4.5867e-07, 1.5954e-07, 3.0732e-07, 3.7053e-07,\n            2.6145e-07, 2.5517e-07, 2.3267e-07, 3.3625e-07, 3.1350e-07, 2.3708e-07,\n            1.2693e-07, 3.8072e-07, 2.0011e-07, 2.4542e-07, 4.3181e-07, 1.6242e-07,\n            2.9007e-07, 3.1942e-07, 1.4755e-07, 5.3508e-07, 3.7503e-07, 1.7176e-07,\n            2.7501e-07, 2.2192e-07, 2.6026e-07, 3.1069e-07, 2.4059e-07, 2.8375e-07,\n            4.4661e-07, 3.4479e-07, 2.0044e-07, 3.4714e-07, 1.2705e-07, 2.4951e-07,\n            2.0150e-07, 1.5577e-07, 2.3933e-07, 3.3813e-07, 3.3803e-07, 2.1967e-07,\n            3.3501e-07, 2.4920e-07, 2.7034e-07, 2.0320e-07, 2.9095e-07, 5.3253e-07,\n            2.5238e-07, 2.1754e-07, 4.3217e-07, 2.6863e-07, 1.2262e-07, 2.9781e-07,\n            3.1630e-07, 2.4533e-07, 2.8587e-07, 2.2951e-07, 2.1578e-07, 1.9314e-07,\n            1.5846e-07, 1.0444e-07, 2.0699e-07, 4.9511e-07, 2.0139e-07, 2.2268e-07,\n            3.0316e-07, 1.9378e-07, 1.8380e-07, 2.5871e-07, 5.0792e-07, 4.2999e-07,\n            3.3576e-07, 2.5988e-07, 1.8863e-07, 2.3836e-07, 3.1182e-07, 2.3087e-07,\n            2.4245e-07, 3.7093e-07, 1.3807e-07, 4.4472e-07, 1.4770e-07, 2.4391e-07,\n            2.5015e-07, 2.2658e-07, 1.9636e-07, 4.2792e-07, 1.3409e-07, 1.5561e-07,\n            2.1638e-07, 2.7603e-07, 2.0476e-07, 2.3592e-07, 2.8923e-07, 2.6035e-07,\n            3.9575e-07, 3.3376e-07], device='cuda:0')}},\n  'param_groups': [{'lr': 0.001,\n    'betas': (0.9, 0.999),\n    'eps': 1e-08,\n    'weight_decay': 0.01,\n    'amsgrad': False,\n    'foreach': None,\n    'maximize': False,\n    'capturable': False,\n    'differentiable': False,\n    'fused': None,\n    'params': [0,\n     1,\n     2,\n     3,\n     4,\n     5,\n     6,\n     7,\n     8,\n     9,\n     10,\n     11,\n     12,\n     13,\n     14,\n     15,\n     16,\n     17,\n     18,\n     19,\n     20,\n     21,\n     22,\n     23,\n     24,\n     25,\n     26,\n     27,\n     28,\n     29,\n     30,\n     31,\n     32,\n     33,\n     34,\n     35,\n     36,\n     37,\n     38,\n     39]}]},\n 'scheduler_state_dict': {'factor': 0.1,\n  'min_lrs': [0],\n  'patience': 10,\n  'verbose': False,\n  'cooldown': 0,\n  'cooldown_counter': 0,\n  'mode': 'min',\n  'threshold': 0.0001,\n  'threshold_mode': 'rel',\n  'best': 0.03191128408782769,\n  'num_bad_epochs': 9,\n  'mode_worse': inf,\n  'eps': 1e-08,\n  'last_epoch': 25,\n  '_last_lr': [0.001]},\n 'loss': 0.006991757128787143,\n 'running_loss_train': [0.3149492478095242,\n  0.05987151649943884,\n  0.04142564216626464,\n  0.034010360364044104,\n  0.026907527701895004,\n  0.02346789608894344,\n  0.020165720581566977,\n  0.019688999152607703,\n  0.014866683601859132,\n  0.01720529766147791,\n  0.014102692829583765,\n  0.013546108771276924,\n  0.012256489901266047,\n  0.011063687351361727,\n  0.010860560820801593,\n  0.00994129587735442,\n  0.011064039098756607,\n  0.009681815728947264,\n  0.008285080113809793,\n  0.008408538251939384,\n  0.009575411498763361,\n  0.006007468672058995,\n  0.008472113746886547,\n  0.009033998725962628,\n  0.006991757128787143],\n 'running_loss_val': [0.07616074890532393,\n  0.05269769177910812,\n  0.03524034865636458,\n  0.03789309152659583,\n  0.03632917657910668,\n  0.042108012809160544,\n  0.057129071559756994,\n  0.03309238691655721,\n  0.032314871253316946,\n  0.03918597357690661,\n  0.039644044267875635,\n  0.04614307842975324,\n  0.04038161819025458,\n  0.036215023885040165,\n  0.04018692260650185,\n  0.03191128408782769,\n  0.04316775202057622,\n  0.04398412934963918,\n  0.047662445012742576,\n  0.061036515107844025,\n  0.05499405140413883,\n  0.03902351489972368,\n  0.04727778534752941,\n  0.04540907082883542,\n  0.0432018525805572],\n 'test_accuracy': None,\n 'train_time': [16.417205810546875,\n  15.53947377204895,\n  15.855643272399902,\n  15.889739513397217,\n  15.923620462417603,\n  16.024991989135742,\n  15.894362449645996,\n  15.951533555984497,\n  15.989155054092407,\n  15.774015188217163,\n  15.888649225234985,\n  15.617486953735352,\n  15.88852071762085,\n  15.587123394012451,\n  15.792410135269165,\n  15.804240465164185,\n  15.98119831085205,\n  15.85136604309082,\n  15.950923681259155,\n  15.726938724517822,\n  16.187628030776978,\n  15.73666763305664,\n  16.065512657165527,\n  15.827620506286621,\n  15.827749490737915],\n 'inference_time': None}"},"metadata":{}}]},{"cell_type":"markdown","source":"**Create CMAPs to visualize the attention maps.**","metadata":{}},{"cell_type":"code","source":"# CMAP\n# reference: https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html\n!pip install grad-cam # grad-cam-1.5.2","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:33:33.974688Z","iopub.execute_input":"2024-06-26T07:33:33.975626Z","iopub.status.idle":"2024-06-26T07:34:00.580817Z","shell.execute_reply.started":"2024-06-26T07:33:33.975579Z","shell.execute_reply":"2024-06-26T07:34:00.579569Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Collecting grad-cam\n  Downloading grad-cam-1.5.2.tar.gz (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from grad-cam) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from grad-cam) (9.5.0)\nRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from grad-cam) (2.1.2)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from grad-cam) (0.16.2)\nCollecting ttach (from grad-cam)\n  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from grad-cam) (4.66.1)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from grad-cam) (4.9.0.80)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from grad-cam) (3.7.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from grad-cam) (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8.2->grad-cam) (2.31.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (2.8.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\nDownloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\nBuilding wheels for collected packages: grad-cam\n  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for grad-cam: filename=grad_cam-1.5.2-py3-none-any.whl size=38335 sha256=089e4b86c07c35bcb433f16a1b72953974b94e99f5e229f748ca8282f122448b\n  Stored in directory: /root/.cache/pip/wheels/b4/68/bb/d10381e86dc0de1c9354bce3d86bffcd247305058c40ce2e55\nSuccessfully built grad-cam\nInstalling collected packages: ttach, grad-cam\nSuccessfully installed grad-cam-1.5.2 ttach-0.0.3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CMAP\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# reference: https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install grad-cam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgrad\u001b[49m\u001b[38;5;241m-\u001b[39mcam\u001b[38;5;241m.\u001b[39m__version__\n","\u001b[0;31mNameError\u001b[0m: name 'grad' is not defined"],"ename":"NameError","evalue":"name 'grad' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Functions to create the acivation maps.\nimport cv2 #opencv-python==4.9.0.80\n\n\ndef get_gradcam(model, image, target_layer):\n    activations = []\n    gradients = []\n\n    def forward_hook(module, input, output):\n        activations.append(output)\n\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0])\n\n    # Register hooks\n    hook_handles = []\n    hook_handles.append(target_layer.register_forward_hook(forward_hook))\n    hook_handles.append(target_layer.register_backward_hook(backward_hook))\n\n    # Add batch dimension and prepare the input\n    image = img_tensor.unsqueeze(0).unsqueeze(0)  # shape: [1, 1, 28, 28]\n\n    # Forward pass\n    model.eval()\n    output = model(image)\n    pred_class = output.argmax(dim=1).item()\n\n    # Backward pass\n    model.zero_grad()\n    output[0, pred_class].backward()\n\n    # Remove hooks\n    for handle in hook_handles:\n        handle.remove()\n\n    # Get gradients and activations\n    gradients = gradients[0].cpu().data.numpy()[0]\n    activations = activations[0].cpu().data.numpy()[0]\n\n    # Compute weights\n    weights = np.mean(gradients, axis=(1, 2))\n    cam = np.zeros(activations.shape[1:], dtype=np.float32)\n    for i, w in enumerate(weights):\n        cam += w * activations[i]\n\n    cam = np.maximum(cam, 0)\n    cam = cv2.resize(cam, (image.shape[2], image.shape[3]))  # Resize to input image dimensions\n    cam = cam - np.min(cam)\n    cam = cam / np.max(cam)\n\n    return cam\n\n# Overlay the Grad-CAM on the original image\ndef overlay_cam_on_image(image, cam, alpha=0.4):\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_MAGMA)\n    heatmap = np.float32(heatmap) / 255\n    image = image.numpy().transpose(1, 2, 0)\n    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    overlay = heatmap + np.float32(image)\n    overlay = overlay / np.max(overlay)\n    return np.uint8(255 * overlay)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:43:32.737770Z","iopub.execute_input":"2024-06-26T07:43:32.738464Z","iopub.status.idle":"2024-06-26T07:43:32.751107Z","shell.execute_reply.started":"2024-06-26T07:43:32.738431Z","shell.execute_reply":"2024-06-26T07:43:32.750097Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# GradCAM comparison of VAN and CNN on cluttered_MNIST\n# Set models into eval mode\nvan.eval()\ncnn.eval()\n# Define target layer (last convolutional layer) for each network and move models to cpu.\ntarget_layer_van = van.cpu().block_2[-1].FFN.conv3  \ntarget_layer_cnn = cnn.cpu().conv2","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:36:58.845088Z","iopub.execute_input":"2024-06-26T07:36:58.845741Z","iopub.status.idle":"2024-06-26T07:36:58.852950Z","shell.execute_reply.started":"2024-06-26T07:36:58.845707Z","shell.execute_reply":"2024-06-26T07:36:58.852049Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Create GradCAMs.\nimg_id = 3 # Select image_id for GradCAM\naugmented_img, ground_truth = augmented_dataset_train[img_id]\naugmented_img = augmented_img.squeeze(0)\nimg_tensor = augmented_img\nimage = img_tensor.unsqueeze(0).to('cpu')\n\n'''\noutput = self.model(data)\n_, predicted = torch.max(output.data, 1)\n'''\n\noutput_van = van(image.unsqueeze(0))\n_, prediction_van = torch.max(output_van.data, 1)\noutput_cnn = cnn(image.unsqueeze(0))\n\ncam_van = get_gradcam(van, image, target_layer_van)\noverlay_van = overlay_cam_on_image(image, cam_van)\n\ncam_cnn = get_gradcam(cnn, image, target_layer_cnn)\noverlay_cnn = overlay_cam_on_image(image, cam_cnn)\n\n\n# Plot the results\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\naxes[0].imshow(overlay_van)\naxes[0].set_title('VAN')\naxes[0].annotate(f'VAN Prediction: {prediction_van}', xy=(1, -0.1))\naxes[0].axis('off')\naxes[1].imshow(overlay_cnn)\naxes[1].set_title('CNN')\naxes[1].axis('off')\n\nfig.suptitle('Side-by-Side Plots', fontsize=16)\nfig.text(0.5, 0.90, f'Ground Truth: {ground_truth}', ha='center', fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:01:22.836915Z","iopub.execute_input":"2024-06-26T08:01:22.837285Z","iopub.status.idle":"2024-06-26T08:01:23.290412Z","shell.execute_reply.started":"2024-06-26T08:01:22.837256Z","shell.execute_reply":"2024-06-26T08:01:23.289453Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA7YAAAIKCAYAAAAXhdMiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLdUlEQVR4nO3dd3hVVdr38d8+6Y3QBST0DlIHnBEFxEJRcFREEETsOmDvI0PxQUXGhoM4zCOKqGAXGLCjFBsoCIiAUgJIUWogJCF1v3/w5jysvU/KCaEs+X6uK9c19y5rr30Ms3Ofte+1HNd1XQEAAAAAYKnAie4AAAAAAABHg8QWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWACywbt06DR8+XC1atFBCQoJiY2NVu3ZtdezYUcOHD9e7777rO6dbt25yHEfz588P61qjR4+W4zgaPXp0+XS+BGXt57Gyf/9+jR07VmeeeaaSk5MVFRWl0047TWeccYauvvpqTZ48WRkZGcY5Zf3M5s+fL8dx1K1bt/K7gSJMnTpVjuMYP4FAQMnJyerUqZMeffRRHTx40Hde4bEAAJzMIk90BwAAxXvvvfd01VVXKTs7W1WqVFHnzp1VrVo17du3T8uXL9fzzz+vN954Q5dffvmJ7qr1fv75Z51//vnaunWrYmJidOaZZ6pWrVo6dOiQ1qxZo9dee02vvfaaOnfurFatWp3o7pZJQkKC+vXrJ0nKz8/Xxo0b9e233+q7777TtGnTtHDhQp122mnH5NqbNm1S/fr1VbduXW3atOmYXAMAcGoisQWAk9jvv/+ua665RtnZ2brnnns0duxYxcbGGscsXbpU77zzju/cadOmKTMzU3Xq1Dle3bXe4MGDtXXrVp177rl68803Va1aNWP/li1b9MorrygxMdHYPnz4cA0YMEBVq1Y9nt0tk6pVq2rq1KnGtiVLlui8887TL7/8ovvuu0/Tpk07MZ0DAKCMeBUZAE5ic+bM0cGDB1WrVi09+eSTvqRWkjp06KDHH3/ct71OnTpq1qyZ4uPjj0dXrbdhwwZ9//33kqR///vfvqRWOvyZ/uMf/1C9evWM7VWrVlWzZs2sSGxD6dSpk+655x5Jh98QyMvLO8E9AgAgPCS2AHAS+/333yUpZJJVkuJqV7OysjR69Gg1btxYMTExqlmzpq655hpt2bKlxHaXLl2qQYMGqU6dOoqJiVHlypXVo0cPffDBB2H30WvBggW68MILVblyZcXHx6tTp0569dVXfcd17dpVjuNoxowZRbY1fvx4OY6j/v37l+rahZ+1JFWvXj2sfpdUYztt2jR17NhR8fHxqly5snr27KlFixaV2O727dt19913q3nz5oqPj1dSUpI6duyoiRMnlnvy2aFDB0lSRkaGdu/eXapz9u7dq7///e9q2bJlsH8dOnTQ+PHjlZWVZRw7dOhQ1a9fX5K0efNmX71voYKCAv3nP/9R586dVbFiRUVFRal69epq06aNbrvtNl5hBgCERGILACexwteIV61apXnz5pVLm5mZmerevbvGjBmjHTt26MILL9Q555yjjz/+WO3bt1dqamqR506YMEGdOnXS9OnTVaVKFfXt21ctW7bU/PnzddFFF+mRRx4pc7/ef/99de/eXdu2bVOPHj3UsWNHLV26VEOGDAmOJha64447JEkTJ04M2VZBQYFeeOEFSYdfEy6NI1/ZnjBhQlluIaQ77rhD11xzjZYtW6aOHTuqR48e+vXXX9WtWzfNnDmzyPMWLlyoVq1a6ZlnntGhQ4d0wQUXqHPnztqwYYNuu+02XXTRRcrNzS23fh44cCD4v2NiYko8fuPGjWrfvr0ef/xx7dq1S71791b37t21bt06PfDAAzr77LO1b9++4PFnn312sA48ISFB11xzjfFT6IYbbtDNN98c/LyuuOIKtW/fXllZWZo4caKWL19ebvcMAPgDcQEAJ6309HT39NNPdyW5juO43bp1c//nf/7HnTt3rrtz585iz+3atasryf3iiy+M7ffee68ryW3WrJm7bdu24PaMjAz3kksucSW5ktxRo0YZ53300Ueu4zhu1apV3QULFhj7Vq5c6dauXduV5M6fPz+seyzspyT3scceM/bNnz/fjYuLcyW5H330UXB7Xl6eW7duXVeSu2zZMl+b//3vf11JbuvWrcPqy5H336JFC/fee+9133zzTXf9+vXFnjdq1KiQn9mcOXNcSW5CQoK7cOFCY99jjz0WvFbXrl2NfTt27HCrVKniOo7jTpo0yc3Pzw/u2717t9u9e3dXkjtmzJhS39vLL7/sSnLr1q0bcn+/fv1cSW6dOnWM7YV99DrzzDNdSW7fvn3dgwcPBrfv3LnTbd++vSvJveqqq4xzUlNTi+3D5s2bXUlu7dq13R07dvj2r1692t28eXMJdwoAOBWR2ALASW7t2rXBJML707ZtW/eFF15w8/LyfOeFSmwzMzPdpKQkV5L74Ycf+s7ZsWOHGxsbGzJJK+zDO++8E7Kfb731livJvfzyy8O6v8J+tmvXLuT+e+65x5XkXnDBBcb28ePHu5Lc66+/3ndOjx49XEnu5MmTw+rLgQMH3MGDB7uO4/g+69q1a7sPPfSQu3fvXt95RSW2559/vivJfeCBB0Jer23btiET2wceeMCV5A4fPjzkeVu3bnWjoqLcatWquQUFBaW6t1CJbV5enrtu3Tr3jjvuCN7n008/bZwXKrFdtGiRK8mNj493f/vtN9+1vv/+e1eSGwgE3F9//TW4vaTEdsmSJcFkGQCAcPAqMgCc5Jo2bapvv/1Wixcv1siRI9WjR49gze3y5ct16623qmfPnsrJySmxrWXLlik9PV1Vq1ZVz549fftr1KihCy+80Ld99+7dWrJkieLi4tSnT5+QbReuxfr111+HcXf/Z8iQISG3F76m+uWXXyo/Pz+4/YYbblB8fLymT59uvPK6fv16ffLJJ6pYsaIGDx4cVh+SkpL06quvasOGDXr66afVr18/NWjQQJK0detWPf7442rbtm2p6jzz8vL05ZdfSlKR/SjqnufOnStJuvLKK0PuP/3009W4cWPt2rVL69atK7EvRzqyvjUyMlKNGzfWhAkTFAgEdPfdd+vOO+8ssY3Cuu2ePXuGXBqoQ4cOatOmjQoKCrRgwYJS961Zs2ZKSkrSBx98oEcffbTY1+IBADgSy/0AgCU6deqkTp06SZJc19UPP/ygf/7zn3rjjTf02WefacKECbrvvvuKbWPr1q2S5JvV90iFE/wcKTU1Va7rKisrq8T6y127dgX/97hx47R27VrfMU8++aRvBuFQ1z1ye1ZWlvbs2ROc2KlSpUq6+uqrNXnyZE2ZMkX33nuvJGnSpElyXVfXXnutMSN0uH256667dNddd0k6nAxOmTJF48eP15YtWzRs2LBg8lmUPXv26NChQ6W6N6+NGzdKks4555xiryEd/rybNGlS4nGFjlzH1nEcJSYmqkmTJrr44ouL7I/Xtm3bJBXdf0lq2LChVqxYETy2NJKSkvTyyy/r2muv1YgRIzRixAjVrFlTf/7zn9WzZ09dddVVvqWWAACQSGwBwEqO46h9+/aaMWOGMjMzNXv2bM2cObPExLasCgoKJEmJiYnBCYBK46OPPgo5Yjd69OgyLY3juq4R33777Zo8ebJeeOEF3X333Tp06JBefvllOY6jYcOGlVtf6tatq0ceeUSVKlXS3XffrU8++URZWVmKi4sL+x5Ko/Dz7tevnxISEoo9tkqVKmG1HWod25PJ5ZdfrvPPP1+zZ8/WokWL9NVXX+n999/X+++/r5EjR+rTTz/VGWeccaK7CQA4yZDYAoDlLrzwQs2ePbtUS7ScfvrpklTsq7Sh9qWkpEg6nFC/9NJLCgRKV8kSaqmhohT12mlhf2JjY31JXIsWLXT++efrs88+04cffqjt27crLS1NvXr1UsOGDcvcl6IUvqadl5entLS0YhPbKlWqKCYmRtnZ2dq0aZNatmzpO6ao/w4pKSnB2YX/9Kc/HXW/y1vh71HhyHIohfsKjw1HcnKyrr76al199dWSpF9//VW33XabZs2apeHDh4f1ejMA4NRAjS0AnMS8I5ShFK49W7t27RKP7dChgxITE7V792598sknvv2///57yO21atVS69atlZ6ero8++qgUPQ/fa6+9FnL7tGnTJB1eLiYy0v997JFL/zz//POSSr/Ez5HC+axjYmJKHOWNjIxU586dJUmvv/56yGNCrdErSb169ZIkvfXWWyX26UQorKf+6KOPjPV/C/3www9avny5AoGAunTpEtweHR0tSWGvwZuSkqIxY8ZIEsv9AABCIrEFgJPYpEmTdM0114SckMl1Xb333nvBtVwHDBhQYntxcXG66aabJEl33XWXduzYEdyXlZWlW2+9VVlZWSHPHTt2rCTp2muv1X//+9+Q/Vm8eHHIxLg0li5dqvHjxxvbvvzyy2CyWljv6tW7d281atRIH330kVasWKGGDRsGE8NwrFy5Uueee67ef//9kBNxrVixIphEX3755YqKiiqxzcKJmP71r3/5/huOHz9ey5YtC3nefffdp4oVK+rpp5/WU089FbI/qampRX4ZcKydffbZOvPMM5WVlaWbb75ZmZmZwX27d+/WzTffLOnw72ThaL8kVatWTdHR0frtt9+0d+9eX7s//PCD3nzzzZC/g4W/c3Xr1i3v2wEA/AHwKjIAnMRyc3M1bdo0TZs2TdWqVVO7du1UtWpVpaWlafXq1cFXWQcPHqzrr7++VG0+8sgj+vLLL7VkyRI1adJE5557rmJjY7Vo0SLl5uZqyJAhwVHSI/Xp00cTJkzQPffco759+6pRo0Zq2rSpkpOTtWvXLq1YsUI7d+7UAw88EHJm5ZLcfvvteuihhzRt2jS1bt1a27dv16JFi1RQUKA77rhDvXv3DnleIBDQ8OHDg0nk3/72NzmOE/b1XdfV/PnzNX/+fCUkJKhdu3Y6/fTTlZOTo9TU1OBIYdu2bfXss8+Wqs0+ffpo2LBhev7553XOOeeoS5cuqlmzplauXKk1a9bojjvu0IQJE3zn1a5dW7NmzdLll1+ue++9V+PHj1erVq1Us2ZN7d+/X2vWrNGGDRt05plnhj3zc3mZPn26unfvrlmzZql+/frq0qWLcnNz9cUXX+jAgQNq37598EuXQlFRUerbt6/eeecdtW3bVmeffXZwgq8XX3xRmzdv1oABAxQXF6f27dsrJSVFeXl5+vHHH/Xzzz8rOjra9+UHAAASiS0AnNSuv/561a9fX/PmzdPixYu1evVq/f7774qMjFStWrU0cOBADRkyJOTSPUVJSEjQF198oXHjxmn69On6+OOPValSJZ1//vkaO3ZssRML3X777erevbv+9a9/6YsvvtC8efMUCARUo0YNtWvXThdddFFYk0sd6dJLL9Ull1yixx57TB988IFycnLUvn17DR8+PLjkT1F69OghSYqPj9d1111Xpuu3atVKCxYs0Lx587Rw4UJt2bJFy5YtU15eXnB5pMsuu0xDhw4t1WhtoYkTJ6pDhw56/vnn9e233yomJkYdO3YMJn2hEltJ6tKli3766SdNnDhRc+fO1Xfffafs7GxVr15dderU0eDBg8v8WZeHBg0aaNmyZXryySc1c+ZMzZkzR4FAQE2bNtWVV16p22+/PWQN8uTJk1WlShV9+OGHeuedd5SbmyvpcGL75z//WePGjdPChQu1Zs0a/fDDD4qMjFTt2rU1bNgw3XbbbWratOnxvlUAgAUctzRFRQAAnMRGjBihRx99VDfddJMmT558orsDAACOMxJbAIDVduzYoRYtWujAgQNatWqVmjdvfqK7BAAAjjNeRQYAWOnBBx/Utm3b9NlnnyktLU233HILSS0AAKcoRmwBAFaqV6+etmzZoho1aujKK6/UuHHjFBMTc6K7BQAATgASWwAAAACA1VjHFgAAAABgNRJbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWI3EFgAAAABgNRJbAAD+P8dxNHr06BPdjWINHTpUiYmJJ7obAACcVEhsAQBhSU1N1fDhw9WkSRPFx8crPj5eLVq00LBhw7Ry5coT3b1jqlu3bnIcp8Sfo02OMzMzNXr0aM2fP79c+l0ab775pgYPHqzGjRvLcRx169btuF0bAICjFXmiOwAAsMecOXN05ZVXKjIyUoMGDVKbNm0UCAS0du1avffee3rhhReUmpqqunXrnuiuHhMPP/ywbrjhhmD83Xff6bnnntPf//53NW/ePLi9devWR3WdzMxMjRkzRpKOW4L5wgsvaOnSperYsaP27NlzXK4JAEB5IbEFAJTKhg0bNGDAANWtW1fz5s1TzZo1jf1PPPGEJk2apECg+JeBMjIylJCQcCy7esxccMEFRhwbG6vnnntOF1xwQbEJqA33/Oqrr+r0009XIBBQq1atTnR3AAAIC68iAwBKZfz48crIyNDLL7/sS2olKTIyUrfffrtSUlKC2wrrQTds2KDevXsrKSlJgwYNknQ42bvnnnuUkpKimJgYNW3aVE8++aRc1w2ev2nTJjmOo6lTp/qu533ld/To0XIcR+vXr9fQoUNVsWJFJScn69prr1VmZqZxbnZ2tu666y5Vq1ZNSUlJ6tu3r7Zu3XqUn5DZj9WrV+uqq65SpUqVdPbZZ0s6PPoaKgEeOnSo6tWrF7znatWqSZLGjBlT5OvN27Zt01//+lclJiaqWrVquvfee5Wfn28cs2PHDq1du1a5ubkl9jslJaXELyUAADhZ8QQDAJTKnDlz1KhRI5155plhnZeXl6cePXqoevXqevLJJ3X55ZfLdV317dtXzzzzjHr27Kmnn35aTZs21X333ae77777qPrZv39/paen6/HHH1f//v01derU4Gu9hW644QY9++yzuvDCCzVu3DhFRUXpoosuOqrrel1xxRXKzMzUY489phtvvLHU51WrVk0vvPCCJOnSSy/Vq6++qldffVWXXXZZ8Jj8/Hz16NFDVapU0ZNPPqmuXbvqqaee0n/+8x+jrYceekjNmzfXtm3byuemAAA4SfEqMgCgRAcOHND27dv117/+1bcvLS1NeXl5wTghIUFxcXHBODs7W1dccYUef/zx4LZZs2bp888/19ixY/Xwww9LkoYNG6YrrrhCEyZM0PDhw9WwYcMy9bVdu3aaMmVKMN6zZ4+mTJmiJ554QpK0YsUKvfbaa/rb3/6m559/PnjtQYMGlevkV23atNH06dPDPi8hIUH9+vXTrbfeqtatW2vw4MG+Yw4dOqQrr7xS//jHPyRJt9xyi9q3b68pU6bo1ltvPeq+AwBgG0ZsAQAlOnDggCSFXGamW7duqlatWvCnMFk8kjfZ+uCDDxQREaHbb7/d2H7PPffIdV19+OGHZe7rLbfcYsTnnHOO9uzZE7yHDz74QJJ8177zzjvLfM3S9KO8hbrPjRs3GtumTp0q13WDrzkDAPBHxYgtAKBESUlJkqSDBw/69k2ePFnp6en6/fffQ44uRkZGqnbt2sa2zZs3q1atWsF2CxXOLLx58+Yy97VOnTpGXKlSJUnSvn37VKFCBW3evFmBQMA3Ity0adMyXzOU+vXrl2t7R4qNjQ3W4RaqVKmS9u3bd8yuCQDAyYzEFgBQouTkZNWsWVOrVq3y7Susud20aVPIc2NiYso8KZHjOCG3eydJOlJERETI7UdOSnU8HPk6diHHcUL2o7j7CaWoewQA4FTFq8gAgFK56KKLtH79ei1ZsuSo26pbt662b9+u9PR0Y/vatWuD+6X/G21NS0szjjuaEd26deuqoKBAGzZsMLb//PPPZW6ztCpVquS7F8l/P0Ul9AAAIDQSWwBAqdx///2Kj4/Xddddp99//923P5wR0d69eys/P18TJ040tj/zzDNyHEe9evWSJFWoUEFVq1bVwoULjeMmTZpUhjs4rLDt5557ztj+7LPPlrnN0mrYsKHWrl2rXbt2BbetWLFCX331lXFcfHy8JH9CH65wlvsBAMBmvIoMACiVxo0ba/r06Ro4cKCaNm2qQYMGqU2bNnJdV6mpqZo+fboCgYCvnjaUPn366Nxzz9XDDz+sTZs2qU2bNvrkk080a9Ys3XnnnUb96w033KBx48bphhtu0J/+9CctXLhQv/zyS5nvo23btho4cKAmTZqk/fv366yzztK8efO0fv36MrdZWtddd52efvpp9ejRQ9dff7127typf//732rZsmVwcivp8GvMLVq00JtvvqkmTZqocuXKatWqlVq1ahXW9R566CG98sorSk1NLXECqYULFwa/QNi1a5cyMjI0duxYSVKXLl3UpUuX8G4WAIDjiMQWAFBql1xyiX788Uc99dRT+uSTT/TSSy/JcRzVrVtXF110kW655Ra1adOmxHYCgYBmz56tkSNH6s0339TLL7+sevXq6Z///Kfuuece49iRI0dq165deuedd/TWW2+pV69e+vDDD1W9evUy38dLL72katWq6fXXX9fMmTPVvXt3zZ07VykpKWVuszSaN2+uadOmaeTIkbr77rvVokULvfrqq5o+fbrmz59vHPviiy/qtttu01133aWcnByNGjUq7MQ2HJ9//rlvvd/C5YRGjRpFYgsAOKk57vGeTQMAAAAAgHJEjS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktkAY+vbtq/j4eKWnpxd5zKBBgxQdHa09e/ZIktLS0hQbGyvHcbRmzZqQ5wwdOlSO46h169ZyXde333EcDR8+vHxuAgAAlMqGDRt08803q0GDBoqNjVWFChXUuXNnTZgwQVlZWZKkevXqyXEc3Xbbbb7z58+fL8dx9M477wS3TZ06VY7jKDY2Vtu2bfOd061bN7Vq1erY3RTwB0ViC4Rh0KBBysrK0vvvvx9yf2ZmpmbNmqWePXuqSpUqkqS3335bjuOoRo0aev3114tt/8cff9R7771X7v0GAADhmTt3rs444wy99dZb6tOnj/71r3/p8ccfV506dXTffffpjjvuMI7/3//9X23fvr3U7WdnZ2vcuHHl3W3glEViC4Shb9++SkpK0vTp00PunzVrljIyMjRo0KDgttdee029e/fWwIEDizxPkuLi4tSkSRM98sgjIUdtAQDA8ZGamqoBAwaobt26Wr16tSZMmKAbb7xRw4YN04wZM7R69Wq1bNkyeHzLli2Vn58fVqLatm3bsJNhAEUjsQXCEBcXp8suu0zz5s3Tzp07ffunT5+upKQk9e3bV5K0ZcsWLVq0SAMGDNCAAQOUmpqqr7/+OmTbgUBAI0aM0MqVK4scEQYAAMfe+PHjdfDgQU2ZMkU1a9b07W/UqJExYluvXj0NGTIkrET173//e9jJMICikdgCYRo0aJDy8vL01ltvGdv37t2rjz/+WJdeeqni4uIkSTNmzFBCQoIuvvhiderUSQ0bNiz2deSrrrpKjRs3ZtQWAIAT6L///a8aNGigs846q9TnPPzww8rLyyt1olq/fv2wk2EARSOxBcLUvXt31axZ0/da8dtvv63c3FzjNeTXX39dl1xySTDRvfLKK/XWW28pLy8vZNsREREaMWKEVqxYoZkzZx6zewAAAKEdOHBA27Zt0xlnnBHWeQ0aNNDVV1+t//3f/9WOHTtKdU5hMvzEE0+UpasAjkBiC4QpIiJCAwYM0DfffKNNmzYFt0+fPl2nnXaazjvvPEnSypUr9eOPP2rgwIHBYwYOHKjdu3fr448/LrL9QYMGMWoLAMAJcuDAAUlSUlJS2OeOGDEirFHbwmT4P//5T6mTYQChkdgCZVA4Kls4art169ZgLW1ERISkw5NGJSQkqEGDBlq/fr3Wr1+v2NhY1atXr9jXkQtHbZcvX86oLQAAx1mFChUkqdil/YpSlkQ13GQYQGgktkAZdOjQQc2aNdOMGTMkHa6ldV03mPC6rqsZM2YoIyNDLVq0UOPGjYM/mzZt0qxZs3Tw4MEi2x80aJAaNWrEqC0AAMdZhQoVVKtWLa1atapM54f7enGDBg00ePBgRm2Bo0RiC5TRoEGDtGrVKq1cuVLTp09X48aN1bFjR0nSggULtHXrVj3yyCN6++23jZ///Oc/yszMLHY09shR21mzZh2nOwIAAJJ08cUXa8OGDfrmm2/CPrdhw4YaPHiwJk+eHPaoLbW2QNmR2AJlVDg6O3LkSC1fvty3dm1CQoLuu+8+9evXz/i58cYb1bhx42JfR5akwYMHq1GjRhozZswxvQ8AAGC6//77lZCQoBtuuEG///67b/+GDRs0YcKEIs8fMWKEcnNzNX78+FJd78hk+Lfffitzv4FTGYktUEb169fXWWedFRxRLUxss7Oz9e677+qCCy5QbGxsyHP79u2rzz77LORauIUiIiL08MMPa/ny5eXedwAAULSGDRtq+vTp2rhxo5o3b64777xTL774oiZNmqTBgwerRYsWWr16dbHnDx48OKxn+MMPP6zc3Fz9/PPP5XAHwKmHxLYMunXrpjvvvDMY16tXT88+++xRtVkebZwqcnJy1KhRI3399deSpE2bNslxHDmOo7Zt24bV1tChQ4PnFr4avHr1atWuXVsZGRklnl+YzHbq1EmNGjWSJM2dO1dpaWnq06dPkef16dNHeXl5euONN4ptf/DgwWrYsGEp7wYAAJSXvn37auXKlerXr59mzZqlYcOG6cEHH9SmTZv01FNP6bnnniv2/BEjRgQnlCyNRo0aafDgwUfbbeDU5R6liy++2O3Ro0fIfQsXLnQluStWrAhuu+mmm9xAIOC+9dZbvuNHjRrlSnJvvvlmY/sPP/zgSnJTU1OL7EfXrl1dSa4kNyYmxm3evLn7/PPPl+2mStC1a1f3jjvuCMY7d+50MzIySnXuyy+/7CYnJ/u2h9NGefLeiw0mTJjgnn/++cE4NTXVleR+9tln7u7du4PbV61a5V522WVu3bp1XUnuM88842srLS3N3bFjhyvJff/994PbL7/8cveRRx45lrcBAAAAoJwc9Yjt9ddfr08//VRbt2717Xv55Zf1pz/9Sa1bt5YkZWZm6o033tD999+vl156KWR7sbGxmjJlitatWxd2X2688Ubt2LFDq1evVv/+/TVs2LDgrLVeOTk5YbdflGrVqik+Pv6Et/FHlp+fr4KCArmuq4kTJ+r666/3HVOlShVVqVIlGGdmZqpBgwYaN26catSoEbLd5OTkkPuuvfZavfDCC8rLyyu/mwAAAABwTBx1YnvxxRerWrVqmjp1qrH94MGDevvtt40E5O2331aLFi304IMPauHChfr111997TVt2lTnnnuuHn744bD7Eh8frxo1aqhBgwYaPXq0GjdurNmzZ0s6/Prw8OHDdeedd6pq1arq0aOHJGnVqlXq1auXEhMTddppp+nqq6/W7t27g21mZGRoyJAhSkxMVM2aNfXUU0/5rut9jTgtLU0333yzTjvtNMXGxqpVq1aaM2eO5s+fr2uvvVb79+8Pvv46evTokG1s2bJFl1xyiRITE1WhQgX179/fmLxg9OjRatu2rV599VXVq1dPycnJGjBgQFhrrg0dOlQLFizQhAkTgv3ZtGlTqT6Xbt266fbbb9f999+vypUrq0aNGsF7kQ4vdzN69GjVqVNHMTExqlWrlm6//fbg/n379mnIkCGqVKmS4uPj1atXL+PLjKlTp6pixYqaPXu2WrRooZiYGG3ZskVLly7Vhg0bdNFFF5V4fx07dtQ///lPDRgwQDExMaX+XCTpggsu0N69e7VgwYKwzgMAAABw/B11YhsZGakhQ4Zo6tSpxnqbb7/9tvLz8zVw4MDgtilTpmjw4MFKTk5Wr169fMlwoXHjxundd9/V999/f1R9i4uLM0ZmX3nlFUVHR+urr77Sv//9b6Wlpal79+5q166dvv/+e3300Uf6/fff1b9//+A59913nxYsWKBZs2bpk08+0fz587Vs2bIir1lQUKBevXrpq6++0muvvabVq1dr3LhxioiI0FlnnaVnn31WFSpU0I4dO7Rjxw7de++9Idu45JJLgonVp59+qo0bN+rKK680jtuwYYNmzpypOXPmaM6cOVqwYIGxuPfUqVPlOE6RfZ0wYYL+8pe/BEe6d+zYoZSUlFJ9LoWfZ0JCghYvXqzx48frkUce0aeffipJevfdd/XMM89o8uTJWrdunWbOnKkzzjgjeO7QoUP1/fffa/bs2frmm2/kuq569+6t3Nzc4DGZmZl64okn9OKLL+qnn35S9erVtWjRIjVp0kRJSUlF3ld5iI6OVtu2bbVo0aJjeh0AAAAARy+yPBq57rrr9M9//lMLFixQt27dJB1+Dfnyyy9XcnKyJGndunX69ttv9d5770k6PCnO3XffrREjRviSr/bt26t///564IEHNG/evLD7k5+frxkzZmjlypW66aabgtsbN25sTLs+duxYtWvXTo899lhw20svvaSUlBT98ssvqlWrlqZMmaLXXntN5513nqTDyVzt2rWLvPZnn32mJUuWaM2aNWrSpImkwwtvF0pOTpbjOEW+GitJ8+bN048//qjU1FSlpKRIkqZNm6aWLVvqu+++C66VWlBQoKlTpwaTvKuvvlrz5s3To48+GrxW06ZNi7xOcnKyoqOjgyPdhSZOnFjs51J4X61bt9aoUaOCn+3EiRM1b948XXDBBdqyZYtq1Kih888/X1FRUapTp446deok6fDvwuzZs/XVV1/prLPOkiS9/vrrSklJ0cyZM3XFFVdIknJzczVp0iS1adMm2I/NmzerVq1aRd5TeapVq5Y2b958XK4FAAAAoOzKZVbkZs2a6ayzzgrWza5fv16LFi0yXkN+6aWX1KNHD1WtWlWS1Lt3b+3fv1+ff/55yDbHjh2rRYsW6ZNPPil1PyZNmqTExETFxcXpxhtv1F133aVbb701uL9Dhw7G8StWrNAXX3yhxMTE4E+zZs0kHR4N3bBhg3JycnTmmWcGz6lcuXKxyeLy5ctVu3btYPJXFmvWrFFKSkowqZWkFi1aqGLFilqzZk1wW7169YyRy5o1axrLx1x66aVau3Zt2Ncv6XMpVFg7Her6V1xxhbKystSgQQPdeOONev/994P1qmvWrFFkZKTxuVapUkVNmzY17i86Otp3jaysrCKX0ClvcXFxyszMPC7XAgAAAFB25bbcz/XXX693331X6enpevnll9WwYUN17dpV0uER1FdeeUVz585VZGSkIiMjFR8fr7179xY5iVTDhg1144036sEHHzRecS7OoEGDtHz5cqWmpiojI0NPP/20AoH/u8WEhATj+IMHD6pPnz5avny58bNu3Tp16dKlTJ9DXFxcmc4ri6ioKCN2HEcFBQVH3W5pP5firp+SkqKff/5ZkyZNUlxcnP72t7+pS5cuxqvGJYmLi/ON5letWlX79u07irsrvb1796patWrH5VoAAAAAyq7cEtv+/fsrEAho+vTpmjZtmq677rpgUvLBBx8oPT1dP/zwg5EozZgxQ++9957S0tJCtjly5Ej98ssvJa71WSg5OVmNGjXS6aefbiS0RWnfvr1++ukn1atXT40aNTJ+EhIS1LBhQ0VFRWnx4sXBc/bt26dffvmlyDZbt26trVu3FnlMdHS08vPzi+1X8+bN9euvvxqTa61evVppaWlq0aJFifcVjlD9KelzKa24uDj16dNHzz33nObPn69vvvlGP/74o5o3b668vDzjc92zZ49+/vnnEu+vXbt2Wrt2bam/7Dgaq1atUrt27Y75dQAAAAAcHcctZYYQUcwkRIWOHCt0/v/Pkdu9qWbhwrOFx4Y6rqg2i7p2UelsqP2F1y9s+8jt3j45R+wrVNhWgULfr7evjsx7PnL7kW2E6ldhfOQ1i7qfQBFxKEX1tbSfS6j/VgGZn5O3veI+V+9nUNLvTFHbjmwz1P1479f737CoNgGcXPKPw5dcp5LSPOsBADieSvusL7cRW8mfrEmhE6SijimpzfIWKpHx9iVUolOadotqs6TrFdWvI88tT0VdpzT9LIn33CPbLem6pelvOH0oKi4Jf+IBAAAAJ79yHbEFjgfvaGp5jK6GGjFntBY4+TFiW7541gMATjalfdaXy3I/wPFU1J9dpXn12quoqbZIagEAAAB7kNjCSqHql8ujnaNpCwAAAMCJQWIL6x1NIkoSCwAAANivXCePAgAAAADgeCOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA11rHFSWvkyJHF7s/MzDTiTZs2hX2N2rVrG3GFChVKPOeRRx4J+zoAAJzqAiFWjx89erQRF8g1Yu+zPjU1Nezrep/1ycnJJZ7j7ReAkx8jtgAAAAAAq5HYAgAAAACsRmILAAAAALAaNbawxrp164x49uzZRpyRkRF2m9WrVzfi87qfZ8SNmzQOu00AAODnrZ8Nte2XX34x4lmzZhlxuTzrzzOf9R1btPSd0zWxrREvOLg87OsCOL4YsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVHNd1/QUPIUQ4/rXHgGOpXt16Rrxp86bj3oe77rrLt+2ZZ5457v0AEFp+6R5hKCWe9Tje6tWrZ8RlWZM+XN7f8skPP+o75pWpq434q22vH8MeAShOaZ/1jNgCAAAAAKxGYgsAAAAAsBqJLQAAAADAaqxji5PWiaipTamdYsRxsXHHvQ8AAJwqjkdNrVdtz7M+Ki7ad0xeVNLx6g6AcsKILQAAAADAaiS2AAAAAACrkdgCAAAAAKxGYgsAAAAAsBqTR+GU1rx5cyM+r/t5RhwZxT8RAABskhgRYcTd2nQ04hbnnGnEKZG1fW3EZH1V/h0DcEwxYgsAAAAAsBqJLQAAAADAaiS2AAAAAACrUUCIU0qTJk2M+NK/XmrE1NQCAGC3uo0aGXGXPj2N+JDjGPEG91dfG9v3zy/3fgE4thixBQAAAABYjcQWAAAAAGA1ElsAAAAAgNUoKMQflreeVpIGDhxoxK7rFttGTk5OufYJAACUn1DP+v5XXWXEmb5nvRlvTk/ztbH+kH8bgJMbI7YAAAAAAKuR2AIAAAAArEZiCwAAAACwGjW2+MOoUqWKEf/1r3/1HVNSTa3XG2+8cTRdAgAA5cj7rL/ssst8x4T7rJ8xY8ZR9QnAyYERWwAAAACA1UhsAQAAAABWI7EFAAAAAFiNxBYAAAAAYDUmj8IfRv369Y04KirKd4wjx4gzMjOMeOnSpUb866+/llPvAADA0fI+6yMjS/5TNjMz04i///57I96yZcvRdwzACceILQAAAADAaiS2AAAAAACrkdgCAAAAAKzmuKVcxTrCcUo+CACA4yi/dI8wlBLPegDAyaa0z3pGbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYLfJEdwA4kRrUb2DEG1M3nqCeAACAY6FBA8+zfiPPeuCPiBFbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWM1xXdctzYERjnOs+wIcd3FxcUaclZV1gnoCoCzyS/cIQynxrMcfEc96wG6lfdYzYgsAAAAAsBqJLQAAAADAaiS2AAAAAACrUWMLALAWNbbli2c9AOBkQ40tAAAAAOCUQGILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq0We6A4AtjvnnHOMuPu53Ys9/rnnnvNt25e2r1z7BAAAyk8X77P+vPOKPf7ZZ5/1bUtLSyvHHgHwYsQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAVqPGFghT5UqVjbh9+/bFHr9kyRIjPpB+oNz7BAAAyk+lSpWMuH2HDkYcHTDHhpYsXmzE6enpx6ZjAIrEiC0AAAAAwGoktgAAAAAAq5HYAgAAAACsRo0tcATHcYw4IT7Bd8xll11mxBWTKxrxrl27jPjjjz824gK34Ch6CAAAjob3WR8fH+87pl+/fkZcvbI5v0aDA1WM+NOv9hhxfn7+0XQRQBkwYgsAAAAAsBqJLQAAAADAaiS2AAAAAACrUWOLU1pCgllD612Ttvu53Utsw3VdI16+YrkRU1MLAMCJ433Wd/A+6887z3dOwFOHmxgw/2R+8ds3jPiLtG+PposAygEjtgAAAAAAq5HYAgAAAACsRmILAAAAALAaNbY4pV111VVGXKtmrbDbmDdvnhF//fXXR9UnAABQfgYNGmTEtWqZz3pHZj2t5B/5efPDuUb81VdflUvfAJQfRmwBAAAAAFYjsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVmDwKpzTvZFGuXCPet2+f75y33nzLiHfu2ln+HQMAAOXCO1mU1569e3zb3njjDSPetWtXufYJQPljxBYAAAAAYDUSWwAAAACA1UhsAQAAAABWo8YWp7SsQ1lG/MnHnxjx8hXLj2NvAABAecs+dMiI539iPuu/WbbseHYHwDHCiC0AAAAAwGoktgAAAAAAq5HYAgAAAACs5riu65Z8mBThOMe6L8BxV7VqVSPevXv3CeoJgLLIL90jDKXEsx5/RDzrAbuV9lnPiC0AAAAAwGoktgAAAAAAq5HYAgAAAACsRo0tAOCkVa9ePSPetGmTEVNjW7541gMAjrfyetYzYgsAAAAAsBqJLQAAAADAaiS2AAAAAACrkdgCAAAAAKwWeaI7AABAUc455xwj9k4oAQAA7NalSxcjLuuznhFbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWI0aWwDASSsmJuZEdwEAABxD5fWsZ8QWAAAAAGA1ElsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAVmMdWwDASevAgQMnugsAAOAYKq9nPSO2AAAAAACrkdgCAAAAAKxGYgsAAAAAsBqJLQAAAADAakweBQAnkRo1ahhxVGSUEf/lrL+U2EZKSooRJyYmht2PMWPGhH3OsfDdd9+d6C4AAFCuvM/6yEgzJevcuXOJbXif9UlJSWH3Y9SoUWGfcyyU17OeEVsAAAAAgNVIbAEAAAAAViOxBQAAAABYjRpbADiB+vXrZ8TNmjYz4ojIiOPZHQAAUM6uuOIKI27WzHzWe2tsUTaM2AIAAAAArEZiCwAAAACwGoktAAAAAMBqvNANACdQy5Ytj/k10g+kG/HmzZuP+TXLS2pq6onuAgAAR6VVq1bH/BoHDhww4k2bNh3za5aXjRs3lks7jNgCAAAAAKxGYgsAAAAAsBqJLQAAAADAatTYAsBJbPv27Ua8ZPES3zGVKlcy4h+W/WDEuXm5RpyZmVlOvQMAAEdr27ZtRrx48WLfMZUrVzbipUuXGnFeXp4Rn4rPekZsAQAAAABWI7EFAAAAAFiNxBYAAAAAYDVqbAHgBFr6vVkjs2DBAiPOzDJrZPLz8495nwAAQPn57rvvjHj+/PlGnJWVZcQ868uGEVsAAAAAgNVIbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1x3VdtzQHRjjOse4LAABhyS/dIwylxLMeAHCyKe2znhFbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWC3yRHcAAHB8JSQkGHGvXr18x7zzzjvHqzsAAKCceZ/1vXv39h3z9ttvH6/uHBeM2AIAAAAArEZiCwAAAACwGoktAAAAAMBq1NgCwB9cYmKiEXfu3NmIW7Zs6TuHGlsAAOzhfdafffbZRtyqVSvfOdTYAgAAAABwEiGxBQAAAABYjcQWAAAAAGA1ElsAAAAAgNUc13Xd0hwY4TjHui8AAIQlv3SPMJQSz3oAwMmmtM96RmwBAAAAAFYjsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVSGwBAAAAAFYjsQUAAAAAWI3EFgAAAABgtcgT3QHAduGu+lgeq0Qeq5U7WREUAAAANmLEFgAAAABgNRJbAAAAAIDVSGwBAAAAAFajxhYIk7dG1l8za25xyqOotsRrhs8NUVDreKpsC8rhOgAAAMCxxogtAAAAAMBqJLYAAAAAAKuR2AIAAAAArEZiCwAAAACwGpNHAUfNnMop4BS39/h9m1TSxE9uiBmoCjwbvZNJhZhvCgAAADjhGLEFAAAAAFiNxBYAAAAAYDUSWwAAAACA1aixBcLkLU11SqipjXS8dashilvD5Jai2tX7rdWoUaM8bfilpacbcXxCohEXlHBd1/Xv379/vxEnJCQY8WOPPVZsmwAAoHTGjBlT4jHpnmd9UlJSWNcoy7N+7NixYV0DKAtGbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1amyBo+T9dijKMbd4a2wjnPC/T/LW8YYSouTFEBOIMOKCECfERpj/l7Dih2VGnHHokBFv2LDRiA9lm/slKS0tzYiHDx9efEcBAMAxEwiYf4csXbrUiA95nvUbN24sdr/kf9bfdtttR9FDoGwYsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVqLEFihGqtNXxFLx6a2q9tazeONIxY0mKKGGt20Ap1r71rm3rXXM2MSLaiNdtWO9r4/05s434t927jTinIN+Ic31luv663dZt2hhxTEyM7xgAAHD0vGvMbtiwwXfMnDlzjHjfvn1Hfd02nmd9bGysESdGmH/7JEea+yVpW3bGUfcDpzZGbAEAAAAAViOxBQAAAABYjcQWAAAAAGA1amyBI5RiuVjfMd51ar01tfEBs7Y11vH/swu45ndMEfLGZpujR43ytVHgeGtsC4x48fdLjHjmnFm+NrLcXCOO9tyLr463wLxGfGKSr82uXboYcX5+vu8YAABgGjNmTNjn7PhxjRGnflXgO6ZlVnUj/lLh1dgmJib6tnXt2tWI8/PyjDglOtmIq0c19LWxLfu7sPoBeDFiCwAAAACwGoktAAAAAMBqJLYAAAAAAKuR2AIAAAAArMbkUUCYIjxxZMD8figmYP6zSlCMud81J5OSpAjXbDXgmPEtN91qxLGOf+IG76LsrsxJmvZu2WvEiYrzteEEzImwvJNF5bvmJBQFnpm0mjdt6muzcuUqRvzL+nW+YwAAONXdfPPNR92GsyXbiPOjKvuO8f4dE66mIZ71VaqYz/p168xnvXeSTDmMraH88VsFAAAAALAaiS0AAAAAwGoktgAAAAAAq1Fji1OKU1LsOMXul6TogFmdEh8wa2YreGpXE2XWw0Y4sf5+RZj/FB1PjW1K3eYhemJy3XzPBjOODiQYcaziQ7Rh1tTmOmYbhxxzwfVqVc3anYt79/a1uT/9gBHPnTPXdwwAAKeahrHJRlzn9NONOM/zTA4l0vN3y0HnoBHv2vmF75xfs/eXtouSpKpVqxrxRSGe9QcOmM/6xR9/ZsTVVNGI92WnhtUHoDQYsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVqLHFKcVbMxvhqU2J8qyr5q1dkaTESLOmtrJj1qqOH/mUET/2+LNmA5Eh1o91zH+Kf/7L2eYBMeb6cJK/7sYpyDXiMaMeNs/w7I+I8NfYxuSZNbWxgRwjPuj5fFq3PsPTpn91vPUbNhjx/v1pvmMAALDJ6NGji41Lo1+vEUb851qXGXGgwFyTVpKy9n1jxI+PNv/m2OL+asS/Hkr3tVHg21K8tq3bGHFUZJTvmG0bNxpxtXRzfpGoqIpGvPqg+bcBUB4YsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVqLHFKab4mtrogBnHBvz/RJIC5jq0iW6SEUdGm+vSKbqiEbqR5nqyktSiVVsjPr/vACPOC3jqWUKsbbdt8zrzkChPP/LNWh0nxPdakZ463Gg3y4hTPGvsde7c2YgPpPtreRYsWODbBgDAqebKP3Uz4nP/bK4PG5FnPkOdgkO+NvbsNefpSK9rrilbM/UdI94s/3O5JLVq1TLiszzP+v2eNWslaflX3xpxBdf8+2lj9k9GXJo1eoFwMWILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsxuRROKU45txRivBsiPZMFhXnnbRJUpIbb7YRYU4G9fOG7UZcEF3JiPMjzUXLJanvoJuMODsi2jwnwuxXoMC/vPq/X3vfiCM9k0cFAplG7Mg/cUMg35wsqpJn4qteV1xqthERYcQ//WRODiFJB/bv920DAMBma9esNeKaMebfBntzzOepJN068E4jXpPzixG3OPS7EbsxVXxtjP/oRyNOzNhrxLudtJD9LU5Cgvl3TP/+/Y04ItJ81q/6aZWvjQ27fjPiaM/knJkF+WH3CwgXI7YAAAAAAKuR2AIAAAAArEZiCwAAAACwGjW2OKV4f+FjPDW1SYEYI64gcyF0SYp1KhhxINqsZV2z0ayRyfHUyOTEmrUskpSVZNbhulERntj8DuqD2XN8beTEJJnneL63ivIshh4R8C/8Ls/nkZlh1gg9N+FfRpyWZ+5nwXUAwKlg/c9mjW3jihcYcU5sdd85TqRZh9si5iwjzs805+iY+/kK/4UrmXN/RK1fasSbssOf1yIjI8OIn3322bDb8Mp1qanF8ceILQAAAADAaiS2AAAAAACrkdgCAAAAAKxGjS2s5ZRhf6RnXbWECLNWpaLM+tc4x6yflaRAjFkPW+CpoU3dm2PEmRXN40c/Pdbf1yizt55laxXw7F+2cbmvjfyEWPOc/Fwjjswz63ZL/gT9XM/at6UpqaXqFgDwR7N1u1kP2zS/mhHfP3my75ysnbs9W8wnZJwnXvDTIl8blfcsM+IV2auNOJe5LnAKY8QWAAAAAGA1ElsAAAAAgNVIbAEAAAAAVqPGFtYIv6bWvyUy4KmxdaKNONZNNGLvGrWS5MZUNuKcmKpGvDfKrHV94LHbjTiphrfW1bd8rK/GduzY0UacUMV/b+nZ5kl5h8w1eaNzzOvWb9DA10azun8y4l+3/mTGSz/ynXMkN0Rtz1/+/Gcj/uqbb4ptAwCAk11ulrmO+/BxLxhxlfVf+s7JqHiGEccc2mvEDz7xkhEnpP/sa2P5we+MeH9eru+YI9WvX9+3rVmzZka8c+dOI1661FwbtzT+8pe/GPE3POtxAjBiCwAAAACwGoktAAAAAMBqJLYAAAAAAKuR2AIAAAAArMbkUbBWSZNFOSFmm4p0zO9y4lxzoqeIqATzhBCTR+VFm5NHZSVWMuKLr+xhxHUaVzDipGT/BEsRnvmk/ud/Rhtxterm/vR0/81lZ5jbcvZHGXHnc7oaceXIg742liyca8Tbd6z1HXOk0iwD/82335biKAAA7NGlWzcjru7sMeL86NN858RlbDXi26a8aR5Q83QjzNk109dGSZNFnX322UaclJTkO2bx4sVGvHfvXt8x4WKyKJwMGLEFAAAAAFiNxBYAAAAAYDUSWwAAAACA1aixxR+Gt6Y2RImtIjw1tlGuWYeqQIwRuhFxvjbyIuONuCDeU8varaMRx3nKdhMS/JWp3r7HmN1QwPMVVGRkqDbMRnr27GnESXt/M+JvPv/Q18b+PbuMuED5Zuy6nv3e2M8tTSEuAAAnsfiAORlG787mvBXKzzJCJ8L8W0GSMirUNeLWF5h/hm998QcjXp6bXmK/evXqVez+r7/+2rdt//79JbYL2IgRWwAAAACA1UhsAQAAAABWI7EFAAAAAFiNGltYLFQVbfF7vd/kON4tnhpc1/H/Eynw1NmMGjO62H545eX5tz3xxHgjzs72XNNTvJpx0H93OQfNg77//Asjzt262ogjDpk1t5JUkG9eOMcx18vL9XQk31dzG4pbTAQAwMknwfOs/+CZj8wDXHMOivSq7Y04afcyX5vjZ5tzW9Rvau7/bf9CIz6QH+IPBg9vDS31sziVMWILAAAAALAaiS0AAAAAwGoktgAAAAAAq1Fjiz8Mb9VpqG9tvGu9+uOSa2yr1zzdc6Hia3299bH5Bf7j9x8w17/zltXk5ZnnZGWEuE6GeaGMXTuNODY/04jd/EMh+mrW2B7y1NjmueY1vDW1rFkLALCNt55Wki5uaK4FXxBbw4jdgLmGfeL+jWYDkZ5F7CVl55jP+p9f2mrEu/JCPNxLQE0t8H8YsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVqLHFKcVf3eqtqY0oNpakm4b9zYjzvEvhei7irTv9xz9G+9rMzTFPyskx9+d54kMH/cWsgQyzHjYyz6yplad2x80za30kKdc1626zZRb75ntqbF2XNWoBAHarFBXn2zZk9OvmhoxtRpgfYZ7jpK0y4oeem+trM6dVXSPesuNVI97nfdgDCAsjtgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGpMHgVreSdp8k4MFfAeICng+S7HCTGdlMk/HZLjnQ0q14xzD5nxjyvXGXHGQf81cw4VH3vbzNlrTuIkSbGZ5uRQEXnp5gGeyaPyCjyTS0nKcrKNONv1Th5V/GRRTB4FADjZJUdGGXHDuDa+YwIF5vPvYAVz4qekfWuN+Jcd+Ubs/rmer83ERT8acVZ+nu8YAGXHiC0AAAAAwGoktgAAAAAAq5HYAgAAAACsRo0trOGtTPXGEZ6a2kCI+llv2a3rqQp1XLNGZszIh31tZOeaC6jnZ5q1Orn7ze+LJv/LXOQ9EOHvV26W2Y+8TE8NbZYZRx3w18fGZO81Yid3vxEX5Jo1tlnyt5Ep895yCszPo6CEGlsAAE52KZEVjfiOt770HePuNJ+pUTnmM9TJ3mXE/ztzkREXtKzqa/PHne8bcabnGQvg6DBiCwAAAACwGoktAAAAAMBqJLYAAAAAAKtRY4s/jNKtY+vdZlaJnnZaNbNN17/GXES+udZrZLb5zyg33fy+KNu75myopXMzzTqbiCyz1jUq21zYNjrbrJ+VpIgcsx5IOeYxOQXmurYHnCx/NwrM6+Z66n+oBgIA2CYxIsKIW5zezojjf9vpOyerQiUjjk0/YMR5iU3Ma3RdYsQFc7/ztZnt+tegB1B+GLEFAAAAAFiNxBYAAAAAYDUSWwAAAACA1aixhbX869p61rENUWPrPcZbY1urZg3z+IJcXxsBTx1qVK75/dCY+0ebx8fEeNr0r/4alW3Wu0bmmmvMRuab6+cFcs1aH0mSZx3bPM8xac5BI04vMGuFJf+6tbmeeiDvOrYAAJzs6kVVNuIqbfoZcdzeZb5z4vaZfyK7eeZzeMRTbxhxQZfmRvztzrm+NgtY/R04phixBQAAAABYjcQWAAAAAGA1ElsAAAAAgNVIbAEAAAAAVmPyKPxh+KeK8nM9Ezfku3nm/nxzQiUn35zESZIics2F3gOec2KzzEmcCnKizTZDLNAe4ZmUIpBvTiYlz+RRyjUngpKk/BxzsqiDMuN01+znoXzz3iUpz9O3XM9kUUx7AQA42XlHbTJlPv/iDq434vzEbr42IrK2G7Gb2NCI8zqbk0Xlfm8++3liAscfI7YAAAAAAKuR2AIAAAAArEZiCwAAAACwGjW2+MPwVrO4rr++JdfNN+JDziEj/n7Jl0Z8ySWX+toIFOSYGxzzn1Fk9m/F7penD5LkeOp03fxDxcb53hpcSVky627TZZ7jran11tMe3mZ+ZgWUCAEALJMQYT53kxRrxJ8tnGrEF53f3tfGXu0z4sqB1kYcn2Qe//n6CWH2EkB5Y8QWAAAAAGA1ElsAAAAAgNVIbAEAAAAAVnPcUIWIIUQ4pVklFDh2vN/CBDy/k1GeONTvbFxElBFXDJh1NxWUYMSxnliSAoEozxbPdZwSvi8KUdtaUJBrxPmeNWdzHLOu95Bj7pekDNc85mC+GWd5amxzQvXD9/8Gnppb3xnAiZVfukcYSolnPRCa98leJ7aCEW86dEAAjo3SPusZsQUAAAAAWI3EFgAAAABgNRJbAAAAAIDVqLGFNUqqsY301th6a18lRQcijDjOs9ZdnBNtxPEy48Ptmj0p8FSeFnjrUh1zf6j1Y3Nlrm2b41nrNtdzTm6Bfy1cb7vec3I8BbSub+Vf/1rAwMmOGtvyxbMeAHCyocYWAAAAAHBKILEFAAAAAFiNxBYAAAAAYDUSWwAAAACA1SJLPgQ4OXjLxn2xp7DcDTEJindip5wCc4IlJ5BrHu/4J3pyPJNS5cs7aZM5sVOeZ9Km/BCTR3m3eYvk/XGINjxxgecc/xkAAADAHwMjtgAAAAAAq5HYAgAAAACsRmILAAAAALAaiS0AAAAAwGoktgAAAAAAq5HYAgAAAACsRmILAAAAALCa43oX/wQAAAAAwCKM2AIAAAAArEZiCwAAAACwGoktAAAAAMBqJLYAAAAAAKuR2AIAAAAArEZiCwAAAACwGoktAAAAAMBqJLYAAAAAAKuR2AIAAAAArPb/ANYDQEA5lZiFAAAAAElFTkSuQmCC"},"metadata":{}}]},{"cell_type":"code","source":"F.softmax(output_van)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:00:39.902791Z","iopub.execute_input":"2024-06-26T08:00:39.903395Z","iopub.status.idle":"2024-06-26T08:00:39.910727Z","shell.execute_reply.started":"2024-06-26T08:00:39.903364Z","shell.execute_reply":"2024-06-26T08:00:39.909791Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/268539198.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  F.softmax(output_van)\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"tensor([[6.5714e-09, 9.9999e-01, 1.9511e-08, 1.1298e-09, 1.5135e-07, 4.5946e-09,\n         2.7642e-06, 2.6587e-06, 4.9570e-07, 2.3572e-09]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"F.softmax(output_van, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:29:45.178444Z","iopub.execute_input":"2024-06-26T08:29:45.178837Z","iopub.status.idle":"2024-06-26T08:29:45.186111Z","shell.execute_reply.started":"2024-06-26T08:29:45.178808Z","shell.execute_reply":"2024-06-26T08:29:45.185244Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"tensor([[6.3369e-08, 9.9998e-01, 7.5404e-08, 6.5448e-09, 8.7516e-07, 2.9241e-08,\n         9.9585e-06, 5.1826e-06, 2.9752e-06, 2.1393e-08]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"m = VAN_MNIST(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:20:52.028095Z","iopub.execute_input":"2024-06-26T08:20:52.028881Z","iopub.status.idle":"2024-06-26T08:20:52.041441Z","shell.execute_reply.started":"2024-06-26T08:20:52.028847Z","shell.execute_reply":"2024-06-26T08:20:52.040499Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"m(image.unsqueeze(0)).sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:24:36.479229Z","iopub.execute_input":"2024-06-26T08:24:36.479666Z","iopub.status.idle":"2024-06-26T08:24:36.517719Z","shell.execute_reply.started":"2024-06-26T08:24:36.479634Z","shell.execute_reply":"2024-06-26T08:24:36.516557Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"tensor(1.0000, grad_fn=<SumBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"m2 = CNN_MNIST()\nm2(image.unsqueeze(0))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:22:13.980128Z","iopub.execute_input":"2024-06-26T08:22:13.980828Z","iopub.status.idle":"2024-06-26T08:22:14.176817Z","shell.execute_reply.started":"2024-06-26T08:22:13.980793Z","shell.execute_reply":"2024-06-26T08:22:14.175276Z"},"trusted":true},"execution_count":94,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m2 \u001b[38;5;241m=\u001b[39m CNN_MNIST()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mCNN_MNIST.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x147456 and 9216x128)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x147456 and 9216x128)","output_type":"error"}]},{"cell_type":"code","source":"output_cnn","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:22:08.449494Z","iopub.execute_input":"2024-06-26T08:22:08.450411Z","iopub.status.idle":"2024-06-26T08:22:08.457093Z","shell.execute_reply.started":"2024-06-26T08:22:08.450378Z","shell.execute_reply":"2024-06-26T08:22:08.456144Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"tensor([[3.9504e-08, 1.0000e+00, 4.3885e-17, 2.6136e-25, 4.3957e-07, 1.6146e-15,\n         3.5147e-12, 1.6628e-07, 1.1997e-19, 7.4361e-10]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compare the accuracy of the models**","metadata":{}},{"cell_type":"code","source":"# Function to load checkpoints\ndef load_ckpt(path: str):\n    '''\n    Loads checkpoint from given path.\n    '''    \n    return torch.load(path)\n\n# Load checkpoints\n#van_ckt = load_ckpt()\n#cnn_ckp = load_ckpt()\nvan_MNIST_ckpt = load_ckpt('/kaggle/working/VAN_MNIST(channels=[64, 128], stages=2, l=[1, 1], expansion_ratio=[2, 4]).pth')\n#cnn_MNIST_ckpt = load_ckpt()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:05:46.656076Z","iopub.execute_input":"2024-06-26T10:05:46.656459Z","iopub.status.idle":"2024-06-26T10:05:46.691914Z","shell.execute_reply.started":"2024-06-26T10:05:46.656426Z","shell.execute_reply":"2024-06-26T10:05:46.690988Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"van_MNIST_ckpt['running_loss_train']\nvan_MNIST_ckpt['running_loss_test']\nvan_MNIST_ckpt['test_accuracy']\nvan_MNIST_ckpt['train_time']\nvan_MNIST_ckpt.keys()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:08:27.897839Z","iopub.execute_input":"2024-06-26T10:08:27.898556Z","iopub.status.idle":"2024-06-26T10:08:27.904409Z","shell.execute_reply.started":"2024-06-26T10:08:27.898498Z","shell.execute_reply":"2024-06-26T10:08:27.903559Z"},"trusted":true},"execution_count":161,"outputs":[{"execution_count":161,"output_type":"execute_result","data":{"text/plain":"dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'loss', 'running_loss_train', 'running_loss_val', 'test_accuracy', 'train_time', 'inference_time'])"},"metadata":{}}]},{"cell_type":"code","source":"# Plot Train and Validation Loss for models\n# Funciton to plot losses\ndef plot_loss(train_loss, val_loss):\n    '''\n    Function to plot the train and validation losses during training.\n    '''\n    epochs = range(1, len(train_loss) + 1)\n    plt.plot(epochs, train_loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# VAN on MNIST\nplot_loss(van_MNIST_ckpt['running_loss_train'], van_MNIST_ckpt['running_loss_val'])\n# VAN on cluttered MNIST\nplot_loss(van_ckpt['running_loss_train'], van_ckpt['running_loss_val'])\n# CNN on MNIST\nplot_loss(cnn_MNIST_ckpt['running_loss_train'], cnn_MNIST_ckpt['running_loss_val'])\n# CNN on cluterred MNIST\nplot_loss(cnn_ckpt['running_loss_train'], cnn_ckpt['running_loss_val'])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:12:42.942403Z","iopub.execute_input":"2024-06-26T10:12:42.943290Z","iopub.status.idle":"2024-06-26T10:12:43.262211Z","shell.execute_reply.started":"2024-06-26T10:12:42.943255Z","shell.execute_reply":"2024-06-26T10:12:43.260846Z"},"trusted":true},"execution_count":168,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[168], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m plot_loss(van_MNIST_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_loss_train\u001b[39m\u001b[38;5;124m'\u001b[39m], van_MNIST_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_loss_val\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# VAN on cluttered MNIST\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m plot_loss(\u001b[43mvan_ckpt\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_loss_train\u001b[39m\u001b[38;5;124m'\u001b[39m], van_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_loss_val\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'van_ckpt' is not defined"],"ename":"NameError","evalue":"name 'van_ckpt' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Compare the Test Accuracy\nimport pandas as pd #pandas==2.1.4\n\nmodels = ['VAN_MNIST', 'CNN_MNIST', 'VAN', 'CNN']\nvalues = [van_MNIST_ckpt['test_accuracy'], cnn_MNIST_ckpt['test_accuracy'],\n          van_ckpt['test_accuracy'], cnn_ckpt['test_accuracy']]\naccuracy_df = pd.DataFrame({'Model': models,\n                            'Accuracy': values})","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:21:07.098164Z","iopub.execute_input":"2024-06-26T10:21:07.099871Z","iopub.status.idle":"2024-06-26T10:21:07.486921Z","shell.execute_reply.started":"2024-06-26T10:21:07.099821Z","shell.execute_reply":"2024-06-26T10:21:07.485689Z"},"trusted":true},"execution_count":169,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[169], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m#pandas==2.1.4\u001b[39;00m\n\u001b[1;32m      4\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAN_MNIST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN_MNIST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m values \u001b[38;5;241m=\u001b[39m [van_MNIST_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mcnn_MNIST_ckpt\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m           van_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], cnn_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      7\u001b[0m accuracy_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: models,\n\u001b[1;32m      8\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: values})\n","\u001b[0;31mNameError\u001b[0m: name 'cnn_MNIST_ckpt' is not defined"],"ename":"NameError","evalue":"name 'cnn_MNIST_ckpt' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Compare the Test Accuracy\nimport pandas as pd #pandas==2.1.4\n# Create accuracy table\nmodels = ['VAN_MNIST']\nvalues = [van_MNIST_ckpt['test_accuracy']]\naccuracy_df = pd.DataFrame({'Model': models,\n                            'Accuracy': values})\n# Plot the values\nprint(accuracy_df.to_string(index=False))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:22:23.084640Z","iopub.execute_input":"2024-06-26T10:22:23.085653Z","iopub.status.idle":"2024-06-26T10:22:23.094839Z","shell.execute_reply.started":"2024-06-26T10:22:23.085610Z","shell.execute_reply":"2024-06-26T10:22:23.093717Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"    Model Accuracy\nVAN_MNIST     None\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Computational Cost Comparison**","metadata":{}},{"cell_type":"code","source":"# Compare number of parameters\ndef count_params(model):\n    '''\n    Funciton that returns the number of trainable parameters and the total number of parameters.\n    '''\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n    \n    return trainable_params, total_paramas\n# VAN\nvan_trainable_params, van_total_params = count_params()\nprint('Trainable Parameters VAN: '\"{:,}\".format(van_trainable_params), 'Total Parameters VAN: '\"{:,}\".format(van_total_params))\n# CNN\ncnn_trainable_params, cnn_total_params = count_params()\nprint('Trainable Parameters CNN: '\"{:,}\".format(cnn_trainable_params), 'Total Parameters CNN: '\"{:,}\".format(cnn_total_params))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:25:04.018153Z","iopub.execute_input":"2024-06-26T10:25:04.019107Z","iopub.status.idle":"2024-06-26T10:25:04.026324Z","shell.execute_reply.started":"2024-06-26T10:25:04.019068Z","shell.execute_reply":"2024-06-26T10:25:04.025290Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"# Compare number of modueles\ndef count_modules(model):\n    '''\n    Function that returns the number of modules in a network.\n    '''\n    num_modules = sum(1 for _ in model.modules())\n    return num_modules\n\nvan_num_modules = count_modules(baseline)\ncnn_num_modules = count_modules(van)\n\nprint(\"Number of modules in VAN:\", van_num_modules)\nprint(\"Number of modules in CNN:\", cnn_num_modules)","metadata":{"id":"6a5e_FegAoGs","executionInfo":{"status":"ok","timestamp":1711019998475,"user_tz":-60,"elapsed":336,"user":{"displayName":"Tobias Jedlicka","userId":"10721733797122281021"}},"outputId":"85dbf9d1-b18e-46b1-b7d1-4ff66b5b309c","execution":{"iopub.status.busy":"2024-03-21T11:39:11.847181Z","iopub.execute_input":"2024-03-21T11:39:11.847897Z","iopub.status.idle":"2024-03-21T11:39:11.924267Z","shell.execute_reply.started":"2024-03-21T11:39:11.847864Z","shell.execute_reply":"2024-03-21T11:39:11.923437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use deepspeed profiler to estimate FLOPS, MACS and Latency\n!pip install deepspeed\nfrom deepspeed.profiling.flops_profiler import FlopsProfiler","metadata":{"id":"5npplbR9v-vu","outputId":"3c1e8282-709e-4771-ae20-ee0d3d59ae2c","execution":{"iopub.status.busy":"2024-03-26T18:06:35.292615Z","iopub.execute_input":"2024-03-26T18:06:35.293076Z","iopub.status.idle":"2024-03-26T18:07:19.351944Z","shell.execute_reply.started":"2024-03-26T18:06:35.293044Z","shell.execute_reply":"2024-03-26T18:07:19.350905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1 = VAN(num_classes=14, stages=2, channels=[64, 128], image_channels=3, l=[1,1], expansion_ratio=[2,4])\nprofile_model(input_model=m1, data_loader=pc_train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-25T17:52:16.531727Z","iopub.execute_input":"2024-03-25T17:52:16.532454Z","iopub.status.idle":"2024-03-25T17:52:19.048782Z","shell.execute_reply.started":"2024-03-25T17:52:16.532421Z","shell.execute_reply":"2024-03-25T17:52:19.047886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def profile_model(model, data_loader=train_loader):\n    '''\n    Function that uses the deepspeed profiler.\n    Uses a short warum-up stage and profiles at batch_idx = 50.\n    '''\n    # Profile a model to get the FLOPS and Latency\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n    prof = FlopsProfiler(model)\n    # Profile at step 50 to warmup\n    profile_step = 50\n    for batch_idx, (data, target) in enumerate(data_loader):\n      if batch_idx == profile_step:\n        prof.start_profile()\n\n      data, target = data.to(device), target.to(device)\n      optimizer.zero_grad()\n      output = model(data)\n      loss = criterion(output, target)\n\n      if batch_idx == profile_step:\n        prof.stop_profile()\n        flops = prof.get_total_flops()\n        macs = prof.get_total_macs()\n        params = prof.get_total_params()\n        prof.print_model_profile(profile_step=profile_step)\n        prof.end_profile()\n\n      loss.backward()\n      optimizer.step()\n\n      if batch_idx == profile_step:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-03-25T17:47:37.606860Z","iopub.execute_input":"2024-03-25T17:47:37.607618Z","iopub.status.idle":"2024-03-25T17:47:37.616235Z","shell.execute_reply.started":"2024-03-25T17:47:37.607586Z","shell.execute_reply":"2024-03-25T17:47:37.615214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FLOPs for VAN\nfor data, target in augmented_loader:\n  input_data, input_target = data.to(device), target.to(device)\n  break\n\nflops_van, params = profile(van, inputs=(input_data,))\nprint(f'Estimated Params for VAN:, {(\"{:,}\".format(params))}')\nprint(f'Estimated FLOPs for VAN:, {(\"{:,}\".format(flops_van))}')","metadata":{"id":"zqHb0WdSA3Rc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect bottlenecks for Baseline CNN\n\n# Define your model and input data\nbaseline = BaselineCNN().to(device) # Your PyTorch model\ninput_data = input_data  # Your input data\n\n# Perform forward pass while profiling\nwith profile(profile_memory=True, record_shapes=True, use_cuda=True) as prof:\n    with record_function(\"model_inference\"):\n        output = baseline(input_data)\n\n# Print profiling results\nprint(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))","metadata":{"id":"6vf3Yhg3BFbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect bottlenecks for VAN\n\n# Define your model and input data\nvan = VAN().to(device) # Your PyTorch model\ninput_data = input_data  # Your input data\n\n# Perform forward pass while profiling\nwith profile(profile_memory=True, record_shapes=True, use_cuda=True) as prof:\n    with record_function(\"model_inference\"):\n        output = van(input_data)\n\n# Print profiling results\nprint(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))","metadata":{"id":"jvtS_uZKBFSo"},"execution_count":null,"outputs":[]}]}